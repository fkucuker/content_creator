so we should be live again i'm very
sorry about the interruption it looks
like stream yeah that's big problems and
not only streaming but like pretty much
hard for the internet
but uh the good news is we're back i
will give a bit of time to people to
join us on the new live stream i will
share the new url for the youtube
livestream
in the meantime
lewis if you wanted to
show people
a bit the part of the course maybe what
i share on the on twitter and everything
sure that sounds good
so i'm just going to share my screen now
and hopefully you can see that
so
basically um in part two of the course
um if if this is the first time you're
tuning in
we kind of have four main sections we
have a four chapters so i don't know if
that's uh zoomed in enough or not so
long just let me know
um but basically we have uh a whole
chapter on the datasets library
um we have a whole chapter on the
tokenizers and so this is where you'll
get to learn
all of the techniques that lucille just
showed us about training tokenizers from
scratch but also how to really
understand what these tokenization
algorithms are doing under the hood and
i think this is a a really great time
way to take a deep dive
and then in the
main nlp tasks
this is where we've collected the most
common kind of
applications of transformers in nlp
and we have
one section for each
application where you'll see how you can
um basically fine-tune your models um in
two different ways so one way is using
the trainer which i mentioned at the
start of this
session and the second way is using the
hugging pace accelerate library which uh
sylvan will
show us shortly
so
the kind of idea that we have in mind
for the kind of projects that you'll be
building is
we try to pick projects that kind of
build on top of these applications so
there's projects involving summarization
or translation or
even
things like question answering and so if
you're working on one of these project
ideas in the forum um and you're like i
don't know where to get started i
recommend checking out chapter seven um
of the of the course and that's where
you'll get kind of all the details you
need
and if you still find that there is
something that is not clear just put a
question in the forum or in discord and
we'll be here to help you
um and then the last chapter um actually
kind of relates to a question we had at
the very start of the session which was
kind of like okay i've got a problem
with my with my pipeline or with my
trainer what do i do
and so in chapter 8 we kind of provide
some sort of like good practices or
strategies for handling this
so the first one that we talked about is
kind of like what happens when you get
an error and this will inevitably happen
um in your projects you'll get some
uh kind of maybe cryptic problem that
you're trying to debug and here we give
you
um some ideas of like how to do it how
to debug this
um and then we talk about if this is the
first time that you're joining the
hugging fest community we have um the
forums that we're using today and in the
forums um we explain how you can ask for
help
and uh the kind of say ways you might
want to phrase your question and your
problem so that people can kind of get
all the information they need to help
you
and then comes debugging the training
pipeline where sylvan gives you a kind
of end-to-end um sort of sequence of
steps of how to really debug
um the trainer and this um this also
goes into things like understanding how
to handle cuda errors which are kind of
common
and then finally if you in the course of
your projects actually discover that
there's a bug or something is maybe
missing in the transformers library that
you think would be really useful to have
and we have a section here on how to
write a good issue on the github
repository
so that's kind of like a high level
breakdown of part two of the course
and um
we're um yeah looking forward to see
what you build in your projects
and let me see i think i'll stop sharing
my screen now
how
are we back are we back in action or
should we wait by connections uh yeah
maybe let's go ahead with the schedule
talk people can catch up afterwards on
the on the ministry
on the new stream
great
so
just in case it got lost at the end
sylvan's going to now talk to us about
how to supercharge your pi torch
training loop with hugging base
accelerate
so without further ado moment of truth
hi everyone i'm silva i'm a research
engineer at tagging face and today i'm
here to talk to you about our library
accelerate
this is a library built on top of by
torch to make training on any kind of
distributed setup as easy as possible
while letting you retain full control
over your training loop
this is super convenient when you want
to implement a training tweet but is not
supported by the trainer api but you
still want to run your script on several
gpus or ntps
for this talk i've written a small
script that fine-tunes the birth model
on the glucolate data set to go over it
quickly i load the data set using the
urging phase datasets library proposes
it by tokenizing everything
load my pre-trained model create a data
collateral that will pad the examples
dynamically to the longest one in each
patch
create my training and validation data
loaders
my optimizer
a learning rate scheduler then go over a
traditional pytorch training loop with
an evaluation at each e-book
i can launch my training script quite
easily but what if i now want to launch
it on several gpus at the same time
this is going to require a few changes
in my code let's have a look at why
let's say we have 4 gpus
on each gpu we are going to load a
different batch of data
that will go through our model
and we'll get a loss for that batch
this will allow us to compute the
gradients different on each gpu
we then average those gradients across
all the gpus before performing the
optimizer step
this way
the model which was copied on all gpus
stays the same on each gpu during the
wall training
on this slide everything colored in blue
is the same on the four gpus while the
boxes in pink indicate objects that are
different on each gpu
in terms of code here is what we first
need to add to a script
first it needs to accept a new argument
called localrank that will indicate the
index of the process launched
then we have to initialize the torch
distributed environment and setup the
device using that local rank
lastly we need to wrap the model in a
distributed data parallel container
which will be responsible for the
average across processes of the
gradients we saw on the previous slide
this part was easy enough the main
challenge
is to make sure our data is then built
exactly the same way on each gpu
we need to launch the training script on
each gpu
each one of the script will execute the
same operations and we need to give the
same results
the dataset needs to be prepared exactly
the same way on each of those processes
it looks easy said like this but some
operations like converting a list into a
set to remove duplicates are not
deterministic
this means that if you use such an
operation for instance to generate your
labels you may have your labels ordered
differently on each gpu
then you may need to make sure the
training data is traveled the same way
on each process
otherwise each gpu might look at data
another gpu has already tweaked it
only then can you have reliable batches
to send to your model
in terms of code this require all of
those changes just replace the shuffle
equal troll line we use in our training
data loader
into something that will work properly
for distributed training
now that we have those changes ready we
need to launch our training on gpus
by torch provides a launcher for this so
you can type this command python minus m
dot distributed dot launch
minus n proc per node the number of gpus
you have two here
the name of the training script followed
by the arguments to your script the
script launches properly and starts
training
but note that if we try to execute it
like a regular python script it will
fail
if we want to train on tpus we'll need
additional changes compared to the
distributed training on several gpus the
device changes we need to wrap the
training data order in some parallel
loader and rather change the optimizer
to step line which needs to become
xm.optimizer step optimizer
launching the script with a distributed
launcher with so before would then fail
we need a special launcher for tpus and
of course launching the script like a
regular python script fails again
if we wanted to use mixed precision
training which i won't detail here you
need to add even more lines of code to
the original training script
it will then run properly on gpu but
will fail on cpu
in general adding support for any new
training technique will require you to
learn a lot about that given technique
and add lines to your code that will
then make your script stop working on a
simple setup and make debugging super
hard
accelerate has been created to solve
that problem
by learning one new api that requires a
few changes to your script you then get
something you can run on all kinds of
distributed setup which also supports
mixed precision training or advanced
techniques like zero data parallelism or
zero offload developed by deep speed
the changes to your training loop are
kept to a bare minimum you need to
import a new object called accelerator
instantiate it at the beginning of your
training script then send all your main
objects module optimizer data orders to
the prepare method of that accelerator
the last line to change is the lusted
backward line which needs to become
accelerator.backward loss
using accelerates also allows you to get
rid of the device placement lines as the
library will automatically handle that
for you
it also makes it easy to implement
distributed evaluation which we didn't
even start showing in the previous
examples
you just need to gather the prediction
on labels across all gpus when you want
to use them to compute your metric
to then launch your script there are a
few utilities added by accelerate
first you can set up a configuration by
typing accelerate config
this will pop up a few questions you
need to answer
once it's finished a config is stored in
your cache by default in
your home directory dot cache slash
editing face accelerate
you can then use accelerate launched
followed by the name of your training
script and all the arguments
but your script can still be launched as
a regular python script if you need to
debug it works regardless of whether you
have a gpu or not you can use accelerate
launch to launch your script on several
gpus or on tpus
you can still use the torches within
launcher instead of accelerate launch if
you prefer
all in all with just learning one new
api which added four new lines of code
your custom training loop can now run on
any kind of distributed setup
now if you are in a notebook and want to
launch a distributed training for
instance on collab accelerate comes with
a launcher you can use
there aren't any tpu is available at the
time i was shooting this video
so i'm using a machine with two gpus
instead
the main thing to remember is that you
should put all the parts of the code
that concern objects that will go on
multiple devices gpus or tpu cores in
one big training function
you can have a look at your data set
outside of this training function but
the model the data loaders your
optimizer and the accelerator should all
be defined inside
here my training function is very
similar to the script i just showed you
the only difference is that i'm not
using arc paths and set all my upper
parameters in a cell
if you have executed any instruction
that initializes cuda or python xla you
will need to restart your notebook or
get an error for instance here we ask
for the number of gpus at the very top
and when trying to launch our training
function the launcher failed because
cuda was already initialized
restarting or not executing the first
cell will make everything work smoothly
we can see the training function is
launched twice as many of the logs are
duplicated
before the training starts
since i added a few lines to only
display the progress bar on print on
main process the training logs only
appear once
this concludes my presentation of the
accelerate library i hope you enjoyed it
and that will give accelerate a try the
next time you need to use a custom
training loop if it doesn't work as
expected please let us know on the
forums or on github
awesome
thanks a lot for that super detailed
video and really nice transitions you
like the adobe premiere pro
thank you it makes my poor version of
just a collab notebook i feel very
inferior
so i think we have time for one question
um and the question might be like
you know on the space of like
training models we have things like the
trainer we have accelerate and there's
also some other frameworks that people
can use
um what kind of decisions would you
think about like when should i use
accelerate versus the trainer or
something else for example if i'm using
like tensorflow does it work with
accelerate that kind of stuff
sure great question um so my my advice
is always to to say that if you just
want to quickly function the model to
see uh like what kind of results you can
get
or if you on your in your data set and
in your task you should just use the
interface trainer or if you're using
tons of flow care as that fit because
that's gonna
just work and you will give you'll get
results without needing to to go under
how to train a model with pie touch
in just a few minutes if you know if you
want to try a specific training trick
then that's where accelerate was
specifically designed because those
objects like the trainer once you want
to to change a little thing like you
should in your talk for instance to use
a custom loss you have to learn like how
it works internally you have to learn
the methods to that you need to to
subclass so
when you're there it's actually i think
more useful to just go back to a
training loop and and supercharge it
with acceleration if you need
distributed training
uh and if you it doesn't work with
tensorflow it's specifically built on
top of python for tensorflow you'll need
to
to go to a manual training loop in terms
of
um
but yeah that's pretty much uh was my
advice
cool and then maybe just a very quick
last one which i've asked the other uh
library developers what's on the roadmap
for accelerate what are the kind of
things you're interested in working on
so yeah a lot in uh is another roadblock
for for accelerate uh we have a few
improvements to the existing uh for
instance the speed integration could be
made a little bit better so we're going
to work on that
and then we also add a
new utility to to make it easier to have
checkpoint directly inside the
accelerate for now you still need to
separately save your model your
your your randomness generator states or
all the optimized states also learning
great schedule states so we're going to
build one nice utility function that
does all of that for you and upload them
to the aging test app and so that you
can resume your training from any
machine surprisingly with just one layer
of code for accelerate
awesome
so should we introduce the next speaker
what do you think i think so i think
we're ready for our next speaker let me
add after the stream
uh hi merv
hello
so uh our next speaker is marvin nerjan
and murphy is a developer advocate
attacking face working on developing
tools and building content around them
to democratize machine learning for
everyone and she'll explain to us how to
showcase our model demos with spaces so
i'm very exciting to learn more about
those
that's awesome just a question for you
movie um the the one that i should share
is move a talk or move a space demo yeah
navi talk first and then mirror space
demo second great
just adding this now
moment of truth are we still alive yeah
you soon for the questions
hello everyone and welcome to the
hugging face course event
i'm marve and in this talk i'll be
talking about spaces
i was a machine learning engineer
building information extraction tools
and chat bots
currently i'm working as a developer
advocate at hugging face and a graduate
student working on nlp
at hacking phase we provide you data
sets and models you need to build your
machine learning applications
those are the hub
transformers and data sets
i come from a data science pad rather
than a developer path and one thing i
found painful was building the most of
the models i created and deploying them
if you are just like me we've got you
covered with our new feature called
spaces
for instance i have this story generator
app which consists of three models
sequentially added together
i have one model that is from french to
english one english story generator and
another model from english to french
i written this up with only few lines of
code and deployed it in minutes and i
will show you how to do that
and no i didn't write any javascript
code or didn't build my api with flask
i used gradio for this and deployed it
on spaces
we support both streamlet and graduate
sdks
basically i define my interface in
gradio
i pass the example inputs to help the
end user
define title and description to my demo
pass three models as series added
together
i defined my input component which is a
text box and launched it uploading this
piece of code to spaces will be enough
for me to serve it and showcase my
projects to the stakeholders
in hugging phase you can work with the
stakeholders of your project in hugging
face organizations
and share your spaces internally build
your demos and showcase your models to
the stakeholders or clients
personally i find easy to showcase my
portfolio through hugging face spaces
they are hosted on your profile
you might say it's cool but which sdk
should i use
we recommend you to give a try to both
our two sense is
with gradio you can build easy to use
simple interfaces by specifying your
model inputs and outputs
meanwhile streamlit provides more
flexibility with the community
components
there are many cool spaces out there for
instance this anime gun space draws you
as an anime character this is built with
gradio
another cool space lets you chat with
gandalf this one is built with gradio as
well
you simply need to go to hub
on top right corner click your icon and
create a new space
name your space pick the sdk you want to
use and create your space
directly drag and drop your app.pi file
or use git lfs to push your application
to space
and you will have your app in minutes
if you need extra help with sdks and
spaces i've written two blog posts for
both of the sdks to help you get started
thanks a lot for listening and enjoy the
event
now just quickly have a look at a
hands-on demo from merve with a space
and then we'll dive into a q a
thanks
this is one of my favorite projects and
it has generative models that you can
write to it
so
let's write my name is marve and i like
to
[Music]
this is going to complete my text and i
have replicated with streamlit and
transformers
it takes couple of parameters that i
will explain shortly
so let's go there
so this is my space and i have
replicated right with transformers
uh it takes maximum length uh
temperature uh this is a parameter that
decides the
conservativeness and
how random your
outputs are going to be
these are sampling parameters
let's do this
so i have written hi i'm marv and i like
to play games let's say my name is marv
and i like to
i like to play guitar i am student at
university of texas at austin okay
so how i did it
good question i'm going to show you how
did i do that shortly
so my streamlined application actually
consists of only an app.pi file
requirements file and the readme
so
so i have written an inference code
i have written the same thing using only
pipeline which was a very short code but
i wanted to show you this one
in particular so in streamlit if you
would like to
optimize some processes that take a lot
of time you can do
sd.cache
it will load your model in a faster way
and
i have written this inference code which
takes a couple of parameters
these parameters will come from the ui
here
and i'm going to tell you how to do that
shortly
so this is the title
which is here it's like a header
and
this is sd.write you write some text
and you can take your parameters here
so if you do st sidebar
this is going to take your slider to the
sidebar if not
it will be here but i want it to be here
so i have taken it to the sidebar
i have temperature i have top k and top
p if you have an integer input use
number input if you have a
continuous input use slider
and i have my text area which i will
input the text
in this is the default value so it's see
how a neural network autocompletes your
text so it was here previously and i
have taken it to here
this was the default value
and i do inference
and i just
output
the
result of the inference
and i just write it there so it is
it is here
next i'm going to show you a gradual
application
so i have made another application which
is story generation with gpt2
i'm going to show you how to do that but
let's run it for a second
so i use it to generate dungeons and
dragons
stories
i have entered adventures approached by
a mysterious stranger in the tavern for
a new quest
and it has generated me a nice beautiful
probably scandinavian related
story
and i'm going to show you how i did that
so
this is my app.pi file i only have this
and nothing else by the way
i have given description
which is this
and the title
and an example
to give a hint to the user so that they
know
uh
what they are supposed to input
so at examples are given as a list
uh i am going to give
the
model name
make sure to put a hugging face
uh slash to indicate that this is a
hugging face model
this is the model name
i have past title and description and i
just launched it and i have this
beautiful
interface
now i'm going to show you something even
more interesting which is gradua series
so this is
a
french story generator but i have used
the same model
so i have an example again i'm going to
submit this and i will probably not
understand uh what is what does it
output
but
still this is probably something valid
so let me show you how i did that
so i'm going to go to my app.pi file
so in graduate there is something called
series
it adds your models
sequentially
so i have three models here
one is from french to english
another one is story generator and this
one is from english to french
so
what i'm basically doing is that my
first input
is inferred inside this
from french to english model
here
and
my second input
which is the output of the first input
is going to be inferred in this story
generator and my uh
the output of this story generator will
be inferred here
so in the end i'm going to autocomplete
this input
so in the end i have a nice
uh french story generator
which is this beautiful application done
with gradio with only
few lines of code
so last application i will show you is a
streamlined exploratory data analysis
hosted in hugging face spaces
so i have this poetry data set
which consists of
various poetries authors
the name
age and the type
i have uh i have done couple of data
visualization on this data set
some of them consist of streamlets
components which are very good
interactive nicely done
other ones are matplotlib and seaborn
so let's get started
the good thing about this component is
that uh
you can see the content of a cell
you can
order
uh
by columns
which is very good
i have done couple of visualizations in
matplotlib i'm going to show you
i have
visualized the number of
poems for the each alter
and this is done in seaborn
i have shown the distribution
of poem types according to ages and
authors
this is also done in matplotlib
i have i have done a word cloud of
modern poems and renaissance poems
and i have standardized the content of
the data set
uh by getting rid of special characters
and lowercasing them
so i'm writing my explanations and
headers using streamlet components write
subheader
so if you would like to use seaborn and
matte plaid lib
you can use st.piplot
to print your matte platleband seaborne
plots
that's what i did with the matte
plataban seaborn platz in this
in this application
so to get the distributions i have done
a couple of encoding and i plot it and
i have shown it with streamlet piplot as
well
so for the word cloud i got the
renaissance poems and the modern poems
i have passed them to the word cloud to
prompt them
so lastly i have this bar charts that
prompts most appearing words including
stop words i have
the most appearing 50 words this was the
bar chart i have just shown you
the interactive one
so
this is the end of the demo enjoy the
rest of the event
welcome back
hi again
so thanks a lot for all this uh token
for this demo
thank you so much for hosting me
is that you showed an example of a live
demo of a model and another example
where you did some data visualization of
a data set and what are our application
of species
that are common or that you could think
of
um can you rephrase the question again
i i was saying like you you should an
example of using a model
and then you also showed an example of
like doing a data set visualization both
using species and i was wondering are
there like other amazing application of
spaces
that are common
like those two or is this done pretty
much for the two things you showed us
ah
i i have to think about it i i don't
know uh there is probably like there are
so many good applications like there is
this one application called pipeline
visualizer by omar
uh he has visualized the
visualize the spacey pipelines uh it is
really good
um
what else i i want to create this um
this uh data visualization
metric tool sort of using streamlight on
hugging face like i'm on it at the
moment so hopefully it will be done
but people are also like showing their
tools uh demoing them
which is awesome so i think you will
probably get to do that as well
nice very nice and uh another question
was like for the for the model
demonstrations uh what kind of hardware
does the space run is on is it like just
a cpu or do we have access to gpus using
spaces
uh we do have access to gpus but i don't
think that all of the spaces are
actually getting them it's
it's a complicated answer actually but
most of them have cpus yeah
that makes sense
and then let me check so
it's a forum
[Music]
did you have any questions serious on
your end
yeah i was wondering like um you know if
we if we want to build something in
extreme little radio what do you see is
like the main
sort of benefits or trade-offs between
them because they're kind of quite
different philosophically
yeah i just feel like um
graduate is incredibly easy
machine learning demo you know like you
can build your machine learning model
demos in like minutes because it also
has built-in uh components
for as for streamlid it has this basis
and it covers you from exploratory data
analysis to the end
uh but the problem is if you want to
if you want to um if you want to use
something additional like spacey
components or you know video components
you you need additional dependencies for
them
so i would go with um if if i were to
demo some data visualizations and stuff
i would definitely go with streaming it
also provides a lot of flexibility
but if i want to wrap up a
very easy
demo of my machine learning model i
would use gradio we we recommend both of
them and both of them are awesome it's
just there are some trade-offs
in both of them
cool and exactly
another question might be that um
when i'm building say a project in the
in the course event in the next couple
of days
um most people who say are using
transformers they're pretty familiar
with the pipeline function which wraps
all of the post-processing and
pre-processing
um but i think i saw in in one of your
demos that you were
actually kind of calling the back end i
think you were you were using something
um maybe on the inference api and i was
just wondering if you could kind of
explain about that and why would i do
that instead of just using a pipeline
function
to be honest i just saw more flexibility
by calling from pre-trained than colon
pipeline or inference api i just wanted
the model to be cached and
just
ready to use and also when you do
streamlined cache i think it's cached in
container
so it's like it it loads really
it it loads really um fast but you can
also use pipeline it's it's very um
it's very easy to write your inferences
with pipeline and also inference api i
totally recommend them i have bought by
the way like i have done both but for
this one
i just wanted to show more code that's
why i actually picked the uh one that i
have written with pro from pre-trained
yeah i i think one thing that i found is
um if you have a lot of models like for
example right now i'm building a
streamlit
application which has like 10 like
different models
and this is maybe a case where the
inference api is really useful because
you can't load in the yeah of course of
the container
like
people are demoing their models in
streamlight by putting a drop down and
in that drop down there are like
multiple models like for right with
transformers if i were going to
uh do with you know gpt to you know
excel net and other generative models i
would definitely use inference api but
for this one i only had gpt2 that's why
i also picked it but i i i have written
this for my blog post actually and i
have explained all of the code there
so that's why i actually wanted to demo
that then that was my reason for that
particular
one cool
um maybe one last question is asking
everyone this uh what do you do at
hugging face so what what is a developer
advocate at hugging face and maybe you
can just explain a little bit like um
you know that it's not the standard kind
of job
yeah
um to be honest before i landed in
hugging phase i was a huge fan of
hugging face and everyone in here
and
and what i have observed was that you
actually like in hugging face everyone
is actually sort of a developer advocate
empowering the users of hugging face
like i attended this print and i have
learned a lot of things because people
were actually helping me out uh so i
felt like this is what i'm going to do
so that's why i applied for you know
developer advocacy
uh
we basically build tools and do
integrations it's mostly a technical job
by the way because develop advocacy
varies from
company to company
we do integrations uh we
we build content around them so that we
can democratize it currently i have this
project called task
pages and task series
uh it is
it the audience of this project is
mostly the software developers because i
was a struggling machine learning
engineer previously
and i thought you know like if you want
to build a product the things that you
need to know are actually use cases
metrics you know a little bit of them
uh explain simply
um the models that you need to use the
data sets you have to find you know like
all of them
need to be like in one place as a task
in my opinion at least
so that's why the first time i arrived i
decided i'm going to do this because i
know that there are people like me out
there who are struggling to build
machine learning products
so
this is something i'm currently doing
this is more of a democratization effort
for software developers
yeah like my job consists of doing
integrations and this mostly and doing
um
doing
just getting out there and tell everyone
what we are doing actually i am doing
talks on tensorflow and spaces like this
one
awesome
thank you there any more questions
yeah i'm good thanks a lot for for your
talks and for all the answers to the
questions marvel
thank you so much for hosting me bye bye
bye should we introduce the next speaker
yes by all means
great so we've got him
hi guys hi everyone thank you so much
for having me here
it's a pleasure thank you for joining us
i'm sorry for the you know collapse of
the internet
before your talk
um
i i wouldn't want to be a stream yard
engineer right now um i would have been
probably would have aged 10 years in one
day
that's great you've made it and so um
i'll just uh introduce abu bakar to
everyone so
abu bakar is the ceo of gradios so this
kind of cool applications that
merve just showed us
the kind of brainchild of abu bakar and
his team
and in his role as the ceo of gradio he
works on making machine learning models
easier to demo debug and deploy
and today he'll talk to us about
building machine learning applications
fast
so i'll share the video and then we'll
do a q a afterwards
hello everyone my name is abu bakker i'm
one of the founders and ceo of gradio
and i'll be talking about how to build
machine learning applications fast
before we get started
a decade ago mark and rayson famously
said that software and software
applications were eating the world
fast forward to 2021 and it seems like
machine learning applications are eating
the world this is a screen grab of
hugging face spaces and you see all of
these different machine learning
applications and demos that were created
and deployed on spaces just over the
course of five days
there are machine learning applications
that span things like education machine
learning applications for entertainment
for productivity are being able to find
things more easily and even
accessibility so let's take a look at
just a couple of these applications
so for art
this
demo this space was recently built by
asenkalik and what it does it takes it
as input a photo a realistic image and
it converts it to a
uh
another image that that is essentially
the same image but it's like a
hand-drawn version of it it's called
animegan
and it basically does automated
rotoscoping this is really cool because
something like this takes a lot of time
for people to do manually and now you
have the ability for ordinary people
taking their own photos their own
profile pictures and creating art from
the from those photos and from those
pictures
uh so people you know we once we uh
built and released this demo we saw
thousands of people across the world
really using it on pictures of
themselves pictures of their babies of
their pets and even ordinary objects
that's pretty cool machine learning here
has the potential to bring
artistic you know ability and talent to
anyone who has a particular vision of a
type of art that they want to be able to
create
let's look at another application this
one has elements of productivity into it
uh so here there's a there's a space
that was built by ben decoder and what
it does it uses machine learning models
to take a news article so you just put
in the url of a news article it scrapes
the content and it then summarizes it
for you it gives you a one paragraph
summary so for example in this case you
have
you have you know an article about ai
powered hiring and kind of a summary of
the bias that might happen as a result
that's quite interesting
and this is really powerful because
again it makes it a lot easier to con to
consume content that might be you know
very long you you have articles you have
you know pages books whatever you want
to read you can get summaries of it
first and then if you're interested you
can kind of dive more deeply it makes it
possible to consume a wider array of
content
so the reason i bring all this up is
because we're at a very interesting
point in the usage of machine learning
early on it was you know academics and
hackers that were using machine learning
uh
and you know over time it became you
know machine learning engineers software
engineers people with some sort of
dedicated expertise that were using
machine learning oftentimes in a
business context
and now what you have is you have anyone
who can use a gui or an or a browser can
now actually take advantage of machine
learning again to create art or to learn
things or to build productivity tools or
use these different models that have
been created
that's super powerful especially if you
compare it you know in many ways it it
is parallel to the development of
personal computing in the early days of
personal computing you had really only
maybe hackers that were you know very
kind of on the on the cutting edge of
technology that we're using computers
over time became more dedicated
engineers especially in business context
that we're using it and you know ever
since the introduction of the original
you know maybe perhaps apple uh computer
with the gui
the kind of a mass audience can take
advantage of personal computing and the
mass audience certainly has taken
advantage of personal computing and you
know the the
developments and the possibilities have
been you know i think
really unforeseen by by anyone at that
time and we're at a similar stage with
machine learning and you know as kind of
you can tell here we went a long way
from those early stages of uh
of those guise and i think the same is
probably going to happen with machine
learning as well we're pretty much at
very much at the early days of this
of this widespread adoption
so with that being said i mean given how
much promise machine learning
applications have
uh how do we actually build them how do
we actually build them well it's not so
bad just take your machine learning
model that you've trained
in python in tensorflow or pytorch or
jacks or maybe with some a library on
top like hugging face
you go from there then you
build a web application on top of that
you containerize it with docker you find
a hosting solution like aws and then of
course you want to build an interactive
user interface so a lot of different
people can can use it so you learn some
web scripting you put together a little
gui and then people are using it people
are submitting data you might want to
store that you can see how the model is
doing and improve it so you build a
mysql database
so all of that
is needed just to build a machine
learning application
um or at least it used to be now we have
much better solutions particularly if
you want to build machine learning
applications fast we at gradio have
abstracted away a lot of these different
steps deploying your model building a
gui throwing incoming samples all of
that can be done with with gradio and
the beauty of it is it's all done in
python which is a language we machine
learning engineers uh use on love
so how does this actually look well let
me show you a practical example and i
won't go too much into like a tutorial
around radio there's lots of resources
online if you're interested in that but
i just want to give you kind of a taste
of it so you go ahead and you import
gradio because gradua is a python
package
then you instantiate a gradio.interface
class so this is this interface class is
the core class in gradio and it takes in
three arguments the first argument is
the function the python function
which is usually like a wrapper on the
model that you want to deploy
and then the next two parameters are
what kind of input and output you want
so in this case i want a drag and drop
image interface that feeds into my skin
image classifier the classifier returns
a dictionary of of confidences of
predictions and that gets outputted as a
label
so let's see what this looks like in
practice but before i show you what it
looks like i do want to mention that
when you launch the you can write all of
this code to create this interface and
to launch it and you can run this from
any environment that you can run python
on so you could run this from a python
terminal for sure you could run this
from a jupyter notebook jupyter lab
or a collab environment even kaggle or
rublet anywhere you can run python you
can run this which is which is very
powerful and what you'll see is it'll go
ahead and create an interface like this
and this interface here for example you
can drop in an image here and you can
see the prediction you can even edit it
and you'll notice this edit button over
here this allows you to adjust things
like the brightness or the crop and you
can see how the predictions change in
real time
so
this three lines of code produces this
interface here
of course you can modify things we don't
just support images or text we support
things like video audio
input we support as output html json
files all sorts of different things so
here's
another interface that you could create
around video gpt so this is a model that
you put in a video and it tries to
predict the next frame of the video so
again super easy to wrap a an interface
around that
if you ever want to build a more complex
model that takes in multiple inputs or
multiple outputs that's also super easy
just pass in a list of those inputs or
list of those outputs and then again
same thing just launch it and it creates
this
now you may be wondering where this
application is running and by default it
runs locally so it runs in your local
host
and you can kind of you know run do
everything locally and you can test it
and your data doesn't go anywhere
but in many settings you actually want
to be able to share your model with
somebody else and so by just adding one
parameter here share equals true
what this does is this creates a public
url you can see this here
95338.radio.app here it creates a
temporary public url which allows anyone
to actually try your web application no
deployment you don't have to worry about
deployment or dependencies or anything
like that
you can just pass this link around like
a google doc link and then someone else
will be able to try out your model this
means you can take that model that
you've built and you can share it with
your users with your collaborators with
your boss with your teammates with your
parents anyone who you want to be able
to try out the model and what they can
do is they can you know drag and drop in
their own images and see what the
prediction is like or if they're on
their phone the ui automatically adjusts
to be mobile responsive and instead of
dragging and dropping an image you
can actually pick one from your gallery
uh or to use use the the phone camera to
take one as well and just in real time
you can see the prediction so this is
really powerful and of course this
applies to all the different kinds of ui
components that we have
um so yeah so this lets anyone try out a
model immediately without having to have
worry about software dependencies
hardware dependencies because your model
continues to run locally so the the web
ui gets deployed but gradio handles the
message passing back and forth meaning
that the person on the other end doesn't
have to worry about any dependencies or
really even need any sort of technical
expertise they can just try out your
model
if they can open up a browser window
if you want more permanent hosting we
recommend trying out hugging face spaces
that's the preferred kind of deployment
solution hugging face has made it super
easy to deploy a machine learning model
and you can kind of see how that process
looks like in this gif
i won't go into the details here but you
can there there's a great blog post on
the hugging face website on how to
deploy gradius spaces
and once your app is up on the hugging
face spaces a lot of cool things happen
first of all
now you can actually use that demo that
you've built or anybody else has built
and put up on spaces you can use it like
a function in your regular code so that
new summarization model i mentioned
earlier built by ben decoder i i can
load it as a gradient interface here i'm
loading it just by passing in the spaces
path to it
and
now i can then use it as a function i
can now summarize new links here i've
had it summarize a cnn article uncovered
okay and of course you can do this with
text
in demos we can also do this with image
demos by passing in the url to an image
or the file path to an image or you can
do this with video or audio or even
multimodal by passing in multi
like different parameters corresponding
to the different inputs
if there's a space
up on
hugging face spaces that you like if
there's a demo that you like but you
want to tweak it a little bit to suit
your use case you can do that easily so
for example here's a cool demo that
removes the background from an image i
like that but maybe i want to attach it
to my webcam instead of a drag and drop
image interface well i can just add one
parameter here that overrides the input
to be a webcam i launch it again and now
hey look at that now it's connected to
my webcam and i can now run it run it
with my webcam and confirm that it works
okay you can also mix and match spaces
now this is pretty cool so that new
summarization model maybe i want to
connect it to a model that translates
from english to german and so i can put
them in series
and now voila i have a
a model that takes in a url
and converts it to a summary of that
news article in german
so you kind of see this here so it's
taken this alpha fold article and it's
summarized it and then converted it to
german so i can now put
demos
spaces in series or even in parallel to
compare different things and so you can
read more about that on the gradio
website
but the biggest thing of course why
build machine learning apps and why i
deployed them on spaces the biggest
thing is it completely democratizes
access to machine learning
any
of the 4.7 billion people connected to
the internet that's a huge number can
now use your machine learning model
what bigger way can we actually
democratize the the potential of machine
learning then making our models our
applications accessible to the wider
world
all right with that if you'd like to
give gradio a spin
you can pip install it and then you can
run some sample code that we've provided
here
and you'll see that you'll be able to
spin up an interface in no time if you
want to see more guides like this then
visit our website www.gradio.app
and thank you very much my name is abu
bakr abid i'm on twitter as abid labs
a-b-i-d-l-a-b-s
if you'd like to reach me that's
probably the best way
or of course reach us through the
website the link that i just provided
great welcome back no okay
really cool talk it's nice to to see
this like high level summary of of kind
of the motivation behind gradio and also
the
the cool like modalities and different
applications
thank you
i think you have to actually have lived
the pain of building your own
application to really appreciate how
amazing this is because for example i
think my very first ml model was like
wrapped in flask
um and i had no idea what like flask was
i didn't even know what like web
applications were and you know this is
kind of like not your domain really as
an engineer as i mean maybe it is but
like
you know you should really be focusing
on the ml part not the
not the front end and radio is i think
really awesome for doing that
exactly
exactly yeah now i've definitely lived
the pain a lot i think during my phd
actually several times i had to do this
and it's not it's
it's not just about the fact that it's
hard i think the fact that it's hard
that it's hard also prevents
people from like making sure that their
machine learning applications work in
the real world as well that makes sense
because people know that okay i can
publish a model it doesn't really have
to work as long as it hits certain
metrics
but i think
having this kind of making this the
normal workflow you've built a model
you're going to deploy it um i think
make sure that you know a lot of issues
that we talk about with bias and
robustness actually get surfaced and
then hopefully solve as well
yeah exactly and i think one thing
that's really like exciting is that i
mean most of the time the machine
learning models that we build they serve
they should ideally serve humans in some
way yeah and
you know a lot of the time the person
who's interacting with the model
it really helps if you can if you can do
that right because you know just
showing like a table of light
performance you know exactly
stuff like this this doesn't really
excite anyone and especially when you
work in industry like the the end users
who are often domain experts and so on
they really appreciate like being able
to click buttons and like say oh not
just like does it work but like i've
i've experienced people doing
adversarial attacks online absolutely
absolutely no i've seen that i've seen
that so often actually i remember the
first very first application upgrade
which is you know where we built it it
was for a
a model that looked at these like
ultrasound pictures of the heart and it
could detect different things like if
there is a pacemaker in the heart or
what not and i remember like we
presented it to this cardiologist and he
was skeptical like our numbers were very
good but he was skeptical and then he
like adversarially tested it he like
edited that pacemaker out of it and then
the prediction of the model changed like
from there's a pacemaker that there's no
pacemaker and when that happened
multiple times like the cardiologist was
actually convinced and even us like the
machine learning engineers in the room
we all breathed a sigh of relief
because one thing to have like good
numbers but another thing like when it's
actually stress tested it works that's
actually a really good thing i think
that's kind of the other piece of it is
one
these guis these demos reveal if there's
issues of bias and robustness but then
also like if it's actually a good model
it helps build trust in the model which
is of course very important in these
kind of interdisciplinary machine
learning applications
awesome
so we've got a couple of questions uh
i'd like to maybe ask so one is here
so someone's asked
um gradient supports a lot of modalities
like text images and video what's the
next focus for the team so yeah i don't
know what's beyond the space of these
three but uh maybe you have some cool
ideas
uh definitely yeah so in terms of
modalities uh there's a few modalities
we're looking at right now for example
3d images
a lot of fun work being done there
um but uh
kind of just more broadly speaking uh
there's you know there's a lot of like
enhancements we're doing the library but
and the next big thing we're looking at
is thinking about how to actually scale
up radio demos which are designed mostly
for like single sample inference like
you put in one sample you run it
to more
fully fledged applications where you can
take the same you know function whatever
model you have and then easily build a
gui around it that can actually handle
you know batch you know large volumes of
data of course this is obviously a very
important use case
um
yeah in a lot of different settings and
so we want to
and like a lot of you know repeated work
happens and making these applications
and a lot of different contexts so we're
we're talking to folks right now on how
to handle like bigger volumes of data
what are what do those use cases look
like
and
so if if you're if you work you know if
you want something like this like what
gradient does before you know bigger
volumes of data reach out to me
definitely would be happy to chat with
thinking about how to implement that
awesome that actually segues into like
another question which would be like if
you're operating in a in a enterprise
context
a lot of the time there's questions of
security so like yeah
maybe i need to put like a password on
my application yeah have any plans to
support these features or do maybe you
do already yeah yeah so we so we do
offer authentication so with one extra
parameter in the launch method auth
equals and then you can specify like
username password so you can definitely
do that
um
and people do that all the time uh i
think the the more the very common use
case that we see is people want an
on-premise type solution or or something
that runs entirely on their vpc so like
the radio servers are not included at
all and for something like that we offer
something called radio private um which
is essentially kind of you can think of
an on-premise version of gradio and we
work with you know enterprises different
companies to make that happen
yeah cool cool thanks there's another
question we have here um
so gradient looks great for people who
know how to code do you have plans to
make it even simpler for people who
don't
yeah
so that's a great question um so we
we actually so that is a really good
question i i at this point i think the
general kind of assumption that we have
is that
people who are using radio right you
know do have some kind of familiarity
with uh basics of python and we just
want to make that a lot easier for
people to be able to build these
front-end you know applications you know
the idea is that you know python but you
might not necessarily know these
front-end applications you know
front-end languages or scripting and so
on
um
as of now i think we'd have to think
pretty deeply about
what that kind of use case looks like
that sounds interesting you know making
it completely kind of drag and drop and
all that that could be interesting we
don't currently have plans but we can
definitely think about that more
yeah cool
yeah like i think the whole like no code
like um sort of like domain it's a kind
of a tricky one because
like on the one hand you want to empower
domain experts to like actually say
build their applications but then if you
think about it like what does it mean to
build an ml application and then you
have this whole like like knowledge that
you sort of implicitly assume of the end
user like what is a model what is the
prediction
and metrics all this kind of stuff
exactly
and and i think yeah it's not quite
clear like where the the level or the
you know the entry point is um yeah
absolutely exactly so we'd have to
definitely think about that a little bit
more but it's definitely intriguing
yeah
cool there's maybe one last question
i've seen on the forums which is kind of
a bit related um so will gradio come out
as a friendly desktop interface in the
future sometimes the model can't be used
publicly but it's instead used
internally in a company yeah uh
absolutely so that's where um i think
the radio private that i mentioned you
know briefly can be useful i think web
interfaces is fine as long as it's
running you know either locally or on
your internal network which is what
radio private aims to do so if that
sounds interesting or if that doesn't
cover kind of the secure need or the
internal lead out i'd be happy to hear
more but uh but yeah we work with a you
know a bunch of companies to actually do
this internal use case even like folks
that are doing medical you know very
sensitive data and everything just works
you know internally and everything is
encrypted so usually that that satisfies
that constraint but it's a it's a good
question i'd definitely love to to hear
more uh from this particular user so
feel free to reach out to me
yeah cool awesome
well thank you
thank you
well done right last
no thank you so much you know we we're
working on a lot of very exciting
integrations the hugging phase
specifically i mean there's just such a
natural kind of integration there with
spaces and so on so i really appreciate
you guys you know inviting me over and
and uh good luck you know enjoy the rest
of the event to all the viewers
thank you bye-bye
all right thank you becca
shall we introduce next week
yes i think we're ready for next speaker
hi matthew
hello
hello so matthew devi is our next
speaker and matthieu is a solution
architect at aws he is a technology
enthusiast maker on his free time and he
likes challenges and solving problems of
clients and users
works with talented people to learn
every day and today you will talk to us
about the aws ml vision which is making
machine learning accessible to all
customers
add the video to the stream and then
we'll just go back to the q a
hello i'm matthew solution architect at
aws thank you for joining and i hope
that you will enjoy that session today
we will learn the adobe's vision and
services for machine learning and how to
convert a notebook to train on sagemaker
you will need
during the presentation to connect to a
temporary degree second so we will get
to that so let's jump into it the
adwords vision is to make the machine
learning accessible to all the customers
if you are developer data scientist of
business user doesn't matter we have
some service for you
the adobe sml stack is composed of three
layer the bottom layer is all the
infrastructure where you can find the
gpu the cpu the imi for the most popular
frameworks on the top layer you have the
ai services
it's the machine learning model that is
bundled as an api so you can leverage
them to answer your application and
provide some amazing feature for your
user and the middle stack is the sage
maker and we will go deep into that
this year we've signed a partnership
with ugginface that will reinforce the
adobe's vision
and that partnership focus on nlp and
how to make it easy and accessible for
everyone
with these new partnerships you can now
leverage the dlc container that is
developed with agin face to train and do
the inference for your favorite
frameworks
we also provide an estimator directly
integrated in the sagemaker sdk that
helps to deploy nfp script and train it
on sagemaker
we have a comprehensive
gallery of example
that you can find online
and support wise it's maintained and
supported by adobe rs
so now that you have an overview of the
adobe software
let's dive deeper into the sagemaker
architecture and how it works
everything starts
with a
notebook
it could be something else but
usually
it's what we have that notebook can run
on sagemaker or on your laptop
inside that notebook you will use the
sagemaker sdk with the sagemaker nikkei
you will call the method fit on your
estimator that will
launch
a training job
this training job will leverage
a nicer
registry
where all the
dlc for all the frameworks are stored
we will train and output
the model
to
adobe's
s3 so now that you have your model
you can
then deploy your model using the sdk
so you will call
the deploy method
and
that will create
a model
inside changemaker
that will reference
the model that you have stored in s3
from there
the sdk will continue by creating
an endpoint configuration
and then an endpoint
of course this endpoint
will leverage the endpoint configuration
and the one configuration will leverage
the model
then
the
the
the endpoint will pull the image
from the registry to serve your request
and your users
can access the inference from the
endpoint
so now that you understand the sagemaker
architecture
let's do an example by modifying an
existing notebook
to train it on sagemaker
so today we will use that particular
repository as an example
so i already check out on my computer
and as you can see in the train
notebook
we have a
older notebook with the import
we read the data
we define the model
and the tokenizer
we process the data to split it
after we just create
the data sets
we compile the model with keras
and after we call the method fit
after
the training is done
we save the model
and we do a bit of evaluation
so the goal today is to take that script
and convert it to search maker
in vs code i can just say export to
python script
and here i will remove all the
unnecessary commands
now we have a clean script when
sagemaker will call your script he will
pass a couple of parameter with to know
where is the location of your data and
the output of your model let's parse
those arguments
for that we will need to import a couple
of library
and create a function wrapper
then we will need to use this parameter
inside our script
as you can see there we have a custom
hyper parameter that is called epac so
we will use it also to dynamize our
script
last thing is the model evaluation
we don't have a specifically an
interface because we are in script mode
so we will need to export a json with
the model to be able to evaluate it
now we are good
let's just save this file
we'll create a new folder called script
and
create the trend
that be
file
or script is ready so we will start a
new notebook and use the sagemaker sdk
to create and train our model
first thing we need to import the
sagemaker sdk
and
create a couple of variables to store
the session the whole execution the erm
role that will be used by sagemaker to
train our model
the bucket where we will save our model
artifact and a prefix inside that bucket
to make sure it's well organized
for the moment or data live inside the
project so we will transfer them to s3
to be able for sagemaker to use it
inside a training job
we're gonna import the bottle free sdk
and upload it to s3 we can start using
the sagemaker sdk to create the aginface
sc matter
there you can see that we specify the
endpoint with our training job
we precise the source directory that is
script all the code inside script will
be
send to s3 and host inside the deep
learning container
we were gonna use a p3 uh two weeks
large for that training
only one
and we can precise uh the frameworks
version
here you have your hyper parameter where
you can pass some custom variable to
your script
like the import one
now we have everything so we just need
to call the fit method on our
exterminator
inside it we will
pass a channel called train with
the information and the pastor or data
in s3
so let's run the script
no credential error
it's true we didn't set up or adobe a
second you should have received an email
with the necessary information to
connect to the event engine platform
so from there
copy your team hash
and accept the term
login
for that session we will use
the email one-time password
type your email
and pass the code that you receive
inside that email
you are now connecting to your team
dashboard so you can click the adobe's
custom button
and
click open the adwords console
there you have access to the console and
we will just set up and go to the search
maker
page
but because we are running our script
locally inside the s code we will need
to have some credential to export inside
our environment
for that just come back to the event
engine tab and grab the key
here
switch back to vs code
and create
a dot
of file
where you will print your variables
remove the export
in front of your variable
register it so now we need to load our
environment inside the notebook
so create a new code session
and
and load it with a dot amp
and clean all the output and run or
notebook
now your job is launched inside
sagemaker so let's see it
so inside save maker
go to training and training job and
there you have your training job
running so you can see the details
but also you have all the logs coming
inside your notebook let's wait the job
to complete
[Music]
the training is now complete so let's
see in sagemaker what's
happened
so we can see that the statue is
completed and when you scroll on the
bottom you will see the output so click
on it
and there you have the two
output artifacts from the model
the model himself and the output where
you will have
the evaluation so let's download them
so inside the output
zip
we have all the information about the
model
we also have wall model
with the h5 files the tokenizer and the
with appearable right
so as you will have if you train it
locally
now that we have our model we can easily
deploy it with the search maker sdk
so for that
create a new cell
and from the estimator called the deploy
method
you specify the initial
content stance
and the instance type it will create a
predictor for you
that process will take a couple of
minutes just the time to deploy the
endpoint
[Music]
deployment is complete so let's see in
sagemaker what has been created
so if i come back to sage maker
you see in the inference part
that we have
one model
pointing
to
or model artifact
a new endpoint configuration
with all model selected there
and for traceability you also have the
training job that produced that model
the instance
and the initial count
and finally
the endpoint
with the http url for that
so now that everything is in place we
just have to test our own point
for that
we will
add a sentence
from a movie comment
and use the predictor to predict over
that sentence
and we see that it's positive
and 99
if you don't use anymore your own points
to don't be sharp for it
you can finish that workshop by just
deleting it
for that use the predictor delete
endpoint command
i hope you enjoyed the session and thank
you for having me i cannot wait to see
what you will do with sagemaker
okay welcome back that's it
thanks
so
i think uh everything is still online
and it's new out so we're still we're
still alive great
um maybe
i think there's a few questions i saw
maybe you want to ask matthew about his
talk
sure
um
so one first question is um
so it seems like sagemaker stores the
fine tune model and s3 is it also
possible to push the model to the egging
face hub
yes of course you you can so the the
script that you will run inside
sagemaker is a python basic pattern
strip so you can add your credential in
on by on manual from the hub and push it
to the the the thing that you will lose
uh with that is uh to be able to deploy
the endpoint with the sagemaker
deployment point
method because
you rely on the
on the fact that the model is in is free
got that
that makes sense and speaking of those
end points do the endpoints deployed by
sagemaker automatically scale under i
load
so you need a bit of configuration for
that to to create an alarm inside
cloudwatch based on the metric that you
will define and the best way to to do it
is like to test your model uh in the
load testing environment to define uh
like this threshold maybe you will have
thousands of requests by second that can
be handled by one instance
and you can create an alarm from by
instance request count
and it will automatically scale against
that and you can precise the maximum
number of instances and the the minimum
number
nice
going back to the models that you used
uh can i fine tune any patron model in
sagemaker or only some architecture
supported
so all the architecture that is
supported by again face can be trained
on search maker we directly partner with
a ergin face to to to include
the transformer library and the data set
library directly on the dlc so
everything that is supported by those
libraries will be supported in sagemaker
very cool very cool
another question that we just received
on the forum is uh is there anything
sort of word like tool to monitor model
training on search maker
yes we we have something that is called
model monitor
so you put some call back and you attach
some code back during the training and
you will
just send the matrix to it and you have
reports inside uh the sagemaker studio
id
where you can have that and of course
you can leverage tensorboard also and
weight and base is connected to it
nice
a question more related to the to the
event for the next three days
what kind of gpu slash instances can we
use during the event
so you can use the the p3
instances
but you can run
into some issue of availability
with them so if you re encourage those
issue you can switch to a gcat dn
that will be a bit slower
but also it's like the new gpu where you
can leverage uh the fps 16 so so it
could be also a good fit for training
the models
and also something that i didn't precise
because we didn't send the email with
the invitation to the event engine so it
will come
shortly when we have the team that are
built and so please share your email
inside the google form and so we can
invite you uh to those icons
same thing so it's a good time luis to
show again where where we can
fill that form if you want to share your
screen on the forums sure one second
participating in building projects for
the course then we have all these
different um project ideas on the forums
but um this is all under the the course
category or the course projects category
and the thing that you'll want to look
for for information is this post about
the course projects category
and here there's the information about
the kind of rules for the team numbers
and you'll find here that there's a form
that you need to fill in to get access
so if you click on that link this will
take you to this google form you just
need to put in your email
and whether or not you want to get some
goodies from aws
and then you just provide your username
on the huge face hub so we can
associate you with your account and then
you just provide here the name of the um
the team that you're working on
so that we can then figure out which
people are working together
so once you've done that you'll then be
given access i think from tomorrow
um to
create your sagemaker jobs and then the
last thing you want to do is we have a
sagemaker community
organization on the hub
and this is also where
if you want during the training you can
push your models here so
i think we'll learn about this in the
next talk
and then you can also deploy your spaces
here if you wish you can deploy your
spaces on your own account but if you
want to put them here you're welcome to
do so
i don't think
we had any other questions there is
unless you had uh
got something in mind
um no i think i think that's uh that's
it was a super nice talker i like the
dancing
thanks
he make a bit of fun
yeah definitely
thanks a lot for joining us today
yeah
and uh
so let me
remove you from the stream and invite
our next speaker here is do you want to
introduce philip
you freak with me
hi everyone
hi philip hi philip
so
um
for those who don't know philip um he's
the last speaker for today it's philipp
schmidt and he's a machine learning
engineer and the tech lead at hugging
face where he leads the collaboration
with the amazon sagemaker team
so if you have any questions about
sagemaker in the next couple of days
just post them on the forum and i guess
philip will help you there
um he's also really passionate about
democratizing and productionizing
cutting-edge nlp models and improving
the use of them for deep learning
and today he's going to explain to us
how to do managed training with amazon
sagemaker and and hanging paste
transformers
so i think here we're going to actually
see
how to push models to the hub which you
know most people i think are interested
in learning so that then they can use
those models for building their spaces
so i'll share the um the video and then
we'll come back for a q a
hi everyone my name is phillip i'm the
tech lead for our collaboration with aws
and amazon sagemaker today i'm happy to
talk to you about managed training with
amazon sagemaker and heightened phase
transformers i will show you in a few
minutes how you can train and
automatically push your hiking phase
transformer model using amazon sagemaker
to our hiking phase model hub for those
of you who don't know amazon sagemaker
amazon sagemaker is a fully managed
machine learning service that provides
everyone with the ability to build train
and deploy machine learning models
quickly let's get started
um i already started amazon search maker
notebook instance which is like
managed jupyter environment by aws you
will get access to the notebook
i guess through some resources it's on
github so you can find it pretty easily
in the beginning i included a few links
so we have several
resources and documentation about how to
use amazon sagemaker with hugging face
transformers we have a special
documentation we have a lot of examples
we already did a few workshops over the
cast a couple of months maybe some of
you have been there and then we have a
dedicated search maker section in our
hiking phase forum so if you are
participating on the community even the
next couple of days feel free to ask any
question you have in this community
forum sections okay
let's get started so what we are going
to do is we are going to train a
multi-class text classification model
using transformers and data sets of
course and as data said we are going to
use the emotion data set and as model we
are going to use the
distal bird uncased
the first step is of course we need to
install our
dependencies in this case we are using
the sagemaker package which is the
sagemaker python sdks to interact with
the sagemaker platform behind the scenes
then we have transformers and a data
sets version to pre-process our dataset
since i ran this already i have my
dependencies stalled
and then
we can jump into the next section which
is permission so
um this notebook can be run inside of
aws using amazon sagemaker notebook
instances or the sagemaker studio
then searchmaker provides a nice way to
get access to permissions so if you want
to run your training on sagemaker of
course you will need permissions to
access your data on s3 or to start your
training job with the correct instance
type since we are running inside of aws
i can execute a code sale which
basically gets our permission role gets
our s3 bucket which we will use to store
our trading assets and then the region
we will run in
if you are going to run
or want to run sagemaker training or
training jobs um from your local machine
or from a virtual machine or basically
from anywhere else and there's also like
a selling code that you can comment in
and then yeah basically get access to
the same resources as i am
to start our training we of course want
to pre-process
our data set and what's very nice about
amazon sagemaker is that
we can split up pre-processing and
trading into two different parts and
then for example only um or have the
pre-processed data set already available
when running our training so what we are
going to do is we pick up our tokenizer
and pick up our emotion data set and um
pre-process it um using data sets
and to yeah our preferred format so we
are loading the data set we are
tokenizing it we have like a small
little helper function for it and then
we changing the labels and
setting the format for our
columns
and then what we are going to do is we
are going to leverage the s3 file system
integration into data sets which
basically allows us to save our
pre-processed data set to amazon s3 to
then later use in
our training job so we don't need to
pre-process our data set again when
running the training so that's pretty
nice so we can upload our data set which
is then the error file created by the
data sets library as well the sd
configuration files
after we have uploaded it we can get
like to the nice part to create our
estimator and start our training
the hiking phase sd major is um
yeah basically a python class which
includes all of the needs and grits you
need to start your training so when we
scroll down a bit here we can see the
hanging face estimator which says okay
we want to use our drain.pi script
it is located in our scripts folder we
want to use a p3 instance which has a
nvidia gpu mounted we want to use one
instance so if you want to run
distributed training you can easily
change the number to two and then you
can leverage multi gpus and multi-nodes
and we have the name of course for our
job which will be used to look into the
results afterwards we have our role
which we
got in the beginning to have access to
our data sets we have uploaded to s3 and
then we have our image
which is basically the container where
transformers and pi torch tensorflow
data sets is installed which will be
used for our training we have a python
version and then the hyperparameters and
the hyperparameters
are parameters we can pass into our
training job so you can imagine that
when running a training sagemaker will
start the ec2 instance
which starts starts our container and
then executes
our training with python3
in this case drain.pi and then provides
the hyperparameter we define here as cli
arguments so
the key would be the key in our argument
and then we have the value so the
training script we are going to use has
an aux passer included which accepts all
of our training parameters and then
this can be used inside our training
since we want to push our model to the
hanging phase hub we can use the hiking
phase hub notebook login and then
provide our um pm required hyper
parameters to push our model to the
hiking phase hub which is the push to
hub
argument of course then the model id of
our new created repository when we want
to save our model since we are running
like one epoch um
i guess it doesn't make that much sense
to save every but if we are going to
change it to free
then um the trainer would save after
each epoch
our model to the hiking phase hub and we
can like directly interact with it but
let's keep it for now with one so the
training will run faster and then of
course our hugging face tool
so we can log in
that's my username
copy my password
hit login
and we are logged into our account
we set our hyperparameters we create our
hugging face
instant class and get an error because
we forget our comma here
create our hiking phase estimator and
then what's nice
so to start our training we can
render.fit method and you are probably
aware of that we have pre-processed our
data set inside our notebook and
uploaded to s3 where we defined
variables to the location so our data
set for
the train is saved in this s3 location
and we can now provide the s3 ui into
our
estimator into the fit method and then
stage maker will load before the
training starts the data set from s3
into our environment and we can access
it directly from the file system
so we can start our training with
executing the cell i intentionally
provided a weight faults method
which tells the sdk to not wait until
the draining job is done if we would
remove it then sagemaker would basically
connect to the ec2 instance and we would
see the um we had a comment like the
output of the training job the logs
everything directly inside of the
notebook but since we provide our token
um i guess it's better to provide faults
and rather look up the training job in
the management console additionally i
have a waiter script which basically
checks if the drainage job is done so we
can execute it and then the cell will be
completed after our training job is
finished
and then we can access our model by
running this nice little code which yeah
dynamically creates our
interface url we have our username and
then the model hub id we provided
earlier and then if we jump into our
model page
and i run the training already the
trainer and sagemaker has created a
nicely model card with our accuracy and
the loss we can see okay which model we
have to train on which data set
and we can see the hyper parameter which
we have provided
using our hyper parameter dictionary we
can see our training results we run it
for one epoch we have the framework
version super nice we can run the
inference directly since it's already on
the inference api available
because we can while it's loading
checking into our files and here we can
see basically everything is correctly
saved by our
training job and by search maker and we
can share the model with our friends
and then the key user can use it
perfect
and then
afterwards there's um
yeah
like visual on how stage maker works you
can like look into it to understand a
bit better what's happening but the easy
way is basically we have a huge face
deep learning container or training
container which includes all of our
libraries we need we have our data set
on s3
and then when starting our training job
with the sdk stagemaker will start our
instances pulls the dlc runs the dlc on
the container with our script loads our
pre-trained model from the hiding phase
hub
loads our data set from s3 and then runs
the training and at the end
you can also save it to s3 and then
deploy directly with sagemaker but we
like cutted the step and saved it to the
hub and then use the inference api for
it and then additionally i have provided
a nice little example on how you can use
and leverage already existing training
script so i guess you are familiar with
the hackingfast transformer repository
and in there we have an examples
directory which includes a lot of
scripts for a lot of different tasks
including nlp vision
speech and all of those scripts are
compatible with sagemaker so
to use them sagemaker provides a git
config
where we can point to our github
repository to which branch we want to
use and then we can basically run again
um our estimator and run the training
job
and then the example script will be used
directly from github and you can only
need to provide the hyper parameters and
don't need to write your own training
script if there's already an existing
one for you use case
thank you for
listening i hope to see you all in the
community event come in the next couple
of days if you have questions about
sagemaker or run into some issues
feel free to contact me
and looking forward on what you will
create
and we're back
thanks for that nice talk phillip
it's uh super cool to see that you can
push your models to the hub by kind of
using the same
parameters in the trainer
just the hub model id and the name super
cool
speaking of that we have a question
which is
does the sagemaker estimator leverage
the trainer from transformers
how is the training being done is it did
you did you reinvent the trainer
yes
no
like it really depends on what like
flavor of transformers you're using so
material like the talk before you for
example used um tensorflow and keras and
for this you would like need to use the
keras callback we have for the hugging
phase hub to push your model to the hub
in my example i used pytorch and then of
course the trainer so it's like uh
yeah a normal python training script you
could like run outside of sagemaker on
your local machine with with like the
parameters and then with the nice
integration from the trainer into the
hiking phase hub we just needed to
provide our like token and
a hub id and then we were able to push
it like automatically i think one thing
we should say and people need to watch
out is that if you are running your
training in sagemaker in an agape
environment so you could also configure
like vpc configuration that
your training don't has internet
connection then of course the
integration with the hub wouldn't work
but i
expect since we are like an open
community and share everything with
everyone and we are not running training
inside air gap environments this week
cool cool
so another question is
can i do hyper parameter search with
sagemaker
and if not what would you suggest
someone does if they want to find like a
good learning rate for example
yeah so they're like two different ways
you could do hyperparameter search by
using stage maker so the first way would
be using the sagemaker sdk and the
search maker like features so sagemaker
supports something called
automatic um hyperparameter search i
think where you can um define
like hyperparameter space and provide
this hyperbaric space to your estimator
and then sagemaker will automatically
run multiple training jobs so um in our
like on my example i provided the
hyperparameters in addition to this you
can provide like the hyperparameter
space and then sagemaker will start
multiple training jobs and keeps an eye
on okay which metrics i'm going to track
and do i want to maximize it or minimize
it and but you can also like go the open
source way and create a training script
using optuna ray or whatever
optimization tool you want to use and
then either run a longer training so you
could add like a loop inside of your
training script to run like multiple
iteration experiments and then like save
them to the hiding face hub or add like
early stoppings into it or you could
like
provide multiple hyperparameters and
then loop your estimator to start
multiple training jobs
cool cool
so i'm going to ask you a personal
question i've been asking everyone else
um so
yeah in your bio it says that you're
like the tech lead at hugging face for
the sagemaker partnership and also some
other projects you know what what does a
day in the life of philip look like at
hugging face
oh wow okay um so i'm from germany uh
this means like
i have um a lot of my meetings are like
in the evening or like late evening i
would say since aws is in the u.s mostly
so my normal morning is starting
catching up on slack and everything
happened during the night since we're
like a global company and then working
on some tasks and normally i go into
or working out um before
lunch and then have like a nice lunch
break and then the afternoon i have like
all of those nice meetings with aws
people or like with the hacking phase
around the world and then
that's basically it
but i would say like what kind of sorry
please yeah i think i would say like
every day is like something new and you
need to be very flexible so you cannot
say okay i want to do like this for the
next five days you like always need to
adjust to the needs and what currently
is important to do
cool cool and i also know you do a lot
of work on like productionizing like
machine learning models or like making
them faster or smaller and essentially
all this crazy optimization stuff and i
was kind of curious to hear your
thoughts on like you know one why does
this matter why can't i just you know
train my model and call it a day and two
where do you see like the sort of most
kind of exciting developments in this
like space of optimization of
transformers
yeah i think like so
you could of course use like your
vanilla model you have created but there
are like always two kind of leverage you
could use the one is maybe your model is
too slow to run inside of your product
so for example search is like the best
use case for it if i'm like a google
user i want to type in and want like my
get result immediately and i cannot wait
like a few hundred milliseconds or even
seconds to get my search result because
then i would start using bing or like
duckduckgo or like some different search
service
and the other leverage is of course the
smaller the model the less compute you
need to use and the less like money you
need to spend on and if you for example
if we take a look at gpt3 which i think
is around 400 gigabytes or something or
you need to have like at least a dozen
gpu to run it which is like way too
expensive for startups and then if we
take a look at a new
t0 plus model from the big science um
yeah organization which is i think 16
times smaller which also means 16 times
less expensive than running gpt free and
i think to be able to
give everyone access we the models need
to be small enough or cheap enough to
run so people can use it i mean nobody
helps it that a model can solve any task
if you cannot use it
yeah yeah definitely
and and i'm just wondering like what are
the um sort of
challenges that happen when you want to
make these models smaller like is it
just like pip install optimized model
or is it more complicated
yeah sadly it's more complicated but
like we are getting closer to just
running a optimize model x or model y
um but it like really depends on the
model architectures you are using and
also on the task you're using so for
example sequence to sequence models are
way harder to optimize since you have a
lot of dynamic into it and you have like
the encoder model and the decoder model
and it's like it's not static at all and
if you want to optimize or like the
easiest optimization step is basically
to trace your craft or basically to know
in advance with computation you want to
do so dynamic models are way harder to
optimize but i think we are making good
progress or like the whole deep learning
community is making good progress to
making optimizations easier and more
accessible
and i think in the future we will see a
lot more of this
and i mean like optimum the new open
source library is i guess one of the
best examples on how to leverage
optimization optimization techniques um
for air transformer models
yeah so for those who are interested um
philipp is referring to
um the optimum library so let's find it
and um
do you wanna just tell us in in one
minute or thirty seconds what this is
about
um
yeah i can give it a try i'm like not
the expert in it but optimum is
basically uh you could see it as an
additional layer on top of the
transformers library for every like
optimization for specific hardware
provider which doesn't fit into like the
normal transformers repository so as a
like best example i would say we can see
a craft car ipu's which requires some
changes to the trainer and to the like
training loop and those changes were
implemented into the optimum library and
then optimum again is compatible with
transformers so you can use graphcore
ipu's to train your transformers model
using optimum and transformers and we
don't need to worry about breaking
things inside of transformers just for
the ipu use case
awesome
so let me just have a quick look at the
forum um
see if there's any last questions
okay so there's one more question um
this is maybe more of an aws question
let's uh just chuck it in the banner
so
the question is can we make assumptions
or comparisons between the gpu instances
that are available for this project on
aws and what a regular sort of colab gpu
offers
so i guess the question is like
you know i think matsuya mentioned that
we have like these p3 instances
and uh maybe the question is like if
someone wants to use colab themselves
and maybe pay for it with colorpro you
know
do you what do you see roughly as the
price difference is there
i think it's hard to compare since like
colab is like a managed jupiter
environment and what we are going to use
with aws are like these virtual machine
and instance type so
um you can use them for the search maker
notebook which i've shown in my talk
where you can use them for the managed
training we run and you can like scale
them up and like not limited to the gpu
you get when starting your call-up
environment so you always know okay i
want to use the nvidia a100 for my
training why i want to use for nvidia
a100 for my training because i want to
use or to run it just in a distributed
way to be faster to iterate more and to
achieve better results
and i think it's hard to compare a like
apples with like
strawberries i would say and it really
depends on what you want to do
and it would be unfair to compare like a
collab as like as a free service with
subscriptions because they bring way
more than just your managed environment
i mean in collab i don't think you can
like easily collaborate and have
permissions across your organizations or
make sure that people can do certain
things
yeah yeah but definitely for for the um
like the community um event you can like
type in google ec2 instance type and
then you should find like an aws
documentation page where you can search
for p3
and the p3 um to spoil you um supports a
tesla v100 gpu and um like the smallest
one has um 16gb of memory
and then the g4 which material mentioned
is an nvidia t4 tensor core
so both gpus are pretty good i would say
and
i guess better what you get with collet
free for now so you should definitely
join the community event to test them
nice
with that i think that's there's no more
questions so thank you once again philip
for joining us
and i will see you around in the next
couple of days on discord or in the
forums
also hi
hi philip
sorry we survived
[Music]
so yeah this is uh this is the end of
the live event for today i mean sorry
for the change of live stream in between
in the middle of it but
it was a bit out of our control
and so we'll upload the recordings of
each talk
separately in the probably tomorrow
and now you should be ready to go find
your own model on a given task i made
the space demo out of it
as we showed you before like just go on
the forums in the course project
categories to find a project on the team
to collaborate with and once you've your
team said don't forget to fill the aws
form
to get access to
to the accounts there is only account
for the first one other team so you
should hurry up to make sure to have one
and you should also join the discord
to be able to live chat with your team
and your other participants in your
teams
so thanks again for for coming watch and
watching the events and the talks of
today and yeah see you see you during
the event
bye everyone
bye-bye