hi everyone and welcome to this
community event I'm Silva a research
engineer at ug face and I've led the
team that developed the huging face
course and we're very excited to release
part two of the course today with
content that should help you tackle any
NLP task you can check it out at
.c course and it's going to look like
this I'll close today's event with leis
leis if you want to introduce yourself
sure good day everyone I'm leis and I'm
a machine learning engineer hugging face
and I've been working with Silvan and
the team to build the content for the
course and uh today I'll be also taking
part in um introducing the speakers with
svan but before we introduce the first
Speaker today I'd like to sort of show
you a bit how we're going to do the um
the question and answering so I'll share
my screen and you should see
the hugging face forums so um even
though you can probably see us on
YouTube or twitch we're going to take
all of the questions um on the Forum so
what you want to do um when you're
watching one of the speakers is you head
to the course um category and then here
you'll see um various uh topics for each
of the speakers so the first Speaker
today is going to be Tom Wolf and the
way it works is you um post a a reply to
this topic so you can say hello I have a
question and then once you post this
people can vote on that topic and we'll
then select um the most voted Topics by
when you click a like and this will then
allow us to rank the most important
questions that you have and it will also
let people who can't make it uh today
for the live stream also benefit from
all of your awesome
questions so with that I will head back
to
San so um our first Speaker today is
Thomas
wolf um Thomas wolf is a very own
co-founder and chief science officer at
ug face and top of creating many popular
libraries like Transformers and data
sets he is also the initiator and Senior
chair of the largest research
collaboration that has ever existed in
artificial intelligence called big hands
and today he will talk to us about
transfer learning and the birth of the
Transformers
Library so Tom ready when you
are thanks a lot
San okay hi everyone um today I'm very
happy to make a talk a little bit
different than what I do
usually since you will learn a lot of
things in in the coming course right
this is the this is a full week of
classes a full week of chapters to read
so I thought I won't do an educational
course like I do usually but I'll talk
about the origin of huging face and the
origin of Transformers the origin of our
library and kind of walks you through uh
very personal uh honestly selection of
uh what happened in the past few years
at Hing face and in the field of
transfer
learning so let's go back uh in time uh
to the very beginning of why we're here
today at least why I'm here today which
is uh four years ago five years ago now
uh o before uh Junia who was an old
classmate of of me we were making we
were playing music together in a kind of
progressive rock band um kind of talk to
me about the coming revolution of deep
learning and and say okay you you've
been doing a little bit of science at
before before doing your your low degree
uh do you want to come back to doing s
science and I was like yeah actually
yeah and so we followed together uh this
Standford CS 224d de for NLP class which
was a lot about a lot about
lstm uh skip thought vectors for those
who who were already following a little
bit and actually this is I think
interesting because today we can of back
to this right we're doing a a course at
that time also we followed that over the
Internet there was still no pandemic it
was easier but still the course was at
Stanford we were both in Europe and so
maybe today uh some of you will be
following this class with friends maybe
friends in music gr band I don't know H
and maybe in a few years we will have a
very successful startup that started
today started this week by following
this course together and that's really
all that I hope for you I think for us
it's been an amazing an
amazing uh Journey from this uh from
this day to to today and um the early
days of hugging face were actually quite
different than the Huggy face we know
today uh we were building a an AI
chatbot that was a little bit like a to
talking
tagoi you could uh you could like
discuss of it it was the beginning of
generative Technologies right all with
lstm um and you could also send it
selfie there was beginning of image
classification to try to understand a
little bit what you were sending and to
try to reply with something fun and
that's how huging face started we were
just three four at this time um but
quite quickly we actually started to
open source a little bit of of code so
this is um this is a screenshot from the
hugging face GitHub
Pages four years ago uh you can see
probably libraries and framework you
don't know because they've changed a lot
but it was the first one we were really
proud of our
cerence uh P adaptation of of an emoji
detector of like sentimental
detector but uh why has this all
completely change uh because there was a
couple of revolution in the field of NLP
uh a few years after we started so more
precisely around the beginning of 2018 I
would say so I think if you want to date
back this could maybe go back to this
first unsupervised sentiment
neuron paper from from Alec Radford
already at open AI where they trained an
lstm on uh the largest data set that
people had at the time which was data
set of Amazon
reviews where actually the internet was
already there but still not common
practice right to use that as a training
data set so they trained this lstm and
what they discovered is that the the
model the lstm learned to uh actually
categor to categorize
sentiment without being explicitly
taught to do that so this was trained in
a generative fashion this lstm was was
trained to generate the next token just
like we do today right um and they they
also identified well something that we
don't really use anymore today I would
say but some specific neurons that were
very uh finely tuned to detect
sentiment and so this was the first um
this was the first hint that
unsupervised speed training with a
generative objective could lead to good
performances on Downstream task like
sent classification but this was still
uh using U lstm and the the data set was
still quite I would say DB and specific
Amazon reviews are still not I would say
has widely
um as covering a wide field of NLP as
the DAT set that we have today so a bit
later uh I say at the end of
2017 so a few months later where where
the the first I think most impactful
paper which was this first Elmo which is
uh the the first um this this paper
called Deep contextualized World
representation that was the Elmo the the
beginning of the of the Muppet names
right for for all this model which kind
of show that if you pre-trained on kind
of a an lstm on on a wide wider
diversity of text so this one was
unfortunately using a shuffle text so it
only had like full sentence but not full
paragraph uh you could actually have
good performances on a lot of NLP task
and not only sentiment analysis but also
like question answering textual
entailment and this kind of thing the
idea here was still kind of to extract
um token representation uh more than
maybe what we do today with like full
pre-train model and then really
concurrent to this work was the very
nice ULM fit from Sebastian rder and
Jeremy Howard where they also
pre-trained this
lstm and they also show that you could
adapt it to many many tasks so this two
paper really at the same time at the
beginning of 2018 people were like oh
maybe all this uh approach of transfer
learning that we've been using in
computer vision on image net for a long
time could be useful also in text and so
this kind of LED Sebastian to wrote this
this interesting piece in um around the
summer of
2018 which was maybe the image net
moment of NLP has arrived right and it
was very I think insightful in saying oh
pre-train language model can achieve
state-of-the-art results and that's
something that's going to change
everything
but this was just one part of the
Revolution which is transfer learning
pre-train model and the second part uh
was using Transformers in place from
from uh
lstm and here again uh I think Alec rfor
from a was really uh was really Pion
pioneering this by using the the GPT the
first GPT in 2018 so this was around the
summer uh with this generative
pre-trained
Transformers and actually the name
really GPT was I think only coined in
the following bird paper from Google
which was a few months later in October
8 2018 where I kind of use this same
Transformer model but instead of using
it as a as a coal or autog gressive
model they used the now famous uh masked
language modeling objective right the
noising D noising objective and so these
two paper were very interesting because
they mixed this early idea of transfer
learning
like having a model
pre-trained on a large data set with the
architectural Improvement of the
Transformer which made mostly the the
training very efficient a lot more
efficient than lstm though today we we
have like other well maybe we're coming
back I'm not sure this is maybe what
what you will be in charge to do if you
do research in this
field and uh what was the conclusion of
mixing these two objective well it was
really a storm in the NLP field so you
can cities if you go for instance on
paper with code if you check this curve
showing the the state-ofthe-art at
various time which you see that in
around the end of 2017 beginning of 2018
you see this very strong slope up upward
right there was like performance was
basically pling for like years right
here we see that on on N since basically
2009 we did not really move up anything
and then from like summer 2008 18 17
tagm was already kind of a first
transfer learning approach actually and
then with Transformer we were moving
upward very strongly Squad two you see
also the same very strong increase in
the performance of the model by using
bird you see the jump bird single model
there very strong jump 10
points um in exact match here matric and
then well you can say maybe we're pling
again today I think most of the the
reason we're pling is that squ 2 is now
kind of set saturated actually as a
benchmark and so we were following that
very closely uh and basically when GPT
the first of the Nam so this on went out
we were actually uh at Hy face
participating in a challenge at at newps
well this is a old slide so we still
call it nips but that's that's Now new
rips right
2018 we were participating in this
conversational AI challenge so the goal
was um when you have a some sort of chat
you had to like uh take into account a
little persona for your robot and you
have to answer in a in a convincing way
to a user so it's a little bit like a
touring test right but you have a
personal you're supposed to pretend
you're you're this person here and this
uh challenge was interesting because it
came with a nice uh data set made by by
the New York New York Facebook team and
um for this first time GPT was just out
and we were like okay maybe let's try
this GPT let's try this pre-training
approach for for generative task as well
a lot of people were just using this
model for classification task and so we
tried for the first time to use it for
really purely generative task like Chad
but and the results you can see on the
right were really impressive we we were
directly on top of the leaderboard of
competition and by a very strong margin
the perplexity the First Column you were
like perplexity is is a kind of an
exponential Matrix so it's quite hard to
divide by two the perplexity but we were
like two times lower than the other than
the other model well this Matrix well if
if you've done a little bit of
generative NLP and you will learn that
in the course here it's very hard to
evaluate so this did not translate
always in human evaluation but it was
still already very very very impressive
and when we study a little bit and
compared to the other uh teams what we
saw is that if some team were uh very
good on the validation set they often go
uh when very down on the on the hint
test set so if you've been doing a
little bit of kaggle you know that this
mostly means that you're starting to
overfit on the validation the public the
public data set and some team were a bit
lower and staying lower right which is
more like a sign of kind of underfitting
if you want and what we saw is that our
approach was both using transfer
learning and Transformer as well but
transfer learning I think is the
strongest ingredient um using transfer
learning we were both very strong
validation set and we stayed very robust
on the on the test
setla and so this led us uh in the next
summer in June June 2019 to do a very
long tutorial around transfer learning
in NLP with a few a few friends
Sebastian rder Matthew Peters who
basically created this this field swaba
who also actually did really great stuff
I won't show here but you should check
them out and what we saw there well what
we tried to do there is try to explain a
little bit what was the idea and so just
as an Insight you will learn more in the
course but basically this is the like
the secret ingredient of transfer
learning is that uh in in traditional
machine learning we had all these
separate tasks this is on the left you
had task one you wanted to do any you
had the data set for n and you basically
randomly initialize your model training
from scratch on this data set if you are
now trying to do summarization of
chatbot you will gather another data set
of summarization or chat but and you
will again just create a machine
Learning System specialized for this
task and if you had another task again
same thing you will have very separate
pipelines and very separate uh systems
who are already well at the each time
trans from scratch and the approach of
transfer learning was to do a little bit
more like I would say human do which is
that we don't randomly initialize our
brain each time we tackle a new task
right what we do is that we use all the
knowledge that we've learned in the
class in in our life experiences to
bootstrap our learning right and so when
we have a new task we just reuse all
this knowledge that we have and this
give us mostly two main advantages that
we see also when we when we reproduce
that with with machine learning models
which is first that we are very data
efficient because we can fill the gaps
between all the the examples that people
are are giving of of new task and so we
are very data efficient and we usually
reach better performances and this is
also related because we can fill the Gap
uh between all the between the data
examples that we have on the on the new
new Target task we can reach better
performances and transfer learning is a
way to try to do the same for for
machine learning so to try to build this
kind of generic knowledge that can be
reused on many many
tasks so we have we have questions that
could go here Tom is is there systematic
way to estimate how many annotated
samples to train to fine tune an
existing model for instance on a on an
air
task yeah that's a good question um it's
quite different it's quite dependent uh
so really the the direction the field is
trying to go is toward few what we call
few shot so using less and less example
so I mean requiring less and less
examples so for some task that you will
see in the course you can actually do
zero shot like Tech classification you
actually in many case don't need any
example but then it's it's always very
dependent on how specific your domain is
and and typically the more specific your
domain is the more you will need example
and in some case you might be you might
still be outside of the pre-training
domain of the model where you and then
you will need quite a large data set
yeah
thanks and so um um yeah let's go yeah
why would this work in NLP why you
probably have the intuition but here
they are the idea is that many NLP task
if you look them from if you like take a
step back they share a common knowledge
about language they share that uh oops
working full screen yeah basically if
you if you know for instance um
adjectives if you know Linguistics uh
then it will help you a lot for instance
if you want to do ner right you want to
identify I don't know the name of
organization if your model already if
your model already have a good knowledge
about Linguistics and know what is a
noun that will help it a lot because it
will know that name of organization are
very likely nouns okay they are not
adjectives they are not verbs so all
thing can be useful for the model the
same right if your task is in English if
your model know a little bit about
English you can just learn basically the
missing pieces that it has on your task
and don't and won't spend like
won't need I would say like half of your
um training data set just to learn how
English Works in general right so task
can also inform each other typically as
I as I told you syntax and semantics
they are often related if something is a
noun that's also mean that is has it has
a meaning related to to a noun if it's
an adjective that's here to kind of
change the meaning of a noun so only
thing um obviously annotated data data
you know that is rare it's very
difficult so the more we can gather it
the more we can gather several data sets
together to build a bigger one the
better it is for our model we know that
these new network models they are still
data hungry right we don't we don't have
a lot of feature that's the nice thing
about them but the cont the the
counterpart is that they are that angry
because they will need to learn this
feature themselves and now the most I
would say a down on Earth reason why it
works is that when we try it works so
that's currently the state of the for
many many NLP task it's to use this
transfer learning so that's also a good
reason to think that well maybe it has
reason to work so I talk a lot about
science here like transfer learning
Revolution is is a scientific revolution
in the field but uh if you're here today
it's also because you youve probably
following our open our open source work
and this was kind of studed in parel to
that we always thought that open source
and open science were going hand in hand
and so very early we started to open
source some models so the first one we
did is here it's now uh the summer of
2018 so just before basically this this
transfer learning re Revolution really
really began and so we we did this newor
cerence which was a cerence model for
Spacey uh which already saw very show
very interesting uh performances for for
also a pre-train AC in neural network
but only for cerence
and then there was obviously this GPT so
I told you we were using the first
version of GPT in this first competition
and that's where basically we understood
that wow this was very powerful and
before before using it as always we
actually release an open source version
so actually a bit later we discovered
that some other competitor also used our
open source GPT in the same competition
H so it was very very interesting we
actually made a paper together at at ACL
a year later so it's a bit open source
way of hugging face always trying to do
something collaboratively with everyone
I think that we we can just get this the
sum of every open source thing is is
larger than the having just everything
private
models and these two these two early
effort and the interest we saw from
people led us to try to do that uh again
and this led us to first um the first uh
Stone of Transformers which was this
conversion of bir in Pythor so in
October there was this birth model from
Google and people like wow this this is
really an impressive model and so when
the the code was open source in tens of
Flor one at that time we thought oh
that's a little bit a shame because uh T
of Flor one is still a little bit
difficult to use right and there was
pyos already at that time it was pyto
version 0.3 but it was still quite
interesting and easy to use and so we
decided okay let's convert this Toft
model in P Tor let's see if people will
be uh we will be able to use it easier
right and our idea here was basically
that well first at a time bird sounded
like a very large model it was 300
million parameters which today sounds
like maybe the smallest model that you
that you use at least in research field
but at that time it was like really
really large and so we were like nobody
will be able to train that so it should
make it easy to use the pre-train
weights that that Google really nicely
reled and to make it the easiest we
should actually have them also in
p and so that's what we did um and
that's that lead to this first uh
version called a pyos pre-trained
bird and this was really very far from
Transformers uh but we had a few initial
ID that evolved so One initial IDE was
that we could also host the pre-train
weights on our S3 buckets so that people
didn't have to to to go through the the
painful process of downloading and
converting them and we could maybe just
load them and here we were inspired by
by Joel G's nicely
um nicely made from free tray method in
alen NLP which which I found very very
nice with just one one line of python
you could download a pre-train model in
lnlp so I was like okay let's actually
use the same
ID and so this was the first thing uh
and this evolve over the coming two
years I would say from 2018 to 2020 to a
fully fledged Git Version model Hub that
you will see today which both I think
give you very uh wide access to the
model it's very easy to ins to inspect
it and at the same time give you a lot
of uh interesting like widget to play
with the model online API to use it very
easily so all this you will see in the
class but that's very interesting and
the first idea was also just to give
access to birds right but then I was
like okay we actually have an open
source Cod for GPT and another for bird
why not actually make them in a single
library and this led us very quickly to
basically a single API which today I
think WP not far from a 100 different
architectures which is the the today
Transformers and so this the first
version was pyto Transformers with with
four
architectures and quickly we thought now
tensorflow one is a little bit hard to
use but tensorflow two is quite nicely
done actually and so you're like maybe
maybe we could have both pyto and tensor
fl2 in the same framework maybe we could
even make it this so if you want to use
TPU to train your model on TPU because
you have like free free access you
should and then if you want to find tune
the same model in pyto you should be
able to switch very easily from one to
the other and so we try to make this
these three main the these two main
framework at that time and then then
later Jackson fla which are more more
recent in the same framework and we saw
that was actually not not so difficult
to get them all
together and to get what is now uh
Transformers right not not anymore P
Transformer while pter is still I think
maybe the better supported one but TL
and Jacks are very very strongly
improving um and so this was a
Transformer and what we witness is this
basically explosion in the number of
architecture on model most of them are
today in Transformers because it's very
easy to add new one and so you can see
from bir there were a lot of following
work trying to experiment with variance
in the architecture I think maybe Exel
net is a good example where they try to
do both some autoaggressive but also
bidirectional way to do um to to do a
forward past in the neural
network and one question became um what
is important from all for all this model
I mean how do they compare right how do
you select the model which one do you
know you want to use and here I think
what what became also quite clear in in
the sub in the subsequence here was that
actually more than architecture it was
actually the data that matter a lot so I
talk about Excel net which was very
interesting as a different architecture
and actually got a lot better
performances than bir at the time and
people are like wow we should experiment
with various architectures for uh
modifying the Transformers and this
subsequent work from Facebook called
Roberta actually show that um it's not
so much the architecture but just the
data excelnet was actually trained on a
lot more data than Bert and that's
actually what matter more than the
architecture and so robera took very
basic Transformers just exactly the same
as bir and then just train it on more
data and they got state-of-the-art
results and this was then show in the
these two other graph which are from the
neural scaling paper and they show that
basically the the architecture was the
the performance were really weakly
sensitive to the architecture so you see
at the top you see this very flat curve
for feet for forward ratio this very
flat curve for aspect ratio and these
are all logarithmic curve so they
actually spend very large um aspect
ratio and basically as long as you stay
not Soo far from the middle you're
pretty good the architecture just don't
really matter that much and the T5 paper
who came who came out later also confirm
this basically the the early Transformer
paper were already very close to the
optimal architecture
but what's very important is this data
set this uh graph on the lower left
where you see the data set size this is
also from from the neural scaling paper
from from open Ai and you see that just
by increasing the data set size you have
this very predictable increase in
performances so here this is the test
loss so lower is better and this
actually brought us to this question
well actually modern architecture what
is important here is data and so the
year eight
2020 actually or maybe
2019 uh we decided to do a new library
around data we are like okay we we also
need to focus on data now and so this
new library was called data sets where
early name was NLP but the for for a
long time now the name is data set and
this Library also got some quite strong
interests uh from the community it
passed the 10K Stars well I don't know
how G how GitHub stars are are relevant
as a measure of of interest but it's
also widely used by by a lot of follow
works so I think it's very nice and this
Library the idea was dat access to data
is also very difficult today in NLP
mostly because the situation is very
fragmented if you want to use a data set
you typically need to find where is the
code somewhere on GitHub you need also
to pre-process it and you actually spend
a lot of time doing the same work to
pre-process the data a lot of people are
just writing their CSV or J and loader
so maybe we could share this work and
avoid this and just jump to the data
structure that was one note another one
was that uh also with this very large
language model we're using now terabytes
of data and this this can be very
difficult to handle on your at least on
your personal laptop right if you want
to train a model on one terabyte of data
like Roba this is really a pain because
you typically can't load that in Ram you
need to find a smart way to do like just
loading the the current batch or
prefetching and so we thought maybe we
should solve all these issues at the
same time and the idea was that data set
could be a way to kind of centralize
access to to NLP data set at the
beginning now it's expanded to speech
and images but at the beginning it was
really about NLP data set and also to
cover more languages right uh NLP
English I mean is
really the dominant language in research
but in production today it's not
dominant language right we have lot of
other well I mean still the dominant
language but we have a lot of other
languages that are that are very
important and and every time you're
you're actually an NLP um person working
in another um country than the us or UK
basically you want to or India you want
to work with other languages and so this
led us to developing data set around
this efforts so at the end of November
uh well one year ago now November 2020
we did a First Community event where we
said to the
community uh well we 15 people at H face
trying to add data set if you want you
can join this is going to be fun we're
going to do uh this together you will be
on our slack we'll talk a little bit um
and also we'll do some swag and
everything and this was usually Yugi um
popular actually we had I think about
300 people joining to help us we were
definitely a little bit overwhelmed by
the success of this Fair Community
effort but it was very nice also in
terms of adding data set in many many
languages and the idea is that all these
data set were then very easy to access
and this could Foster more research on
multilinguality and think and so um this
led us also to write a paper with um
with the whole team and also there are
there are also some community members
actually in this paper you see Gan umit
they are both like Community contributor
and to submit it today this here to MLP
where I just actually want the best
demonstration paper so I think this is
very typical hugging facee thing where
we do something very open source uh we
do that with the community at least we
we invite everyone to join if they want
to participate and then we also do that
as a science effort and this all just go
together to make something very
impactful and so there is today also
this Hub that you will see in the in in
the course where where you today we have
about well not so far from 2,000 data
set openly access accessible you can
even Explore them on the
Hub and so this was the 2020 effort
around data data and what happened in
2021 um that was open science more and
big science well what happened I would
say it's very personal work through so I
would mean what I actually spend a lot
of my time on was this big scientif
fault and here the idea was that um
maybe it's time to do very large
collaboration in research in NLP and the
example well I'm I'm a physics PhD
formerly and so I in physics when we
have like very expensive things we
actually um gather as as a lot of like
research lab and try to build them
together one good example is the large
hyron collider which is in Europe which
is very very large particle physics
research tool it cost I think 10 billion
to build and it has involved really a
huge research community and has led to a
lot of research work like you have about
3,000 papers uh this led to the
discovery of many hard run thing like
that and so the idea was maybe it's time
to do that in Ai and NLP and so maybe
why I mean why why would it be TI to do
to do something like that well again I
think um because uh there was some if if
you follow what I've been saying up to
now right there there is this idea that
larger and larger data sets are are
something interesting some some
direction we want to go um this was
proved again by by the gpt3 paper that
that show that if you scale again the
number of tokens here you see it in
green on the Corpus side and this is
again a logarithmic scale so the the
scaling is really like how much is
it six order of magnit no two two order
of magnitude bigger than gpt2 well if
you scale both the Corpus size and the
parameter size you see like very
interesting new effects and these
effects are few shots so um recently I
mean just beginning of the talk some
somebody asked how many how many examp
example would you need and here in this
few shot example you can see that we
want we just need like 10 examples to
get very strong performances so I don't
remember exactly uh what this curve was
in terms of accuracy maybe I think it
was a classification task but this one
this one is just extracted from the gpt3
paper so you can find it back and they
have a lot of cores similar to this they
have a lot of Curves similar to this and
so there was this idea if you scale this
a lot you get some interesting new
effects and then quickly what you saw is
that uh there were new there were a lot
of announcement of reproduction na Labs
train gpt3 like model for Korean
entropic a startup of the barrier raised
a lot of money to train similar model AI
21 also trained a massive model called I
think
Jurassic and another company called
coair also raised a lot of money to to
train all this model and elter was also
this this grassroot Collective who
trying to reproduce so a lot of people
people were trying to reproducing this
but uh for a diversity of reason we were
not super convinced about this mostly
because like all these private company
did not seems to be uh targeting like
open source release and uh and elut was
was the only one but there was still
some main problems so here is a few
issues I see well first all these model
they are not designed as research tool
they're really designed by a company to
be like private thing that they want to
use for their private product but they
are not designed as research thing so
it's very typically it's very difficult
for academic researcher to be involved
in in the development of these models
right and typically what you when you
look at the teams who build them they're
like yeah they're like small smaller
than they should be in my opinion mostly
because they just belong to one one
company and mostly there is as again a
lot of environmental cost to training
all this model in parallel if they were
open source we wouldn't need to retrain
them like in in 60 company we could just
share one and that would be a nice model
for doing all our experiments
um and there was some ethical and and
social issues mostly since the very um
beginning of GPT there there there's
been a stream of paper investigating
okay how are these very large T opra and
we saw that they were lacking some
something for instance they were lacking
representativeness of of some
population um there also some typically
have some stereotypes so basically it's
very hard to investigate these train
data set also because they are typically
not open- sourced and so this call in my
opinion for like a most a more open uh
and collaborative way to develop this
and this is actually possible because if
you look in the public uh you see that
there are super computer that are very
powerful that actually almost free
accessible I mean you just need to
submit like a grun an application to say
I need some compute times on this and
they have like almost uh
2,700 V100 GPU so that's quite a lot of
compute power you can train very large
language model that so early this year I
start to talk with the operator on
builder of uh of Jean which is one super
computer located in France that we knew
a little bit from earlier and they say
yeah I mean come come train your model
basically on a super computer we're very
happy so we we kind of discussed a
little bit internally and then we we
decided that we should AC invite
everyone to come join and to train this
model with us and in particular invite
the academic Community which was
typically outside of all these uh model
creations and so we file a grun
application for enough compute time to
train a very large language model and
this was the beginning of big
science and big science has been going
on for like six months a little bit more
now nine months and it has already uh
produced very interesting papers so if
you've been following uh hugging face
account you probably saw the the tzero
paper which involv actually uh 40
alteros um so some of them are from
Huggy face but a of them are not from
hugy some are from yeah many University
um and so this type of um papers I think
is very interesting because it's outside
of the box so it's fully open it's fully
collaborative and it's also show that
you can outperform typically gpt3 on the
Zero shot Benchmark with a model which
is 16 times smaller so now what we are
trying to do is to actually increase the
size to 16 time larger so that's what
that's what will begin roughly next
January and to train this very large
model on Jon to see if we how far we can
go right with the same idea of like
openness open sharing collaborative
effort and so what's next for 2022 at
huging Phase well there is obviously
education if we want to have a
beneficial impact we think that this is
the the way to start when we talk about
bias in data set when we thought about
understanding your model knowing what is
important for your model basically we
think that we should share all the
knowledge we have
there is this strong Community you're
all here today I'm very happy about that
to because you basically following more
or less what we've been doing open
source this is our DNA and open science
they go hand inand for me sharing
knowledge we try to keep finding for
beneficial responsible AI I talk a
little bit about data set this is very
important I think to understand the bias
to understand the bias of your model as
well and basically try to make this yeah
an inclusive place right and while doing
all that doing also this exciting
research when I say we scale this this
new model to to very large sizes we also
try to have some fun and not taking us
too seriously uh which I think is also
um specificity of hugging face and why
maybe we have this hugging face emoji I
would say as as the name of company so
welcome to this course I'm very happy
very excited about this this second
chapter I think it's it's very
interesting you will learn a lot and
thank you for being here
today thanks a lot Tom um we have a few
questions i' I've waited for the end
because they are very general and not
directly related to what you are talking
about uh one is uh you talked about a
lot about Transformers but what about
using graph noral networks in the tasks
graph structures could capture data with
complex structures and relationships and
GNN provides opportunity to study on
model complex data representation for
NLP
tasks yeah that's a good question I mean
in all generality you could consider
Transformer as as a sort of graph neur
Network it's not Spar though it's fully
connected so maybe it could be more
efficient but if you look at like a
Transformer with pass attention in my
opinion it's it's really close to it's
really close to some graph new network
in particular if you use if you use
weight sharing like Universal
Transformers um I think in NLP
Transformer are kind of dominant today
uh but yeah as a told you in my opinion
the main aspect of the revolution is
about transfer learning more than the
specific architecture so if tomorrow
there is a more efficient architecture
than Transformer uh then I would be
happy to follow follow the the work
there and basically uh switch to
that yeah
yeah uh another question we had was more
about the ging face uh ecosystem uh EOS
is ging face becoming commercial from
open source there are many parts of a
that are not paid model training for
instance we go to NLP please help me
understand the portion of Open Source
versus
paid oh yeah yeah so so it's still um I
mean the idea of f face is I think
somehow very very typical like open core
model so everything is open source but
you need to do the training yourself you
need to do the training ROP yourself so
we do provide example
that's actually Silva has been doing a
lot of work on on the examples to to
have like um an efficient training so
you have open source code to to train
this and then we also provide some paid
solution which are mostly there I would
say to
facilitate your work so typically if
you're in a company you don't want to
wait you don't you don't have time to
actually explore everything yourself you
want some some help then we then we also
here to help and then we have some
specific solution around making this
model very fast and these are typically
mostly interesting to to companies more
than
researchers uh because they try to lower
the latency uh strongly and then here
this are also yeah paid product because
people who are interested in them are
typically companies who can pay for them
so the idea is that if you're new if
you're like new to the to the field if
you're a hobbyist if you're a researcher
you can use all of this for free and
then if you have some like specific
needs like you need to go very fast to
put that in production you need to have
like very low latency because you have
huge band withd in production then we
also here to help but then we we help
with our with our like additional
services that makes sense another
question you ined a little bit about
that at the end but more fun question is
how did the nameing face and the use
come
about yeah this came from our very early
uh Inception as a game company right we
were doing a this kind of talking
chatbot was mainly for for Millennials
for kids or young adults so we wanted a
fun name we also thought it would be fun
if at some point we became a very big
company uh I don't know if we if we'll
do that one day but if we like IPO and
then we have an emoji instead of like
this very serious name that everybody
has we thought yeah let's keep this
Emoji um and I think now it's part of
the fun we we like a lot of people are
taking thems sometimes a bit too
seriously I think in the world so we're
here to to say yeah this is a this is
important but having fun is also
important and a last fun question that
people were interested in is more
personal what do you use to build train
models notebooks vs code Vim
other yeah I typically use vs code um
notebook I'm always lost in the order of
the sales but maybe I have a bad bad bad
workflow now I typically use notebook to
explore data I think it's very very
nicely done for that that's the best way
way to to explore data and then when I
when I train I typically want something
which is very producible so I I switch
to something where I can get version
cleanly and more like typical B code
yeah nice thanks a lot for for your talk
it was very very
interesting and I think we're ready for
next speaker leis if you want to in him
oh I think Le is not hearing us anymore
so that's why there was this s can you
hear me yeah
right sorry what did you say I just
dropped after
forck I just said I think we're ready to
introduce our next speaker if you great
so let me add Jay Alamar to the stream
hey Jay and B you I think you might be
muted at the moment yes
hello right so I I think J probably
needs no introduction to everyone who's
here to uh listen about Transformers um
I think all of us have kind of started
by trying to read the attention is all
you need paper and gotten completely
confused by this crazy architecture
diagram and then immediately gone to
Jay's super influential blog post called
The Illustrated Transformer where he
really nicely shows um kind of
conceptually how these things work and
then there's a whole series of uh
beautiful blog posts ranging from Panda
to numpy to B GPT I mean the whole works
so thank you Jay for this amazing
service you've done to the community
you've kind of demystified Transformers
um which is amazing and today um Jay's
going to talk to us about a gentle
visual introduction to Transformer
models so I'll add yep great you got
your thing there
and are we okay on audio all good
amazing thank you so much that's a very
kind introduction uh happy to be here uh
speaking about
Transformers there's a a couple of
examples on a notebook this is a URL to
it but it's a you know just a very
simple sort of sort of notebook um to go
over
um to to give some um let's say history
um these are some some Milestones from
the history of writing right so uh first
time sort of people put um drawings on
tablets um and then people put language
carved it carved their laws of an entire
let's say Society on on a column that's
hamur Hammurabi's law the second image
below then people started using paper um
and then we have something like the the
printing machine or the printing press
that sort of um pushed the next let's
say Revolution and books became cheaper
to get and knowledge and literacy became
uh easier so once you need a book you
don't need to go and copy it with with
your hand um that sort of changed the
world then I do have a sense looking
back at the history that NLP via sort of
what machine learning is is enabling us
to do that we could be at at a at a
historical sort of Turning Point where
exactly are we did it happen the last
two or three years or is it going to
happen the in in the next few years um
it's very hard to tell if you're not
sort of looking back into history but
I'm extremely fascinated um by these
sort of language models and I'll tell
you a little bit why so yeah just a
brief bit about myself I I have been
blogging about machine learning and uh
Transformers uh machine learning for
maybe five six years a Transformer sort
of since uh I wanted to understand the
paper and this is sort of a map that I
used to for me to understand the paper
and turned out to be useful to to other
people uh as well the blog has gotten
about 4 million page views just because
the the topic became super interesting
to to people and then I wrote about
other types of Transformers as well as
NLP technology so embeddings word ofac
gpt3 ber and I do some some videos on on
YouTube as well to sort of explain some
of these um I do work at cooh here um as
as sort of Thomas mentioned we work on
large language models we offer sort of
large representation ational language
models kind of like bir and large
generation language models kind of like
GPT so these are massive language models
via API for people to experiment with
and and deploy in in Industry so this is
the URL and we're we're hiring at the
moment one thing to allude to uh that
Thomas mentioned is that Transformers
has taken NLP uh by storm uh but not
only NLP but also like computer vision
and it's like going out of out of NLP
this is the the superglue um and you can
see that Transformer and Transformer
based models uh have have dominated the
leaderboards um if you go and search on
Google so I have bring up this example
for people who sort of want to
understand a little bit more of the
current applications so if you go on to
search on on Google um and you type some
some words and you get some
recommendations your your keyboard would
give you some recommendations but also
the search engine would give you some
some recommendations so let's say you
search for Siri technology you get this
page um and then when you click on it
sometimes you get these highlighted sort
of notes I asked people which step in
this process used a language model um
and in an audience like this I think you
would realize that this is maybe a a
trick question and it's in all of them
uh a language model was used so
um auto complete and sort of next word
uh can be used via let's say language
models um this
summarization um and highlighting uh
these are all sort of language model uh
tasks but there's even one that is more
important which is where sort of Google
got you the
results so to give some sort of uh
examples yes so we some of these tasks
are let's say autocomplete text
summarization question answering
semantic search um and if you use Gmail
you have some of these um suggested
responses that's response selection
that's also another thing that language
models are are heavily used for so we're
not talking about specific just research
um or you know interesting ideas these
are heavily in application right now
just even less than less than a year
since like one of these papers is
written it it goes into into industry um
really quickly so this is sort of across
the board gole translate search uh all
the speech detection um and then using
it in in in search as well uh so like a
year I think after the bir paper was out
um the Google search team said that it
implementing let's say semantic search
using a model like Bert represented the
biggest Leap Forward in the past five
years and one of the biggest leaps
forward in the history of
search uh gpt3 took the World by storm
so many interesting applications there I
think most of of the team here has has
has looked at it so to think about
Transformer language models uh for a
second let's look at U one of the
developments that that uh have changed
the game um that it broke down this
training process into two steps one is
pre-training where a model is trained on
a lot of data um and then you fine-tune
it you train it further for for a
specific uh use case uh so if you want
to create let's say a new um semantic
or sentiment analysis let's say
classifier you don't have to train a
model from scratch uh you can train a
model that already understands English
or multilingual or whatever language
that you're so these are one of the this
is one of the uh development sort of
that that made these these models um
revolutionary um the training process of
a language model and I'm here speaking
about a a what's typically called a
language model which is an auto
regressive language model kind of like
GPT it's trained on a on a very basic
task which is you give it let's say
three tokens or three words and uh you
hold off what the corre correct word is
so a robot must obey but you don't tell
it obey you just give it the three
tokens or three words it will make a
prediction the model is not trained so
it will output a random
output we know what the actual output
that we are expecting is we we calculate
the error we update the model um and
then we do this for a lot of uh tokens
300 billion in in gpt3 case and then
once you've train pre-trained it you can
deploy it or fine-tune it or prompt
engineer it for for these other uh use
cases to tackle the the architecture a
little bit um I've always felt that it's
easier to look at it as just one black
box and the initial Transformer uh was
about um translation so you give it a a
sentence in in French and it outputs a a
sentence in English so uh you can
visualize it kind of like this and then
if you were to look under the hood it
has two major components an encoder and
a
decoder and these encoder and decoder
are are made up of of blocks uh six
blocks in the encoder and six in the
decoder that's the initial sort of
Transformer paper um and then these are
the main architectural ideas um I think
that are good to establish first before
thinking about different types of
Transformers so the initial Transformer
was an encoder decoder model so that
works really well for say things like um
machine translation uh but also there
are still models that are you know
continuing to toh tackle this this
structure so it's still very useful uh
on certain uh use cases so T5 t0 that
was sort of recently uh developed as
well is is these encoder decoder um and
I think Mina Google ai's large um chat B
is is one of these encoder decoder
models on the other hand you have models
that are trained completely using
decoders um so these are the your your
GPT models uh that are just like the
traditional Auto regressive um models
and then you have birt like uh models
structured using only the encoder sort
of Stack um and that is tends to be
called masked language modeling so you
have language models masked language
models and you have like encoder decoder
uh Transformers so this is uh to to
orient yourself and all of these are
widely in use uh a lot of them are have
specific tasks that they do better in
than the others uh our talk will focus
um on the Transformer uh language model
uh so this is going to be um the type of
model that we focus
on um another idea is how many layers
can you sort of Stack how many blocks
can you can you stack in BT large has
about 24 gpt2 large has about 36 so
these are another aspect let's say of
Transformers is uh so you can talk about
how large the the neural networks inside
are but also how many blocks that you
you stack to get your
model so let's talk about language
models
um I found that there's
a two really good examples to to break
down the components of a a Transformer
uh layer so let's think about a model
that has let's say only one one
layer if you give the model an example
of this input which is the
Shashank and then this model is trained
so it has hopefully seen uh words kind
of like this or phrases kind of like
this before or um you would be able to
think that it it is able to guess what
the next word is and this is a a
reference to uh the film The Shashank
Redemption um and I I tried this using
the distilled gbt2 example and when you
pass it the Shashank it gives you this
completion so it's able to actually know
what word I was expecting um after
Shashank and this really explains to you
one of the two major components of a
transformer block which is the feed for
neural network so in the training when
the model was trained on vast amounts of
of data uh previously it it is able to
calculate these probabilities of words
appearing after the current context that
you have so two words here what would
the third be and this is a a very
similar so this is not new as a
technology in Transformers this is very
similar to previous uh language models
and then three Gs and n gs uh that were
calculated using just counting how many
times uh this word has appeared before
sort of or after this context and
creating a probability uh distribution
or score via that so this is one of the
two major components the feed forward
neural network and this is an example
where it shines it was able to sort of
um remember or have a calculation of
what the next word would be just judging
on what the model has seen
before um this is a a paper from 200 and
three about these models and how the
they were there sort of previously and
Transformers build on on top of
this now the second component is a
little bit more complex and this is a
great example to uh illustrate it so
when you say the chicken did not cross
the road because it and you want the
model to complete what the next word
would be now the model cannot only
calculate you know what words usually
appear after the word it um that would
not produce a really good model it would
not be able to fit its data set really
well and before it sort of does that um
it needs to resolve what does it refer
to here does it refer to the chicken or
does it refer to the road so that
ambiguity is is a is is there and we can
throw this as at a small GPT uh example
and we can see if we say the the chicken
didn't cross the road because it
was covered in Gra so that's probably
the road right um he thought the sun
wasn't so bad then then now I don't know
maybe it was maybe the chicken was
covered in grass and that the sun was
not so bad so this is sort of a good
example of this other component of a
transformer block which is self
attention what self attention does is
that when the model is processing this
token self attention Associates it
either with the chicken or with the road
or with a mixture of both and really
that it's a mixture of of the
entire sentence so that is one of the
major things that Transformers brought
uh I guess to NLP in addition to um
being easier and better to run in
parallel U as compared to rnns which
were um used heavily before sort of
Transformers took over so these are the
two components self attention which uh
you know whenever you're processing a
token it bakes the in its understanding
or representation of that token uh
relevant information from other um parts
of of the input um and then feed forward
which is neural network which is um
Builds on top of
that the idea of tokenization is another
uh important one uh so the example that
I showed you here was a little bit
simplified when I actually throw the Sha
shank at the tokenizer of gpt2 is what
I'm using here it would break it down
kind of like this so these two words
would be for
tokens and then a through the tokenizer
that would be translated into token IDs
um and these token IDs will be the ones
that are uh hitting the the model and
then the model outputs something that we
can translate into a a word using the
token again and so if you're to to work
with with ber or GPT models these are
the two let's say components pieces of
code and that that you'll have to deal
with it the tokenizer which translates
between our words and let's say token
IDs and the actual model which has never
seen any words as only seen token
IDs so I I'll leave you to be able to um
go through the
uh notebook
um and this is sort of a an interesting
uh idea that I thought uh so gpt3 this
was one of the let's say outputs of of
gbd3 saying to be clear I'm not a person
I'm not self-aware I'm not conscious I
can't feel pain I don't enjoy anything I
am cold calculating machine designed to
simulate a human
response so this and other sort of
interesting Generations from from
massive language models are very
interesting if you can think of them as
um you know these were outputed by a
machine now definitely should be
suspicious until you see sort of what
the prompt was
uh but it's still interesting uh but
what's more interesting is that the
model has never actually seen language
it has only seen word like token IDs
kind of like this so when the model says
this actually this is what its output is
it has never touched or seen uh words
but it just is able to model sequences
let's say of of numbers and then um
embeddings embeddings is the next uh
idea here which is breathing uh meaning
into numbers into these token IDs
um and so when you download let's say
the GPT model you would get with it a
table of uh let's say vocabulary size
and and embedding for each word that the
model knows or each token that in the
model's
vocabulary uh this is how you can look
at it I don't know if this is up to date
with the latest versions but uh it was
when I was sort of looking at it so and
you can just pass it let's say the token
ID so this is the token ID of the word
the and you can if you're like me and
just you're curious and you want to look
at these numbers do they make sense they
look like
this so we have our words uh they're
converted into tokens through embedding
they're converted into vectors kind of
like the numbers that that that we saw
previously and these are really the
input into the Transformer block uh and
if the input is let's say four tokens
You' have these four tracks going
through the Transformer block and an
output at each
one and then however many blocks that
you have you can sort of uh each block
works on the output of the of the
previous one and these are called dat
States when when you're doing uh text
generation uh the one that you you
predict uh the next token using this
final one so if we're predicting let's
say the word uh Redemption it would the
model would be working off of this final
hidden state of the final
token okay so how do we turn computation
into into language how do we project the
output into into language and it looks
kind of like this so where this is the
last hidden State um of the last token
so the model has processed it done a lot
of crunching and then it outputs this
this one vector uh the let's say
language model head would would project
that so it would give a score to each
word in its vocabulary each token in its
vocabulary and we just pick the highest
one or we can that's one type of of
output selection there are other
decoding strategies is is is the word
for this but say you just pick the
highest one that would be the word
Redemption
um so now you scale it and that's how
you sort of have these massive um
language models they process all of the
all of these tokens and then in the end
they give you an output and you feed
that autor regressively so this token
that comes out is fed back into the
model and that's how the model continues
to produce token after
token hey Jay I have a question um about
scaling so what do you mean by this is
this uh scaling like the architecture or
do are there other things we should
consider when we talk about scaling
Transformers yes exactly so I'm talking
about how do you go from let's say gpt2
to GPT three uh so you have larger
models larger training sets um and then
like even you you tra you layer a higher
let's say number of of layers or more
more Transformer
blocks great and then a kind of
follow-up question would be um so at coh
here I think you mentioned you you you
guys are also training large language
models yourself yes and so I'm kind of
wondering like what kind of
considerations go into curating the data
sets um when you're when you're doing
this because we know that gpt3 for
example has a wide set of kind of no and
biases and can lead to some kind of
generations that maybe are sort of
harmful or offensive and so on I'm just
kind of curious how how you guys think
about this yes yes absolutely so there's
the the team works on filtering the the
data sets for for for toxicity I think
there's a there's a paper also uh there
that I can I can share uh after with the
team so uh exactly that that filtering
of of the data set uh is a very
important component that leads to models
uh hopefully that that can produce
better
Generations awesome thank you amazing
thank you so
much ah so was that the end of your talk
that is the end of my talk great cool
cool great so we have a few more
questions um on the forums so this one
is is more of a um say general question
put it here in the chat so do you see a
future in symbolic learning rather than
probabilistic approaches and I think
this is kind of like you know the the
big topic on Twitter between Gary Marcus
and um you know other people who are
like connectionists and I'm curious to
see what your thoughts are true yeah I
don't have sort of much of a an opinion
I think we can probably use a little bit
of both the probabilistic has really
proven itself so I don't think you can
go without the probabilistic but can you
infuse it with uh a little bit more
rules like that's what we're seeing
chatbots so to speak you can use some of
the probabilistic but you you have to
sort of encode some some of the some of
your rules uh to to actually have
something that you can let's say ship
out to your customers
know yeah I think a kind of very related
question here is whether like
Transformers or you know deep neural
networks in general can do true
extrapolation um and you know whether
we're just doing interpolation on some
sort of you know manifold in a lower
Dimension and I I think this is
generally what the symbolists are always
you know wanting us to you know pay
attention to true true and there's
there's some work on on causal sort of
inference uh that's been sort of
Dripping more and more into into NLP and
that sort of maybe goes beyond uh just
let's say correlation into maybe
encoding some some of the of the
causation cool so another question we
have here is um so you know your blog as
I mentioned at the start has kind of
helped everyone visually understand
what's going on under the hood and sort
of what concept or architecture is like
next on your on your list yeah so a lot
of what I focus on especially sort of in
my my work with Co here is how do we get
the best results from these models in
the real world and how do we apply them
to to real world problems so um I've
been thinking about things like you know
what are the best ways to explain and
think about things like prompt
engineering um and you know what's the
best way to sort of you know fine-tune
um a model what should the data set sort
of
uh look like so a lot of it I've been
sort of thinking about the applied side
um for a while now and that sort of
gives me some exposure into things like
um or areas that are interesting both on
generation but also on the embedding
side kind of like dense retrieval and
semantic search um and these sort of you
know very interesting uh applications of
of models both of the bir uh and the GPT
types awesome I think we have another
question here it's a bit of a long one
I'll see if I can just read it out it
doesn't quite fit so um it says let's
see so for T5 um there's been around
three approaches that have been used to
fine-tuned pre-trained models for all
Downstream tasks so this is kind of like
where you fine-tune all of the layers or
you just freeze some layers or you know
do gradual unfreezing and the question
is that for models like Bert um which
are more these encoder based models um
which of these approaches do you think
is best you know kind of using freezing
or just just fine tuning the whole thing
yeah so it's a trade-off it's really a
trade-off so you can you can get some
value without fine tun you can fit some
classifiers on top of the Frozen model
to begin with you can freeze some layers
and uh train let's say the final layers
and you can fine-tune the entire thing
um but the more like fine-tuning that
you do the more training that you do
that will take more time it will might
need a little bit more data
uh there isn't one size fits all
unfortunately in machine learning most
of the answers are always you know
depends on the
case exactly and and I think this is
kind of very closely related to this
follow-up question which is sort of how
how do we know how much we should
fine-tune a pre-trained model so are
there some like General like heris or
strategies that you sort of think about
when you're doing fine tuning so fine
tuning is a different is a different
Beast so it's a word that can use
different things the it depends on the
complexity of the task that you want the
model to accomplish so um if the model
has been has come across your task uh in
uh its training set for example then you
don't need a lot of fine tuning because
if you let's say get data from the web
and you train the model to do language
modeling on it it would see things like
paragraph and then in summary and then a
sum summary under it so it wouldn't need
a lot of data to grasp the concept of of
let's say summarization because it was
already sort of kind of trained on it
but if you have something where uh the
the relationship between the input that
you want and the output is is is
difficult uh then you will need a bit
more sort of data and it's it's similar
to however you want to fit uh features
and labels in a general machine learning
uh algorithm if the relationship is easy
if it's sort of the model is able to do
it with less data if it's complex um
then that's
you know you might need infinite Thea
for it exactly and I I think this is
also perhaps why everyone has got so
excited about models like gpt3 because
they kind of showed like in Tom's talk
you mentioned this kind of curve where
you can see you know the performance as
a function of number of examples and in
the F shot case where you only need 10
examples and you're still you know
almost you know several tens of points
better than than everything else this is
quite impressive and maybe just one last
question for you because okay here I
guess you're doing fot learning is do do
you see this technique of like prompt
engineering or F shot learning becoming
kind of as common place in Industry um
as you know the current Paradigm we use
of you know collecting several thousand
examples and you know training our
classify them yeah so it is and it
depends on on the use case and it's it's
really better for some use cases uh
rather than than others uh you can have
a lot of let's say cases where you don't
have that data or it's very difficult
but that relationship is somehow encoded
through examples that the model has has
seen before so it's uh for people
without a lot of data set uh F shot
learning is one thing to to to to
experiment with it is convenient uh but
it has that it's again it's imper
empirical and experimental uh sort of in
nature uh but if you have only five or
10 uh or 50 examples uh it's it's worth
experimenting with it if if it's too
expensive for you to actually get enough
of a data set to train a
month totally so that's I think all the
questions we have from the community
right now and uh thank you once again
for this beautiful talk and for taking
part in this event I think everyone
really enjoys um seeing your your
amazing slides and images amazing thank
you so much I appreciate it love all all
your work happy to be here thank you so
much by bye
bye have a good
one uh so we are a little bit of time
before next speaker time to talk because
J was super inside fold and very clear
um so I'm going to give you a quick
overview of our part two of the course
uh while we wait for our next speaker to
be fully ready uh let me just share my
screen and we'll get going
so part one of the course I was teaching
you the the basics of how to use a
Transformer model uh especially
pre-trained model and finding it on a
text classification task uh before
sharing it with the community using our
tools uh to to put it on the model Hub
and part two of the course main goal is
to show you how to tackle any NLP tasks
so for this uh we started by bit longer
dive into the the data set library and
the tokenizer library because they're
very helpful uh to they have lots of
useful tools to tackle some of the most
challenging tasks like for instance
question answering uh so the data set so
chapter five goes over the data set
library and shows you how to to load the
data set especially if it isn't from the
but you have your own data which is
going to be super useful if you want to
tackle your own problem and then uh it's
showing showing a little bit how to
preprocess that data uh so to select a
few random samples how to preprocess it
uh any how to split one data set into a
training and validation
Set uh it's showing exactly how data
sets work behind the scenes to to make
sure that you don't go out of ram if you
have a huge data set using aach Aro
behind the scenes to to make sure that
your data set is on Diss and not really
in your in your RAM and then it explains
it will explain to you to create your
own data set and upload it to Z Hub and
there is a last section about how to use
um to do semantic search and using
edings with f f AIS SS I'm not sure if
I'm supposed to spell it or to say it
leis yeah I never know and I also should
apologize in advance for my Australian
accent so if people are watching all
these videos going what the hell is he
saying
then so s you can activate the subers on
each video and they're pretty good
so um the next chapter is a deeper dive
into the tokenizers library to inspect
uh what those tokenizers are backed by
the tokenizers library as tokenizers
that are backed by rust that we call
fast tokenizers in the Transformers
Library by opposition to the to the
python tokenizer which we qualify as
slow that's because with the r Library
backing those fast tokenizers we can
parallelize the tokenization and go like
between 10 and 25 times faster to
toonize GLS of text so this section will
explain to you to train a new tokenizer
for instance if you you have the GPT
model in English but you want to train a
GPT model in your own language so you'll
need to train a new tokenizer for that
but you want to use the same algorithm
as for GPT so there is one single method
to learn and this chapter tells you all
about it that helps you train your
neizer like GPT or like b or like T5
uh then you have a section about the
fast ionizers special powers uh the
first thing is to evaluate why they are
faster and how much faster they are
compared to slow tokenizers and then uh
it goes in depth into the the features
that we added in the tokenizers library
that are very helpful uh to go back from
a token to to the span of text that
generated it which is super useful for
toen classification or question
answering and so that's why uh it's then
you we do then into the inside the token
classification Pipeline and inside the
question answering pipeline to explain
exactly what happens uh between the
model predictions and the answer that
the those pipeline give you and then we
we make a deep dive into what happens
inside the tokenizer so the first step
on the normalization which is like
cleaning the text for instance removing
the accents for for B models or all
lowercasing everything if you have a
model that is used to that and then
there is PR ization which is splitting a
text into words before the model of the
tokenizer um is applied uh for instance
here to separates today into two
separate tokens and then postprocessor
applies the the special toket for for
instance for bir for the Cs at the
beginning and the SE at the end so the
first this this section goes of
normalization protonization and then you
have three different algorithm that are
supported by the tokenizer library which
are bpe word piece and unigram and they
both all got they all get their own
section uh to explain to you they work
to tokenize input how they were
trained and uh yeah and how you could do
go to to train your own tokenizer using
one of those algorithm and then the last
section in this chapter explain to you
how to build tokenizer block back block
using the tokenizer library and then to
use it in the Transformers library to to
tokenize all your text before training
or funing a model with your
neizer and chapter seven is really the
big chapter of this part two so as I
said the goal was to teach you how to
take any NLP task and this is the
chapter that delivers that so it begins
with token classification which is a bit
like text classification but you have to
predict one level for each token inside
your text then uh we go over Mass
language modeling we functioning a birth
model over the MDB data set and we can
see how a model that was used that was
very generic so this is a great mask it
would predict some generic terms and
then after being ftuned on the MDB data
set it will predict movie or film to to
f the sentence because it's been used to
a data set full of movie terms the next
task is translation how to translate the
text into another language then there is
a world section about summarization
which is taking a very long text and
extracting uh a very small summary so
like very very short summary for that
section to train the model fast uh we we
decided to to to train an extreme
summarizer of movie
reviews uh then there is a section about
caal language modeling which is also
going to show you how to train a new
model from
scratch and lastly uh a section about
question answering and how to
pre-process and postprocess the data uh
to create model that is capable of
finding answer to questions inside the
context like this one uh which which
wasable to answer this question from the
from the Transformers Remy and find the
deeping libraries backing the
Transformers
library and then in chapter eight uh we
did a little bit uh about how to tackle
the most common NLP tasks uh sorry how
to ask for helps when you get a problem
uh so it begins with what to do when
when you get an error and in this
chapter we teach you how to read the
python thrb back which can be very
overwhelming at first if you don't know
what to look for but hopefully this will
help you analyze your error and help you
debug your own training then we wrote a
small section about how to ask for helps
on the forums uh which is to make sure
that you include all the necessary
information uh so that all members of
the community can help you very fast to
solve to solve your problem
uh We've then made the section that is
more specific about debugging a machine
learning training uh whether if you're
using the trainer API in pych or caras
with tensorflow all of those section
both have Conta with py or tensorflow
and you can switch from one to the other
with this little button at the right
top and so it's going to explain to you
what to look for so the steps uh to look
for first deing your data then your
model then perhaps a backward path as
the optimiz the step and then we
finished this section with how to write
a good issue on GitHub to make sure
again that you include all the necessary
information so that the maintainers can
help you solve your problem as soon as
possible uh or debug and make a fix if
necessary in the corresponding libraries
so that's it for for a quick overview of
that part two of the course we hope you
really like it if you have questions
about any of those uh parts part you can
go on the forums where is a course
section uh which we showed you before
and in this course section there is one
topic for each of the chapter that we
just created today so don't hesitate to
go there to ask any questions that you
have about the course
content and I think we're ready for next
speaker Lis what do you think yeah maybe
we could just uh for people who are just
joining the stream now just quickly show
um how they can ask questions for the
next speaker oh yes very good
so our next speaker is going to be
Margaret Mitchell I'm going to introduce
her in a moment but you should go on
this topic in the course uh category to
ask any questions uh that you want and
uh if you find a question that you would
really like to be H let's go for
instance in this topic you can see that
there is a little hurt beneath each
topic so make sure that you like the
question that you want to be really
asked because if we have too many
questions and don't have time to ask all
of them we'll select the one but the
most
likes and let me stop sharing my
screen and so like I said our next
speaker is going to be Margaret Michelle
hi
Margaret Hi how are you um I'm very good
so Margaret is a researcher working on
ethical AI I currently focus on the in
and out of EIC in artificial intelligent
development in Tech she has published
over 50 papers on natural language
generation assistive technology computer
vision and higher AI ethics and alls
multiple patterns in the areas of
conservation generation and sentiment
classification and today she will talk
to us about the values in machine
learning
development so let me put your screen in
the Stream and then we'll let you go on
with your talk that's good everyone can
hear me okay oh it even says so in this
little preview cool um and then to see
comments I can look maybe on the right
side of the screen um I would be really
happy for people to ask questions as I
go um that said I might get lost in what
I'm doing and miss that someone is
asking a question uh so I'll try my
best okay so hello uh thank you for
having me um I recently joined hugging
face to work on things like values in ml
development um so I'm going to be here I
guess a little bit as wet blanket but
also hopefully providing uh some useful
insights um into the
development
okay um so by now the machine learning
development pipeline is relatively
familiar to people who work in AI so
first you collect data then you train
and test a model you can add additional
postprocessing such as selecting among A
ranked list or applying thresholds than
evaluating in order for the model to
Output an eventual decision or answer
and people see the
output but before we even begin the
process the data itself encodes a subset
of human perspectives so what people
choose to talk about and how they talk
about it based on their experience so
the idea that a data set can be unbiased
which is one of these uh sort of
questions that comes up in the data
versus model debates uh it's actually
both can be biased um and saying that a
data set can somehow be unbiased
misunderstands what a data set is by
definition right so a data set is not
everything and all experience for all of
humanity uh in the world ever or in the
universe ever right by definition it is
a slice of all of that it's a subset
it's a snapshot so that means that it is
biased with respect to everything else
in the universe ever right it's a very
limited subset specifically collected
from the world with all kinds of SKS
reflecting the way that people represent
themselves um as well as SKS in data
collection and annotation um and the way
that people talk about the world um so
we see biases um we see biases like
reporting bias which is selectively
sharing only specific kinds of
information um think about how often
people announce that they're breathing
for example right uh all these things
are happening all the time we don't
describe them we don't talk about them
they sort of go without saying um and
that means we don't see it in the data
right so for to guess uh how often
people breathe uh based on a web crawl
we would probably get it wrong um and we
also see effects like racism and sexism
and ableism and nationalism Express
expressed throughout um which reflects
just basic human behavior
right so these biases uh then have a
ripple effect throughout the rest of
model development as the training data
itself is the only information that a
supervised uh machine learning model can
learn can learn
from um other kinds of human biases are
then layered on top of this and injected
into model development by the choice of
model the choice of the objective
function how the model is evaluated um
these choices persist in the rest of the
model life cycle
so for example there's the default
effect which is going with whatever is
default uh for the model you're
developing and assuming it is
appropriate so this happens in our model
architecture choices um although you
know this talk about Transformers these
talks about Transformers are sort of
showing what happens when you move
outside of that default effect which
until Transformers were largely you know
sort of recurrent
structures um but it's also things like
our optimization choices and then this
is even more true in applied product
contexts where it's used in different
kinds of context um there's a similar
effect of anchoring bias uh which is
sticking with what one is familiar with
so think of what your loss function is
right we don't tend to think a lot about
that unless we're specifically writing a
paper on that um human biases are
further injected into any
post-processing decisions or a lack of
post-processing right so just because
you don't think about it doesn't mean
that something isn't happening it just
means that you are making the choice to
have whatever is in there as a bias
persist as a bias um and this is also
where things like congruence bias comes
in confirmation biases um both are
biases where you tend to pay attention
to outcomes that are familiar to you and
leave out outcomes that aren't so that
means we need to do additional work in
development to Think Through the ways
that the model can affect different
people and this is a place where um risk
harms analysis is really
key um and finally once a model is
deployed people see the output and begin
to act on that output um but it's not
the end of the ml life cycle right so as
people act on the output um in the
outlets where the data is collected so
think about content moderation that you
see on YouTube on Facebook uh that you
know the recommendation systems you see
um in so many of the uh different apps
we use
um that then that creates more data as
we interact with those systems and then
that becomes training data for an
updated model basically encoding uh all
of these biases essentially permanently
right so this creates an echo chamber
feedback loop where the model then
learns from the data that has already
encoded the biases in the first model
decisions and so this can amplify
problematic decisions over time
um I like I like to refer to this as
bias laundering um where the final model
can have all kinds of ethical issues and
it can be difficult to pinpoint the
exact sources uh that that cause them um
although it can be hard uh it is part
and parcel of the model uh so really
important to actually model and factor
in um I like to call this bias
laundering also because it allows me to
use this gift it is one of the only
gifts I had made by myself and I'm very
proud of it um I just read that when
women make jokes in talks it reflects
poorly on them so uh I guess I'm
reflecting poorly to everyone apologies
okay so the input data set defines what
we care about for the model uh the
default approach to development is to
Simply use the training data that is
available in a specific data set and to
eval valuate on the evaluation data that
is available in that data set right so
if critical aspects of the problem space
aren't represented in the pipeline then
deploying the model means you are making
the value judgment that these aspects
should not be represented so if the data
the training the post-processing creates
models that disproportionately
underperform on some populations you are
implicitly making the value judgment
that some populations should receive CE
worse performance right so machine
learning development isn't value neutral
you can just uh for lack of a better
word be ignorant about those values in
your development um and what this means
is that we have to think critically
about values encoded throughout
development if our goal is to in is to
advance more human aligned more ethics
informed kinds of artificial
intelligence um so this brings us to the
role of Ethics in Ai and the role of
values
so ethics doesn't provide an answer uh
but rather it provides Frameworks for
analyzing problems um in light of
different human values so uh some
example human values are shown on the
right uh and these are exactly the
values that can be incorporated not only
within the development pipeline um but
within the development culture more
broadly so having a basic understanding
of the major ethical theories will help
us in learning how to articulate and
justify our decisions so let's dive a
bit more deeply into the ethical
theories that can drive the production
of AI
systems so broadly speaking there are
four main families of Ethics meta ethics
and descriptive ethics are concerned
with uh analyses of Ethics in and of
itself uh normative is concerned with
the Norms the standards the criteria
that Define principles of ethical
Behavior uh these are what provide the
tools for ethical analyses and
development um and applied ethics are uh
essentially ethical theories realized in
practice and within ethics uh informed
development in a technology company this
would correspond to something like the
organizational ethics or the
organization's
principles um one of the key ideas here
is that we all have different senses of
morality we have different cultures and
religions that prioritize different
kinds of values uh so for example
respect and honor for family uh tends to
be relatively under represented in
Western philosophy um but when we're
working within an organization these
values need to be defined at the
organization level in order to provide a
common ground of values that employees
can build off of and base uh different
decisions on so there's often this
question of who's ethics right like it's
so hard to decide there has to be one
and it has to be one that we all agree
on but that's not right the question of
whose ethics is simply it's the
organization's ethics and every
organization will be different you can
think of this as a a business
differentiator or maybe a grad school
differentiator whatever it is um uh and
these ethics these values can be defined
by calling together the values from all
different sources um including all the
people who have been hired right um so
uh there is not one good ethics there's
not one way to say this is ethical and
this is not it's all a function of the
people there and what the values they
prioritize are all right I hope that
makes sense um so as a vehicle for
operationalizing ethics uh within a
larger development pipeline or within an
organization normative ethics in
particular uh provide several different
perspectives uh so at a high level um
these P these perspectives concern
standards of
criteria um there are three main
theories um I'm probably doing a
horrible job I'm not an ethicist by
training but I've very much tried to
learn ethics uh within AI um so these
are things like virtue
consequentialism uh deontological ethics
um and in the context of Ethics informed
organizational
processes uh we're largely following a
deontological ethics that has been
informed form Med by virtue ethics and
uh
consequentialism um so let's turn again
to our pipeline uh where we earlier
highlighted how different problematic
Behavior can be propagated and Amplified
in a
model um in the collection of data this
first bit um values of concern include
copyright consent Fair pay diversity and
the right to something like contestation
the right to say that you don't want to
be in the data um currently the
state-of-the-art on all of this is
terrible um so generally copyright uh is
not a large part of what's taken into
account during uh data development
consent
is largely absent at least the sort of
informed consent that we would want uh
for people whose data is being used for
training models uh Fair pay often it's
no pay so you know that sucks um and we
generally fail at diversity in part
because the culture around machine
learning looks to get annotations as if
the production of annotations is some
black box where you throw in what you
want annotated and you get it back
ignoring that everything that happens in
that process uh brings with it tons of
different uh biases and what countries
this is happening in is really going to
affect uh what kinds of annotations
you're going to get okay uh um and then
in model training values of concern
include how appropriate uh different
decisions are in light of the
foreseeable effects of the model right
so you have to think about how this will
be used in context will these people
know what a decision threshold is and
how to change it based on what they want
to do usually not um and so it's very
critical to Think Through how these
models will be applied in practice um
and in post-processing values include
the consideration of tradeoffs um and
aiming towards fairness of
outcomes um so we can now further
examine what these ethical values mean
uh within this First Data stage um and I
would say defining values in data
collection and annotation is one of the
most pressing issues in technology
currently um questions of who owns the
data whether it can be deleted what sort
of profit or benefit can be derived from
it all come into play
um and I like to call this the data
bottleneck because it's a touch point of
information flow um from Human thought
to AI but it's the source of an
incredible amount of
issues so in particular there is not a
way to have something like the most
values right or the most ethical thing
there are tradeoffs and tensions in all
decisions and so it goes back to the
organizations uh or or the individuals
uh maybe if you're working alone um what
your defined values are um and PRI what
your prioritization is among them um so
for example we may want to release a
data set to help with benchmarking and
reproducibility but this comes with the
tradeoffs of lack of informed consent
perpetuating problematic ideas that um
that are there without consent um and
the fact that the data can't
realistically be deleted yet we don't
yet have a state-of-the-art where we can
remove data um this means details of
individuals in the data cannot only be
shared but continually Amplified and
proliferated without their consent right
there's this sort of misunderstanding
that well if it's out there it's already
public so we can use it the problem is
one instance being public and sort of
generally difficult to access is
different than being uh spread all over
the world to hundreds thousands of
people these two things are not the same
um and so you know in the effort uh in
the effort to have benchmarking and
repid disability we often making the
choice to amplify and proliferate
problematic content without
consent um we may also want to aim for
things like inclusion and diversity
within our data uh representing
different people with respect to their
different Norms um but this comes up
against things like
exploitation uh and tokenism and
stereotyping um that's when data is
collected with a specific goal of
diversity By ignoring sort of all the
other factors at play um it's not yet
clear what the best methods are um but
there's uh still a lot of work to be
done and at least doing somewhat better
so I'll give you an
example um a couple years ago Google was
working on face verification phone onine
um uh for the pixel and hired a vendor
because they needed more photos of black
people so what did the vendors do they
went to a homeless camp in Atlanta and
paid people there who were black
$5 for their face to build Google's
diverse uh facial recognition system
that you know obviously they would make
a lot of profit from so uh a few issues
there one is equating black people with
homeless people um so that's some sort
of disgusting stereotype I don't think I
was even aware of but there you go uh
exploiting people obviously $ five
dollar and without uh very informed
consent um and also tokenism where
you're not looking at how to include
this person in light of what their
General life is like their culture
everything else like this uh but
actually just focusing on who you could
quickly get because of this single
characteristic that's
tokenism right so one of the key
solutions to operationalizing ethical
data development means defining values
and priorities upfront and constructing
data specifically based on the values um
so this massively expands what it means
to collect and work with data making
data collection itself a detailed and
nuanced process with dedicated roles to
handle different aspects of data
development and uh correspond artifacts
that allow the values encoded in data
selection to be further examined
throughout the pipeline and then within
final uh release
decisions so this can be seen as a life
cycle for the data as a whole um as it
requires continual monitoring and
analyses of the effects of the data and
updates then based on these
analyses um now we can turn to the
details of
operation one second
we have a question that's very relevant
on the
data and which is how do we push for
adaptation of more ethical data sets not
imaginet cc4 Lon for example in major
conferences yeah so obvious maybe not
obviously so a lot of
researchers um have increasingly been
paying attention and a subset of
researchers have been publishing papers
on this so uh ABA ban has done
incredible ible work on this uh Jesse
Dodge has led some amazing work on this
for um C4 right so the fact that these
papers are starting to get accepted is a
massive step forward in sort of this
General understanding um they were not
they were largely rejected you know two
years ago uh so there's been an increase
already in understanding why these
things are important uh in part because
of you know the small subset of richers
really researchers really crying out for
it over and over again um and making it
clear via not only papers but Twitter
and blog posts and interviews and you
know so this is sort of happening now
the key now is to make sure that
reviewing rubric actually cares about
this stuff so uh when you're reading a
paper that talks about model training
and model evaluation you have to know
critical aspects of what that data set
for training is what that data set for
evaluation is and if it's too much to
include in the paper you can have it in
the appendix whatever it is but while we
continue to uh you know really celebrate
papers that don't discuss the data
they're using at all we're not going to
be incentivizing ethical data practices
um so reviewing rubric uh also Awards I
have this I have this idea that you can
get an award at a conference for like
best
evaluation uh or like best analysis of
input data or something like that
currently we have best paper Awards uh
which are is pretty underspecified and
tends to uh bias in in certain ways but
once we can start breaking this down
into what we really want uh both in
terms of reviewing rubric and awards
then I think we can really get
somewhere thanks for that that makes
sense um cool um I didn't see the
question so I guess I'm not like in the
right chat or whatever but cool um so uh
the next bit here in this uh model sort
of pipeline is the model training um and
that's where values um come into play
around the appropriateness of the loss
function uh and the
model
so okay so I'm going to talk about this
via a case study um I was able to work
on uh Corona virus forecasting um which
was you know amazing to be able to work
on um and we quickly found that there
were a mass amount of model-based
decisions that could drastically affect
people when the model was deployed so um
if you're not technical bear with me I'm
going to try and explain if you are this
might be just a repeat um but this is
very important so false positives in a
coronav virus forecasting model means
that hospitals might be ere equipped
expecting a need for more beds and
ventilators but a false negative means
that there would be less resources than
needed which could end in death right so
there are tradeoffs in different kinds
of errors but arguably a model that
tends towards false positives is
preferable to a model that tends towards
false
negatives and one of the issues at play
is the choice of loss function um so if
you take a look at the different kinds
of objectives that can be used which are
on the um bottom left um they treat
positive and negative instances the same
way these are all measured in terms of
absolute values right and so that means
positive false positive false negative
these basic ideas are to treated as
equivalent and so for the value aligned
goals of the model this suggests that
some additional work may be necessary
for the selection of the most suitable
model um and uh perhaps unsurprisingly
trying to bring this forward at Google
mostly meant hostility and it was a
terrible experience now I'm telling all
of you still important even if people
are mean okay so moving on we come to
the post-processing and final evaluation
and so here too it's critical to Think
Through different types of outputs and
errors in order to select the most
suitable metrics to measure model
performance um and it's in this stage
that Concepts like fairness diversity
and inclusion are really important um
some of these are more developed than
others uh so let's talk about them a
bit um so for those of you who are not
deeply familiar with fairness um one of
the most basic ways to think about
fairness um is in terms of a confusion
Matrix so what we have here um are a 2
by is a 2 by two of references the
ground truth the thing that's actually
there and model predictions the thing
that the model is guessing um when
something exists and it's predicted
that's a true positive when something
doesn't exist and it's not predicted
that's a true negative the error types
we really care about when we're talking
about fairness um are false positives
and false negatives so these falses
right so if it doesn't exist but it is
predicted that's a false positive um uh
if it exists and it's not predicted
that's a false
negative um but in one case uh a false
me a false negative may be better than a
false positive and in another case a
false positive might be better than a
false negative um so it's very critical
to to Define what kind of fairness you
care about as the equality of
essentially false negatives across
different groups equality of essentially
false positives across different groups
when you have these equalities based on
these kinds of errors this is what we
mean when we talk about fairness um so a
case where false positives would be
better than false negatives is something
like privacy in images um so a false
positive here something gets blurred um
that shouldn't be blurred that's just
sort of usually a bummer uh but a false
negative could be something that that
needs to be blurred not getting blurred
so that's like identity theft uh so we
clearly have uh a decision to make in
our model development which kind of
Errors we want to focus on um and by
thinking through in context it's clear
that in a situation like this we want to
make sure to minimize false
negatives um false negatives can be
better than false positives uh in
something like spam filtering uh so here
in email that spam is not caught so you
just see it in your inbox um usually
that's just a bit annoying um a false
positive email flag the spam is removed
from your inbox so if it's from a friend
or a loved one or it's a job opportunity
uh that's a loss right but note that the
usage in context really really matters
here so if foreseeable users include
those that are easily scammed leading to
a loss of wealth then false negatives
have a greater risk and model
development needs needs to balance these
different populations in determining the
optimal models the current
state-ofthe-art
nothing so low hanging fruit we can do a
lot
better um okay cool um so finally uh
people see the output uh in this process
and uh it begins again as people
interact with this output and then the
biases uh are further encoded uh and
more deeply ingrained in the model
um I'm not sure how I am on time because
I started a little
early um I can go time if you yeah so I
want time for questions um let's I have
questions right now if you want I can
ask I have a few questions right now
yeah yeah what I'll do is I'll just kind
of like wrap this up really quickly um
and then we can go to questions um
perect yeah so here's some basic steps
forward um and this is sort of what I
would like you guys to remember if you
take anything from this talk um so uh we
need to align considerations of our own
values to this larger group that we're
working in these stated values um
thinking through uh if the technology
should exist at all um by using this
sort of analysis of foreseeable harms
and risks by the expected populations by
the people who will be affected by the
model and how it will be used in context
um uh establishing basic organ
organizational values can be informed by
things like virtue ethics
consequentialism deontological ethics
where things like privacy fairness
Justice consent all these things come
into play and then can inform you know
what we prioritize as we
develop um once we start establishing
these things as intentional processes
throughout the pipeline requiring
artifacts and documentation
um we can start to be able to foresee uh
even more potentially problematic
outcomes and make more informed
decisions on
releases um this goes to the question uh
earlier one of the critical ways to make
this move forward is incentivizing
ethics and form development um so this
is things like what gets uh put in
review rubric what kind of awards we get
um what sorts of things um are rewarded
when they're talked about um and one
method I've worked on to capture much of
this uh is model cards um a large part
of why I joined hugging face is because
they were trying to come up with ways to
scale model cards and I thought I could
help um but it's essentially an artifact
to report sort of all of the
above um and finally throughout uh the
critical critical most serious thing
that you can do uh is encourage and
reward for foresight thinking through
the outcomes with different people in
different contexts and here's here's
what's important to know some people are
good at this and some people are bad at
this so if you're bad at this don't say
we can't develop AI with foresight just
accept that maybe some people have a
strength in this and you don't right um
I've been corrected that instead of
foresight uh what's more important is
rapid hindsight no some people are good
at foresight let them uh stop you from
having the need for Rapid hindsight
right um so this requires placing people
in the appropriate roles to make this
aspect of development really shine okay
thank you I'm done I appreciate
it thanks a lot for your talk Meg it was
very
insightful um so I have a few questions
that were ask on the Forum let me pick
the first one um
so how do we process how do we protect
ourselves against machine Learning
System designed primarily without ethics
in mind because well obviously algorithm
being run out in the world right now and
yeah yeah I
mean this is a really hard thing to
answer um I previously thought that part
of the solution was to um make make sure
that the development that has already
taken place uh is retrofitted as much as
possible um so taking a look at the
kinds of evaluation data sets that we
use the kinds of training data sets that
we use doing uh more further analysis
into what errors it makes and then
essentially fine-tuning updating the
model based on that um unfortunately I
think in part because of this issue
where a lot of people in ml in ml are
just not that great at foresight uh and
don't really see the point of it um we
we're going to need regulation um so
that's that's sort of what it I think it
comes down to at this point makes sense
um another question that was bit too
long so I'm going to read the the last
part from The Forum so model
transparency seems to be a credition
precondition of the mentioned ethical
goals Dr Mitchell seems to emphasize the
role of input data isn't the model's way
of decision making equally important
feature importance etc for ethical AI
um uh equally important um I would say
all of the above is important I'm not
sure how to extract that to numerical
values to to say that it's that it's a
quality um but uh one of the reasons why
I do have a focus on transparency is
because it's incentivization of good
practices throughout right so if you
have to report uh how your model does
with all these errors then you're
probably not going to release it unless
it does well uh so as soon as you have
transparency this is a way to
essentially incentivize good practices
throughout um input data I do have a
focus on in particular because it tends
because it's what the model knows it's
it's the mo the the model's knowledge
Source um so there's so much that can be
said about evaluation and all these
other things but the issues very much
Start um with uh what What's in the data
um so I would say that's that's a
highest priority
yeah and then speaking of data uh what
about retraining models that are already
trained on unal data is it possible to
fix them somehow yeah so I mean by
unethical data I'm taking that to mean
uh you know data that that has a lack of
consent data that our licenses and
copyrights are are not being paid
attention to data with pii um so I'll
assume uh that's what you mean by
unethical data um I I do think it makes
a lot of sense to to retrain and
fine-tune and actually some of the work
that I've done um has been specifically
on on bias mitigation techniques and
identification of stereotypes and things
like this um whether it's sort of
ethical to retrain um I guess that's
better than not retraining if it's going
to be deployed so I guess it depends on
the context that you're in if you're
writing a thesis maybe this will become
your topic of how to start from scratch
if you're in an organization and you're
retrofit maybe the best way is to to
retrain or fine tune a bit um so I think
it depends a little bit on on context um
yeah I I guess that's that's the sort of
most relevant answer I have to that okay
there are lots of questions about
debasing like I have another one here
while training the pr model is it
possible to add a step including the
biasing algorithm are emerging
algorithms today could you make some
interesting bias
disappear um yeah so so there's a lot of
work to be done in data that has tend to
be not incentivized there's also a lot
of work to be done in models and this
and this has been more incentivized um
and so I just want to make clear I guess
that one of the cool tricks is around uh
sort of multitask learning um so uh bias
mitigation techniques that that tend to
work um you know reasonably well have to
do with predicting a few things at once
and then uh discouraging one thing while
encouraging another so for example say
that you're trying to predict whether or
not um someone should get promoted at
the same time you can predict gender and
if you find that the promotion decision
um is predictable via the gender then
you know that you have a bias system
that's biased for gender so one of the
adversarial mitigation techniques um
that you know that's pretty fun and
there's a couple papers on this um I
wrote one uh Al Bell uh L a really cool
paper on this is you essentially um
invert the gradient so you predict both
and for everything it's learning about
gender you say the opposite of that you
essentially multiply by negative one and
back back propagate right so that says
like you don't know what gender is you
and you just tell the model until it's
essentially random um and that's really
a cool way of sort of removing a gender
bias there's other things you can do
with like pairing um but but a lot of
the key techniques are around uh just
this like having two or more things that
you're modeling at once and encouraging
one while discouraging
another that's a very cool technique
indeed um another question let me scroll
a little bit uh yeah there might not be
a magical solution but what's the
easiest way to determine whether a text
generation model is not racist or sexist
and how do we solve this quickly without
losing too much of training data solving
it quickly huh
yeah yeah I mean obviously as quickly as
possible is good if if there's sexist
and racist things in the world so you
know fair enough um so I think that it's
I don't know that it'll ever be possible
to have a model that's not sexist or not
racist because I don't know that it's
ever possible to have a person who's not
sexist or racist right I mean these are
all sort of based on humanistics and how
our our brains work as as we're brought
up and so um part of you know going
through like Ally training and you know
these kinds of things means that you
have to recognize it and uh proactively
address the kind of patterns that you
tend to do um so you would imagine that
similarly for a model you would put in
place normative constraints like you do
with people uh so that means saying that
um you know we are going to mask gender
so there's no distinction between gender
when we're training our toxicity model
and so when we have an input gender we
we have no no way of handle we don't
know what it is right um and so so we
won't correlate the two and toxicity um
actually like particularly has a bias um
towards the term gay and and lgbtq terms
um so there are things you can do like
masking those or adding more data anywh
who um the point here is that um we can
put in place specific normative
constraints based on our social
knowledge so this is where social
science SST come in sociologists come in
on how racism and sexism and things like
this are expressed so we can do that and
that's definitely like a quick fix um
but a full-on fix might might not be
possible um but you know yeah do do as
much as you can
definitely another question around the
bias in data is how could we balance the
filtering of bias between freedom of
speech and offensive biased consent oh
that's such a good question yeah I been
struggling with this because um ideally
we can ground AI development in like
fundamental rights or human rights um
but if you are American Centric then
freedom of speech is like you consider
that like a fundamental right right but
like in other places uh like uh privacy
is nice or like you can imagine freedom
from discrimination as to be a cool
thing um or perhaps a basic right that
people should have uh but that you know
in conflict with the with the free
speech thing um so I mean I think part
of the issue here is that when you say
free speech um there's there tends to be
an assumption that there is some sort of
uniform sampling over all data instances
so that Free Speech as a whole is is
reflected so you have like people
talking about women positively and
people talking about women negatively
and so it's all free speech we want it
all um but what in fact happens is that
um the data sets tend to be heavily
biased towards men and using um and
using sources where there's misogyny so
like Reddit um and that is not
counteracted by free speech about uh
women being awesome or whatever uh so um
so yeah I mean like one of the things
about free speech that that is kind of
fun to do is actual data collection so I
was part of a project uh called project
respect that went to like gay pride
parades and and had people say like
really proud things about their identity
and their uh you know uh their lgbtq
like associations and stuff like that
and that created like tons of data that
we can use in training a toxicity model
um that's a lot of work and you have to
have willing participants and hopefully
compensate them whatever they should be
paid um but um but yeah I mean it's
there there's a few things you can do
but keep in mind that the free speech
idea is a little bit hampered by the
fact that the free speech that we
collect is is biased and tends to be
more misogynistic and and racist than
maybe a larger sample would
have yeah definitely uh and around like
the the compensating participant for
creating the data I have another
question which is is making it mandatory
enforcing to create data sets where each
participant is highly paid going to put
a lot of pressure on researchers from
low resource institutions and researcher
from low resources countries yeah that's
a really good question um I think this
is where things like um governments
start to play a role um
and it's it's I think it's also kind of
important to note that this also is very
destructive to private companies that
are um that have you know a lot of money
so um it's sort of like I sort of feel
like the larger the organization the the
more this will hurt them right so if um
for example you require Google to pay
everyone for the data they're producing
um Google you know Google's business
model would be basically destroyed um so
you know in some ways we might think of
methods where different compensation
tiers you know make make sense for
different kinds of organizations you
could get this via regulation so you
know if you're a grad student um paying
you know a minimum wage or whatever for
your annotations um that's that's pretty
reasonable that's pretty normal um if
you're you know a multi-billion dollar
Corporation um perhaps paying a lot more
is is worthwhile uh especially because
like you're making profit off of this so
it should be a percentage of that profit
in some way um so I think there are ways
not to over disadvantage um uh low
resource uh groups um or or
organizations
um and I think that it could go the
other way like very much harming larger
private
companies and one last question which is
a bit more personal is what does the
regular day-to-day work look like for
you oh well so I just joined hugging
face full-time on November 1st um and
before that I was doing like a lot of
Contracting and Consulting and so I've
just been sort of all over the place I'm
someone who wears many hats like I like
talking about philosophy and then I like
talking about statistics and then I like
coding and then I like writing and uh so
I really don't think I have a
traditional day uh I will say that I
hate waking up before 900 am Pacific
time and so presenting at 9:30 was a
feat I hope I seemed coherent um I think
that's that's basically the only the
only standard throughout my day the only
common thing is that I try and not get
up until 9: um but other than that yeah
jump around between coding and thinking
through math stuff and writing papers
and reading papers and sometimes
answering
email yeah
sometimes well thanks a lot for for your
time this morning because this was super
interesting talk and very insightful
answers to all the questions awesome
thank you
guys and uh I'll at we introduce our
next speaker while I say bye to Margaret
Mitchell sure so I'll just add
Chen and Matthew hey guys how you going
I think Chen you're on mute he he so so
this is uh Matthew and uh Chen they're
from um the Caris team and Matthew's is
a machine learning engineer um who
focuses on High level modeling apis and
he studied computer Graphics during
undergrad and has a masters from
Stanford and uh Chen is a software
engineer from the Caris team who also
focuses on these high level modeling
apis um and also has a masters in
electrical engineering from Stanford and
is really interested in simplifying code
implementations of ml tasks um so we're
really excited to to have you here to
sort of help people understand kind of
what caros is you know maybe convert all
the P torch die hards um and before
getting started I just want to um show
people um just quickly how they can ask
questions so if you go to the hugging
face forums um you can click on the
course um category and then you can
scroll down to where we have Matthew
Watson and Chen sien and here you can
just post your questions and we'll be
voting on them based on likes and then
and also the the forums contain the
links to the to the collab notebooks
that Matt and Chen is are going to
use cool so with that I think uh Matt
you're first right yeah I'll be first so
we'll um add your screen and um yeah
we'll uh see you soon okay great um yeah
let
me actually start the
presentation um okay cool yeah thanks
for joining um yeah I'm I'm Matt and I'm
speaking with Chen we're both on the
Caris team inside of tensorflow um and
yeah today we're going to be talking to
you about NLP workflows inside of the
kth
library um so quick just kind of plan
for the day I'm going to do the first
part of this talk and I'm going to be
just talking about kind of building like
very basic models very basic NLP models
inside of Caris and showing showing what
that looks like um then I'll hand it
over to Chen who's going to talk about
kind of like pre-training and
fine-tuning using hugging face and caros
um hopefully all of that will take about
half an hour and then we'll have
questions um cool so before I even dive
into my side of the talk I wanted to do
two slides like super quickly just
giving an introduction to to Caris to
anyone who hasn't seen this before um so
yeah first off what is Caris um it is a
deep learning API and the kind of one-
sentence version of it is deep learning
for humans um but what that that means
in terms of how we're trying to build
our library is really centering it
around like simple apis that kind of
reduce cognitive load as you're building
up your models so you can focus on
building your models um and really
trying to focus on kind of a minimal
number of actions that you need to take
for common use cases so basically you
can get the Caris library and spin up a
lot of examples very quickly start to
kind of explore your problems bace very
quickly that's a key goal with the Caris
libraries um and then another way you
can think about it is caros is the high
Lev modeling and training apis that will
come prepackaged with
tensorflow um so yeah that's Caris uh
and then I wanted to also introduce just
two kind of basic abstractions that
you'll run into over and over again with
the Caris library and that's layers and
models um so yeah you you might be you
might have seen something familiar to
this somewhere but basically you can
think of layers as kind of these Lego
blocks of like common motifs that you'll
see in many machine learning model
architectures um and with Caris you can
take these Lego blocks you can combine
them in a whole lot of expressive ways
um and then yeah snap them together and
a model will be this collection of
layers that kind of map from your inputs
to outputs and the model is the going to
be the thing that you can like train
that you can save that has all these
highly level apis available on
it okay so that was a super super quick
introduction to Caris the library as a
whole um now I'm G to do my my side of
the talk that's on these kind of basic
NLP models we can build with Caris um
and I'm going to be doing this with the
sentiment analysis data set from IMDb uh
if you haven't seen this I think it's
like 25,000
in the training data set that are just
movie reviews like three or so
paragraphs of text describing a movie
and we're just trying to build a model
that we'll predict is this a positive
review or a negative review um and my my
goal in showing all this is going to
first be to just kind of show some basic
building blocks for Caris I'm working
with texting Caris um and then the main
kind of the thesis of my side of the
talk is going to be about this this
triangle on the right hand side here
which is that using Caris you can
hopefully go very quickly from an idea
to an experiment to seeing some results
um and use that to inform kind of the
next experiment you run in a nice tight
Loop so we'll actually build up like
four very quick models on on my side of
the
talk okay um so we're g to be writing
this all in code so let's let's start by
downloading our data set um yeah you can
see what what we're doing here is we're
first just downloading
a a bunch of plain text files in a
couple directories and then we're going
to use a Caris utility to load them as a
tensorflow data set um and yeah we're
batching things and when when data is
flowing through tensorflow you're going
to be seeing everything going through as
tensors that's kind of how tensor flow
works and you can see once we've loaded
our data set and batched it we have
these batches of size 32 where every
input we have just a single input
feature coming in for each of our
examples which is a string I I've
truncated it here because they'd be too
long to show reasonably on a slide but
think each string is you know many
paragraphs of text and then each label
is a one or a zero and a one here means
this is a positive review a zero means
it was a negative
review Okay so we've download our days
set you kind of immediately hit this
roadblock that you'll you know hit every
time in NLP which is how do we represent
our data um you're not going to feed
strings directly into any neural network
architecture so you know you need some
sort of transformation of your input
before you can even really start
worrying about your model
architecture um and one approach that
you can use inside the carass library
for this is the text vectorization layer
which we'll see in code in a second um
but yeah it's it's one of our Lego
blocks that's kind of just centered
specifically around working with text
input um so it'll do a few different
steps they're they're all configurable
but by default it'll have a
normalization stage where you'll remove
punctuation and lowercase um a splitting
stage and the I think the default option
will just split on whites space so
splitting into different words um and
then there's a number of different ways
you can map from from that idea of
string tokens to some sort of numeric
inputs for your
model and the the simple model the first
thing that we're going to build you may
have seen this before but it's called a
a unigram model you'll also see this
called a bag of words model um and yeah
it's a it's a nice simple thing we can
build to start trying to guess if our
reviews are positive or negative where
we're essentially just going to look at
what words are present in a review and
what words are not present in a review
so we're going to build up this
vocabulary of the most common words in
all our reviews first um and then when
we get around to kind of encoding each
individual review we're going to encode
each individual review is this big long
array of zeros and ones that kind of
aligns with our vocabulary that we've
learned and if you get a one that means
that word was present in your bag um so
the word movie is present in this review
we get a one at that movie index good is
not present we get a zero um so that's
the rough idea for the model that we're
going to build let's look at this in
code um yeah this is actually a complete
training example for this inogram model
but I'll try to break this down step by
step so first off kind of the overall
flow that we're doing here is we're
going to Define this kind of symbolic
input at the top which is saying that we
have you know our our inputs are a
single feature it's a single element
string that's going to be coming through
then we're going to transform our inputs
with some layers it's two of these Lego
blocks that we're going to snap together
um to transform our inputs to outputs we
build a model over our inputs and
outputs and then we train that model um
so that's that's the rough flow um and
then we can step through these two kind
of blocks here the first first thing
here is probably the most Dench which is
the pre-processing um and this is where
we're going to use that text
factorization layer we talked about so
first thing we're doing here because
we're for a second we're just going to
be working with our our features and not
our labels is we're going to map over
our data set we're going to toss out the
label just to give us the the plain
reviews alone in a data set um then
we're going to build our first Caris
layer and you can see when you're
building a Caris layer you normally will
initialize these with some amount of
configuration as to how they're supposed
to work um so this output mode multi hot
is where we're asking for this bag of
words en coding of our input and then
this Max tokens 10,000 is just saying
how big do we want our vocabulary to be
and we're saying we want to learn the
10,000 most frequent words um in all of
our reviews and the way we actually
learn the 10,000 most frequent words you
could actually Supply a vocabulary
yourself if you knew that and you had
trained it yourself um but you can also
use this utility function that we have
called adapt which is actually going to
Loop over all of our features all of our
input reviews and just learn the 10,000
most common words that we see in all
reviews um so when we've learned our
vocabulary we're ready to actually call
the text vectorization layer on our
inputs Caris layers when you want to
call them you can just treat them as
functions and call them on your input
and that will give us this bag of words
representation that we just talked about
um so from there then we you still need
to build a trainable model on top of
this um and the simplest thing we can
think of to do there is kind of just add
a linear layer on top of this and we'll
just call it a day there so we're gonna
yeah we we mapping each of our inputs to
this 10,000 long array of ones and zeros
and we're just going to learn a linear
combination of all of those 10,000
values pass that through a sigmoid
function and that's our whole model um
so yeah just logistic regression on top
of our bag of words
um yeah so once we've done that we can
Define our model uh compile is a Caris
function on models for just kind of
setting up how your training is going to
work um so we can specify our loss
function here we can say we want to
track accuracy here and then we just
train for five
epics um so yeah that's a complete code
example in Caris I encourage you to try
this out in a collab if you would like
um but yeah essentially now we've we've
built our first model and we've kind of
made two design choices here we've made
a design Choice around how we want to
represent our input data as some sort of
numeric input um and then how do we want
the actual trainable architecture of our
model to look and now we're now that
we've built up our first example we're
just going to iterate on a few different
ideas for how how we can build up our
model for the rest of for the rest of my
part of the
talk um so take one was what we just did
unigram model we're taking in our input
reviews we're mapping it to a bag of
words where we're throwing out all of
our sequence data we're throwing out
everything except just what words were
present what words were not present we
don't even care how many times the word
film appears we just want to know is it
in the review or not um so this is the
exact code I showed you before I've just
kind of zoomed in on the actual meat of
the model here because we're going to
just be iterating on this for the rest
of the talk um but what does this look
like when you actually run it how does
it perform um first thing you can notice
here is that it's a very small model
that we're actually training it's just
10,000 parameters because we're just
learning a weight term for each term in
our vocabulary and that's it that's the
whole model um and we're actually
already doing pretty well we've got 89%
validation accuracy um on a whole that
data set
um so yeah that was that was take one um
and now we can start doing what
hopefully Caris is going to make it easy
to do which is iterate on a bunch of
different ideas kind of explore the
landscape of this little problem that
we're working on um so take two we can
try is a Byram model um and diagrams are
just fancy word for a pair of words
together so if you're looking at this
first sentence this film is a Bagram in
this sentence a travesty is a Bagram in
this sentence and we want to build the
exact same sort of bag of words idea but
we now also want to look at byrams in
addition to unigrams and you could think
how this might be useful already because
not good the Byram not good means
something very different than the the
unigram good so by adding in that little
bit more know of our kind of sequence
data coming in maybe we can build a more
expressive
model um so what does this look like in
code we actually have to just make very
very small changes to our text
factorization layer this is the
pre-processing side of our model
definition um and we're just going to
make two changes here we're gonna one
we're going to just bump our vocabulary
size we've changed from looking at only
words to looking at words and pairs of
words so kind of our vocabulary space is
a lot bigger so we want to learn a
bigger vocabulary in inside of that
space um and then this NRS 2 argument is
just where we ask ask the text
factorization layer to also look at
byrams as well as genograms everything
else stays exactly the same we keep
learning our logistic regression on top
and that is our entire model so if you
ran this in code you would see okay we
bumped the vocabulary size um from
10,000 to 20,000 parameters um just
doubled our vocabulary size so we've
doubled our model size um but we've
actually bumped our accuracy so now
we're over 90% accurate on predicting
whether review is positive or negative
so that's great it was a nice quick and
successful experiment that we ran um and
yeah now we'll try two more models on
this side of the talk um and the first
thing what what we'll do for the next
two models that we build is we'll we'll
switch from we'll we'll say forget the
bag we're not going to do the bag of
words model anymore um and we're going
to do this very common thing in in NLP
which is we're just going to map each
word to an integer index um so we're
keeping our sequence data in our
pre-processing and we have this movie
was bad this list coming in we're going
to map each individual word to an index
so this is the fourth index in our
vocabulary so this maps to a four movie
is the first index it maps to a one and
this output is going to kind of be our
pre-processed inputs that we're working
with for our last two examples here um
okay so what can you do with integer
indices for Words the common thing
you'll do um in most models is embed
them so we're going to take our input
sequence we're going to take our input
string we're going to map it to a
sequence of integers and then we're
going to map each individual integer to
a set of trainable parameters so the
index 11 will map to in this example for
trainable parameters same with 20 um we
GNA learn this big long kind of
horizontal embedding that you can think
of where we're just pulling out
individual columns for each word and
hopefully that learns like this Rich um
but low dimensional space for all the
words that we have coming in um and then
because we're just trying to keep our
models simple on this this side of the
talk we're going to do the simplest
thing we can think of with our sequence
of embedded Words which is just average
them together not even try to do
anything fancier than
that okay so let's dig into this in code
um we still have our pre-processing
block and the the part where we apply
our actual trainable layers the
pre-processing block is going to change
a little bit here um we're switching our
output mode so this is where we're
saying we don't want our bagged model we
actually do want a sequence of integers
coming out of our text vectorization
layer our vocabulary size will move to
10,000 and then output sequence lank
this is just something to keep this
example simple where we're going to say
for every input review
if it's less than 250 words we're just
going to pad this with zeros till it
becomes 250 words and if we get a review
that's longer than 250 words we're just
going to chop it off at 250 words and
that'll just keep training simple here
um and then for the actual trainable
model we have a good bit of new stuff
here um we're using our embedding so
this is where we take each word and map
it to an embedding for each word um
rather than for like on the slide I was
showing we're actually mapping to 32
different trainable parameters for each
word um
we apply some Dropout and this is just
because we're building a much more
expressive model with a lot more
parameters here so it helps to bring in
some Dropout just to avoid overfitting
and then we're doing this very simple
the global average pooling 1D is really
all it's doing it's taking everything in
your sequences in your input data and
averaging them together um so we'll
build that model we'll still build a
logistic regression on top just to map
everything to one single
prediction um and that's our whole model
so if you were to run this um you'd see
we've kind of exploded our parameter
space a bunch because our embedding is
now it's pretty big we have 320,000
parameters compared to 20,000 for our
simple uh byr model that we were looking
at last time and we've actually not
improved our our validation accuracy at
all we've gone from 90% with the byr
model to
87% um so maybe not the best line of
inquiry to keep going down further but
at at least this was a quick experiment
we could run try out and see how it
works um okay and then the last kind of
model that we're going to build here is
a recurrent one and probably a lot of
people in this talk have seen this
before I won't go too deep into the
details of tower recurrent neural
network works or anything but the idea
is we're going to build this lstm model
do the exact same Transformations as we
were doing before where we take our
input sequence we map it to a sequence
of integers for each word each word maps
to an integer we map each integer then
to an embedding for each word which is
hopefully this this low-dimensional but
Rich representation of each word um and
then we feed all of those into this
recurrent neural network which is going
to be a trainable bunch of cells where
we process each word one at a time and
as we're processing the word our our
trainable cell will have some sort of
memory of what it's seen before so
hopefully the lsdm can learn some
interesting things about our sequence is
coming in and start to remember what
words it's seen word what words it's
seen before a particular word that it's
looking at in a
sequence um so that's the idea what does
this look like um in terms of code we'll
we'll keep our input representation
exactly the same we keep our
pre-processing the same we keep our
embedding the same Dropout everything's
the same except we're just going to
switch out our pooling layer with a
trainable lstm layer and that will be it
um so if you ran this this is our most
expressive model um yeah we started with
a 10K parameter model and now we're at
336 parameters um and we're actually
doing the worst of all of them with our
biggest model um so we top out at about
86% validation accuracy and you can see
we're already really struggling with
overfitting um okay so that was a just
very quick introduction to what it looks
like to build four different simple NLP
models with Caris and again rough idea
and what we're trying to do with the
library is to make it really easy to run
kind of experiments like this to come up
with some idea for either how you
represent your your input text um but
when you feed it into your model how you
pre-process it or an idea for how you
want to actually architect your model
itself and go very quickly from that
idea to some sort of results um so yeah
that's entirely what we're aiming to do
with the Caris API and you can already
see how it's it's given us some useful
information into our our problem here
right
we've found this nice sweet spot the
endr models actually turn out to be when
you're not doing pre-training and there
could be a lot of reasons you don't want
to do pre-training in in a problem like
this um where you you know we built a
model with only 20,000 parameters that
is going to be incredibly
computationally efficient it'll give you
really fast inference times um and it's
already greater than 90% accuracy so you
can play with this more certainly you
can get to even better accuracy with
with with engram models but we've
already kind of found this nice kind of
sweet spot to to go dig into and you
could try this you could try more things
attention mechanisms um whatnot but yeah
that was just a quick quick intro to
trying a few different simple
architectures and caros um but yeah if
you did want to get to like
state-of-the-art performance you'll want
to do pre-training and find tuning so
for that I will hand off to
Jen awesome thanks a lot Matt
I'll add Chen to the
stream um would you like us to do
questions for your section all together
at the end I think we're g to just do
questions at the end if that do together
perfect thanks right I'm going to go
present here so I'm just gonna add uh
Chen's slides great
so sounds good so take it away thanks
cool uh hi my name is Chen in the second
half of the hug I would like to
demonstrate with real C examples on how
to do fine training with huging face and
caras the main take way we hope you'll
get from this talk is you will find it
simple to Define training with ha caras
and the hugging
face we will use the same task as the
first half the IMDb sentiment analysis
in the second
part and some background for people not
familiar with fine training the idea is
starting halfway towards our Target
instead of building a model from scratch
it's pretty similar to buying a house
imagine you want to buy a house usually
we do not buy an empty land and build a
new house but we buy a house and do the
remodeling the remodeling process is
pretty similar to fine training idea and
in our IMDb sentiment analysis task we
can load a pressure and burn model and
build our model on top of
it and what is bird bird stands for
bidirectional encoder representations
from Transformers it was published by
Google in 2018 and has gain great
popularity in I Community due to strong
performance explanation on bir is beyond
the scope of this talk the main thing we
need to know is bird is very good at
extracting representations from texts or
say birth is a good text
encoder now let's take an overview how
the model architecture we are going to
build first of all we have the Tex
loaded from IMDb data sets the movie
reviews and we add a special token in
front of each movie named CLS we will
come to the purpose of this token very
soon the first layer we have is a bird
tokenizer as we mentioned in the first
half neural networks cannot understand
string input so we need to map this
sequence of in of text into a sequence
of integers it's a job of this bir
tokenizer the second layer is a bur
encoder it takes in the sequence of
integers and map it into a sequence
vectors and here we only take the first
Vector the vector for the CLS token to
represent the whole sentence this is an
common RP practice basically if we set
up the model properly the information of
the sentence is going to be fused into
the first
token now we have the vector
representation we add a third layer the
fully connected layer to map the vector
into another space which is usually of a
smaller Dimension we call a prodct proed
representation this is going to be our
final representation for inut text and
the last layer is straightforward we
inut this projected representation into
a softmax classifier the output would be
the probability of this movie review
being positive and being negative that's
all for the model before we look into
the code I want to pause the for a few
seconds I want to think about one
question but how complex do you think
make the model work is like like how
many lines of code do you expect to
write or like how many hours to expect
to spend here I want po here like 10
seconds and let you think about it just
give it a
guess okay cool now with the answer in
your mind let's look at the code we have
in total five steps the first step is to
load the tokenizer and PR birth from
hugging face Library as highlighted in
the r box if you haven't installed the
Transformer Library you need to copy
install to install that and you pick up
the model you want to use and in our
case we pick up the P bird tokenizer
fast and this Ste bir model and if you
are not familiar with the input and
output format of the model you can play
around with it this is one sample
output and the second step is to load
IMDb data set into a memory this part is
pretty similar to the first half so I
will not jump into details of it but I
want to pause here for a few seconds to
let you look at the
code let's go to step three step three
is to tokenize the data set as
highlighted in the r box as we already
have the data and the tokenizer we can
simply inut the data into the tokenizer
output like train encodings validation
encoding
will be the sequence of integers we
described
previously and we do one more conversion
we convert the data into the format of
tensorflow data set so it can be
understood by our
car a minor detail here starting from
this tensorflow data set conversion
every code is going to be based on
tensorflow and the caras
library now we are going to step four
build our text classification model on
top of the pr bird to build a carass
model you want to subclass the car model
class and because we are taking in an an
external bir encoder we want to add an
argument named the encoder into the
arist of the Constructor and we
explicitly set a trable field as true so
that the gradients can be back
propagated all the way towards the pr
and burd model the rest part
straightforward we have fully connected
layer which is the self. den one here
and we have soft Max classifi
which is self. dance Too here and the
job is a common practice to avoid over
fitting and remember to Chanel
components together by overwriting a
call method it's very simple you just
specify the output of a previous module
after to input to the next
modu and we are almost there last step
is to tr the model we want to First have
a model instance and do some
configurations like what Optimizer will
you use and what's loss functions you
and some Matrix you want to pay
attention to and you come mod off F to
get a model
TR and this is output of this model
training process we see that within two
epok we arrive at a validation accuracy
up around 92% which is good but we still
see the overfitting problems like say EO
3 we see the training accuracy being 97%
but the Val accuracy is only 91% this is
is because even though the add-ons like
the fully connected ler is very small
but the birth itself is very big so it
has over fading problem and that's all
for the code and we have shared this
code in the Forum I would encourage you
to check it out and especially if you're
interested in buing this overfitting
problem you can use this code as a
starting point and write a code on top
of it and that's all for the talk we can
go to the question
session awesome thanks a lot Chen Matt
for your really nice talks and careful
breakdown of caras um got a couple of
questions here some are a bit technical
um so one question is uh Matt you
explained this text vectorization class
yeah I just this in the here so we can
see it so you know it kind of looks like
a
tokenizer um in some sense and the
question is does it or caros the library
more generally allow users to kind of
train their own tokenizers so if they're
trying to sort of train B
themselves um they do this yeah um
that's a great question tax
factorization is going to give you um
just kind of more basic tokenization so
it'll be good if you want to do like a
character level tokenization or a word
level tokenization um but if you want
like ber on most of these transform
architectures you're usually going to
want subword tokenization um which text
factorization won't do um but there is a
library you could check out called TF
text um I mean for one you could use the
hugging face tokenizers for sure there's
also a library called TF text um which
you could look at which will allow word
piece and sentence piece kind of inside
the tensorflow graph um which there
could be times where you want all of
basically your Transformations happening
inside the tensor flow graph so TF text
is a way you could do that awesome and
speaking of tensor flow the next
question is uh people tend to use
tensorflow and caros interchangeably and
I was wondering what the differences are
or when should you use one over the
other
um yeah I mean I think the way you can
kind of I mean tetor flow is a a broad
term for a large ecosystem but if you
think of the core tensorflow apis as
being yeah kind of a lower level Caris
is a higher level training IPI layered
on top of it um so they're not
interchangeable you can use tensorflow
without using Kos um but Kos is usually
the easiest way to build a model inside
right go ahead
say I think tensorflow 2 kind of if I
remember correctly Kos became kind of
like a first class citizen is that
correct yeah that that this is not the
case in tensor flow one but the tensor
flow that's the the new picture I was
about to say in our initial launch of
tensorflow two we put car tens flow
together so in that case we have no
difference between that because you when
you want to call caras you just do TF
car things and we have split the code
outside car outside tensor flow the car
Ripple is not separate but mainly
speaking like for people building model
the first place we go to is still using
the caras because it's like it has the
layer and the model obstruction there is
more closely aligned with a building
model
process yeah definitely I totally agree
like if you want to you know build an
experiment quickly you just want to just
get done fast you don't want to you know
get into the lowlevel you know graph and
stuff but I think you know from my old
experience in tens flow one using the
graph was really great for performance
uh reasons
sure this is maybe more of a
philosophical question um so since the
Transformer architecture took over NLP
do you see a good use for previous
models like lstms because I think Matt
you showed a pretty nice example and you
kind of answered this a little bit with
this uh you know 20K parameter or 90k
parameter model I think yeah I mean I
think it totally depends on your problem
set but in the example we are looking at
like this it's a very small amount of
data and you might be worried about
being doing pre-training because maybe
you you know it's a lot of kind of extra
data to bring in it's hard to validate
you might be worried about bias um you
might be worried about computational
efficiency when you're doing inference
so in this example the lstms actually
weren't a great way to go down but the
engram models which are some of the like
simplest things you hit in like chapter
one of any NLP textbook are actually
really really performant and it's good
to remember they're out there if you do
want like a really simple model that you
can train that's surprisingly quite
effective
yeah I totally agree I think like one
thing we sometimes forget is like you
know Transformers is not like the first
thing you should reach for maybe like if
if you're building like a real NLP
application naive Bays with engrams is
like a really strong Baseline and you
know you'll often find that the extra
complexity you know causes you some
problems in prod um let's see so this is
maybe a question for Chen um so there's
a lot of optimizers out there these days
and is Adam really we need or should we
be looking at other optimizers because I
think you mentioned in your code when
you were building the model. compile you
had Adam and yeah I know that like
there's like a history of Adam and Adam
W and all these kind of things but I'm
just curious like just generally what do
you think about the space of optimizers
and yes where where this is developing
so that really depends on what you want
so if you want to build a state of our
model Adam is definitely not enough but
if you want to play around with a with a
model I want to try some development
work Al atam is good enough but one
thing there even if you are using atam
you may need to look at something called
moving average Optimizer which is inside
tensor Ado it's just like rapid
Optimizer but you keep the moving
average of the model variables so that
it gives you some boost in the
performance that's something you want to
look at but if you're looking at
building some state of ours model maybe
you can check out something like SCM s
Optimizer and some other
like Advanced strategy it I would say
like short answer is item still good but
if you're looking on something very
Advanced you can think about some other
approach awesome let me just have a
quick look to see what else we have
here
um okay so another question we have
is in chat give me a
sec so the question is um Can layerwise
learning rates help uh during fine
tuning when it comes to B
models uh the short answer is I think it
can but it will be really hard to
configure that probably you should try a
lot of times before you can actually get
the model converged because if you are
find training with such a b model it's
very aprom if you set up like some layer
using a wrong learning rate maybe of
some problem but definitely with at a
high level it should help with the
training process but probably not in the
aspect of doing the the the speed of
making this work but just like
performance the final performance could
be better yep yeah totally agree um and
this is another final question maybe for
Matt I think in um your talk you you
were explaining you know you had these
learning codes or validation um accuracy
and so on and so the question is can you
expand a bit more on this idea of
training loss and validation loss and if
the validation loss starts increasing
can we conclude that the model is over
fitting um yeah if your accuracy is
going down or your loss is going up on
your validation that I mean you things
can be noisy and you want to look at how
much noisy but I mean in a lot of our
with the examples I was showing we
weren't doing any pre-training so we're
just looking at this very small data set
of 25,000 input examples um which is
it's really really easy to fit overfit
at least when you're looking at at NLP
problems so yeah I'm pretty much all of
all of those simple models I showed if
you you keep training for a while you
know notice that you start to be
overfitting a little bit awesome and
maybe I can ask the last question it's a
it's a personal one so um like you know
working at hugging face I you know
contribute to some of the open source
libraries we have here and there's
always this kind of like um trying to
keep an eye out on the future of like
you know where where things are going so
for example you know Transformers
there's a lot of developments outside of
NLP and one thing that um huging face is
interested in is like you know looking
at vision and audio and so you sort of
have to make some design decisions about
your library how it evolves and how do
you make sure things are backwards
compatible and I was kind of curious
like in the Caris team like where do you
see like the sort of like the future
what are the interesting like say
domains or let's say even apis that you
find particularly exciting and maybe you
can shed some light on on where where
the library is
going M do you want to talk about like
car people are doing
yeah I think well I think
one there there's a number of of things
that we're looking at at the Caris
Library um I mean I think one thing we
always continue to look at which
separate from from this kind of side of
the talk is just how do you easily
distribute stuff so I think one one
place we continue to have a lot of work
is like you you've written some simple
training example and you want to like
and as quickly as possible take it to
running on a bunch of machines um doing
a multi-work setting we're always
working on that um but yeah another part
that we're look at a lot is yeah just
kind of bringing in more high level
abstractions for for NLP and CV um kind
of giving more kind of of these Lego
blocks that we kind of give you giving a
lot more that are specific to those
applications to hopefully make it a lot
easier to get get up and running with
kind of more complicated NLP models and
and CB models computer vision models and
one thing I want to add about
distributor training intens flow is we
are trying to make it like even more
seamless currently you if you write this
distributed code in trans for need to
call some strateg things I think there
is some ongoing efforts trying to trying
to make that disappear like you are
going to write the S code which is
called
D like I think it's going to like bridge
the gap between like Stand Out Long
training and distribut training and
hopefully that will be much easier in
the
future yeah that be that'll be like
awesome because I kind of feel that like
where we are now as a field especially
in industry is like there's like a kind
of like you know different age of like
finally you get gpus at your company and
you know like for example like you know
in my old job I was a data scientist and
you could get one GPU and you had to
share it you know with like three data
scientists um but you start to see that
like now that companies are really
investing heavily especially in NLP that
this idea of like distributed training
becomes really important because you
know being able to iterate fast is is in
some sense like the way you make
progress on your problem and yeah I I
think at the moment there's a lot of
like pain in in like you know if if you
think to yourself okay I've got my super
nice say car us you know bit lines of
code you said it's not so much code and
now suddenly I want to scale out to like
you know 20 gpus and then I I feel like
the abstractions are not quite there yet
like at least for me like I feel like it
takes a lot of work to to build out to
that yeah definitely an area word like
and yeah I mean Caris is really all
about that idea the faster you can is
the faster you can like come up with
interesting ideas so we definitely want
to be doing that in the multi-worker
setting awesome so that's it for the
questions um and yeah thank you once
again for taking part in this community
event um it's I think it's given people
a really great primer for um how to
train models in caros in tensor flow and
we have a tensorflow scapegoat as we say
at hugging face who's you know
responsible for building all the API
part of Hing face Transformers and he's
super excited to to see more people um
using this so thanks a lot cool thanks
for having thanks a lot for
joining hey so I think we have a few
minutes before the next speaker right
yes we finished a bit in advance the
previous talk and Matt and Chen were in
advance so we could begin vers in
advance
but it's rare to be ahead of time right
usually exactly so maybe I could ask you
svan um for people who are interested in
like doing projects uh for the you know
the next three days or the last three
days of this week kind of like you know
what should they know um about this and
a lot of people have been asking about
the certificate maybe we could talk
about that for the next few minutes yes
uh so that's a very good idea thanks
leis so let me share my screen again um
to show you how to go on the forms so
the ID is we use the Forum to coordinate
uh Team
project and here it is the Forum again
so if you go in the course
category you'll see that there is a
course project subcategory on which you
can click and then you can brow lot of
existing uh projects uh for instance
this one was Community contributed so
let's have a look at this one sorry leis
I'm not going to pick one of the one you
built uh but this is going to look like
this there is a small description of the
project uh there is a model suggestion
of to fine tune on that project there is
a suggestion of data set or to to scrap
it from the web or or to use an existing
one if it if it's on the on the for
instance on the data set hubs what the
challenges to ex to expect on this
project and what's the desired outcome
and um the idea is that this project
should um uh go find Runing a model and
then create a live demo of it on a
hugging face space so let me show you qu
very quickly what hugging face space
is um it's kind of a demo uh of one
model so one that is very popular right
now is Animan V2 you may have seen some
of it on social media I won't be able to
show you live what it does because I'm
pretty sure there is a
que uh but um it
loads it doesn't even maybe my my
internet yeah another nice demo would
be
um what was the one let me go through
the organization of
the is it for the course that you put it
you
put the Amazon reviews demo space so the
goal of the event from Wednesday to
Friday is to use a free compute from
Amazon sagemaker to build uh to find you
a model on some data set and then finish
with some kind of project like this so
uh this is a model that's going to
predict review score from a customer
review so if I T something like this is
a great
um what kind of product should be
rating
uh wrong button sorry
we should get a nice score of so does it
go from zero to one Aris yeah so so
these ones are actually this is based on
Amazon reviews so it goes from one to
five stars and then I mapped them to
emojis so there's five different
em so why is it so it should so this is
pretty low so the model does not
predict the right label on this one or
how do how do you interpret your
label um yeah so so this is actually one
thing that's like challenging when you
have a festar system that um the the
models like may get some ambiguity um
between you know on the extremes they
will do well but on the the ones that
are sort of in between it um oh no it's
it's a called a Lael so it was
predicting a label that was nice it's
just that you you chose Emoji okay it's
not outputting Z to five okay yeah yeah
sure it's like it's like the probability
right in some sense M so it predicted
like with 89% confident that this was a
bad review which is correct and for the
previous one it was correct as well so
the goal for the the rest of the
community event would be to to build
such a space uh so fing your model liary
in caras or py torch which is so the
general token caras ver the general
token py torch next and uh build a space
like this so once you've found a project
that you like uh you should go should
reply to the topic of the project you
like to say oh I want to help you build
this thing and then uh you can go um on
our Discord
server uh so I think the address is
h. Discord uh because you'll be able to
chat with your team live there which is
going to be super helpful uh on the
forums you can
see the first topic that you will send
this category since I have already read
it it's not apping first for me anymore
but when you arrive on the form it's
going to be first for you uh this topic
is going to explain all the rules
uh of the community event uh so you
should gather to into teams of up to
four people um to create a findun model
and then create a space this is all
that's required to get the certificate
of completion uh you have to create your
space before November 24 and the space
needs to be working one member of thef
team will double check that that space
outputs uh proper results and then every
member of the team will get the
certificate
so we have created a few ideas of
project in this category and you can
create a new project by creating a new
topic yourself just like the person I
was person project I wasting before uh
and then we recommend using fine tuning
uh for your projects like not trying to
pretrain a full model on a new language
because that would require lots of data
and lots of computes uh which you may
not have time to finish that before the
deadline of next week uh fine-tuning an
existing PR model is is going to be way
faster for that and so Amazon is
sponsoring the event by offering free
compute from Wednesday to Friday using
AWS s maker uh you will learn how to use
that tomorrow with the ug face talks but
I'm more focused on how to do things
when more General talks are on the
longcape of
Transformers um so you will have to file
a form you just have to click here for
the link to get access to a special WBS
account don't worry if you don't hear
from AWS before Wednesday because the
account only be um created on that day
and uh you should join uh the Amazon
sagemaker community
organization where because all the mod
that are pre-rain pre compute need to be
SE there once they trained so if you
didn't register for the Advent in
advance you should see a button I'm a
member but you should see a button
somewhere here to ask to join that
organization and you just have to to
click on it and will accept
you uh of course you're welcome to use
your own GP us and cl provider as a
preference but that's not going to be
free and then you can as I said you can
live chat with your team members on our
Discord um
so and then if you have any question
that's more General and of to which
answer could benefit the whole Community
you should ask it on the main forums uh
so that it's more visible but uh that's
pretty much it for the rest of the event
yeah and I think just one thing to
mention if you have an idea for a
project yourself you're more than
welcome to propose it so we have a list
of kind of ideas but if you have
something that you think is cool just
Chuck it in the Forum and go ahead
yeah oh and uh I see that Mark just
joined us so I'm going to remove my
screen and so um
so our next speaker is uh Mark
sarim let me add you to the stream
Mark hi Mark hi everyone how's it going
we going well thank you H so Mark is a
partner engineer at by torch working on
open source software production tools
including torch serve and by torch
Enterprise and Mark was also an applied
scientist and product manager at
graphcore before you read thei Microsoft
and as a CPL his primary patient is to
make program more fun and today M Mark
will explain to us how to train a model
with
py if you want to share your screen Matt
I can then post it and then like before
you should go on our forums to ask your
questions and I've also put the link to
the colab notebook for mat talk
there all right thank you so much folks
for having me uh so yeah so so I guess
like today's talk is really going to be
about like how to train a model with PCH
the hard way so I suspect a lot of you
may have seen uh some of this content
but you know rest assured if you've
never trained a model with pytorch
you'll be able to understand this talk
and if you have trained a model with
pytorch the the the talk is going to be
different enough and the take is going
to be more focused on the internals of
pytorch uh so you'll still get to enjoy
it so please stick around and if you
have any questions uh feel free to let
me know we'll be just monitoring CH so
basically this talk is is going to be
about um how to train a like a model
with py the hard way um and just like a
bit about myself like the the projects
like I work on at pytorch are like model
inferencing tools like torch serve model
uh like mlops tools like torchx and then
like long-term support branches of P
torch with P torch Enterprise uh and if
if you if you do enjoy the stock and
enjoyed uh you know like stuff I'm
saying you know go go ahead follow me on
Twitter uh and just feel free to DM me
with any questions you have so what
we're going to be doing right now is
just like really from scrats we're going
to just start off with importing p and
before we get to training a model I just
thought like uh it'd be helpful to just
discuss very briefly like what is really
machine learning and really the the
framework we're going to be focusing on
today is called supervised learning
where you can imagine that you have like
some data and then you want a model to
make a prediction on that on that data
so if your model is a function f and
then your data is is some some variable
X then the prediction is y and the way
all of this stuff is really going to
work is that you're going to make a
prediction but then you want to compare
to like some labeled data so like the
truth let's call that y Prime and based
on the difference between Y and Y Prime
which is people often call the loss
function we're going to update
F and that's pretty much it so as long
as you can fit problems in this
framework you can train a model uh and
and solve problems with it and in
general as a helpful tic when you're
trying to think about like well what is
really solvable by Machine learning uh
you can think of X as just something
being that something that can fit the
Excel spreadsheet like you know whether
or or or or like whether it's like an
like or also like an image or text but
really fundamentally it's like some data
and then there's going to be some column
that you're trying to predict and that's
your label so really though the the the
topic of of of this uh of this tutorial
is going to be how to train a model
right but before you train a model
there's lots of stuff that that you can
do so this is sort of training a model
is really sort of the last step that you
should explore so one is you could use
something like an open data set so
instead of you labeling data you can go
use something like IMDb imet or use
something from like the hugging face
data sets uh instead of training
creating and training your own model you
could use an open model what I mean by
that is you could copy a model
architecture from some famous paper on
GitHub well you know not only can you
just like pick an architecture you could
also take in the model weights so as in
you don't even train a model there'll be
like a pre-trained model uh with things
like the hugging face Hub or the or
torch Hub where you'll basically just
have a function itself and then make an
INF directly on it without needing to
worry about like updating it U also like
lots of like tools that make models like
libraries like py easier to use like the
hugging face training Loop like the py
lightning Loop like the ignite Loop like
the fast AI Loop or even caros if you're
in the tensorflow ecosystem all make it
easier to do this and so what we're
going to do is really going to unpack
like how these trainers like actually
work and how you implement them and draw
a pytorch so like I said this is going
to be like the focus of the stock will
be how to train train a
model so fundamentally though like
before we start talking about like how
to do machine learning um a lot of these
training Loops look something like this
you'll have some sort of model and then
you want to fit it on some data right
then you have like tools like autom ml
where you don't even worry about the
model you just like fit some data and
then like pre-trained models where you
know like you just essentially fine tune
on some data so that would effectively
be like calling something like this uh
so this Loop uh so let just show you an
example of what this looks like so for
example I'm just going to I'm going to
install Transformers here very
quickly um and what we're essentially
going to do here is we're going to use a
model by the by the Deep set AI team so
they have a mini LM model so this is a
small language model on a question
answering data set
so as you can see I'm literally just
downloading the the model here but also
downloading like a tokenizer for it so a
tokenizer is
because um essentially neural networks
like don't really understand like what a
sentence is right so for example let's
say we want to ask the network a
question like ismar clear so we want to
tokenize a sentence into a bunch of
numbers that in your own network can
understand um essentially the way this
tokenization will work this is like a
word piece token izer so it'll take
basically every word uh in that sentence
but then what we really want to do is
return tensors from it so we we want to
make sure that these are all basically
data that the neural network can
understand and so this is exactly what
this looks like we're going to tokenize
this sentence and we're going to
basically and then we're going to make
the model do an inference on that
sentence and essentially here the model
is going to make a whole bunch of
predictions and we want to find the most
likely prediction so we're going to do
an argmax over all the predictions and
it's going to say no Mark is not clear
actually what is my name though because
this is a yes or no model but uh so so
so this is sort of like the general idea
behind this is that you can think of
your model as just a function on top of
some data so that takes in some data and
gives you a prediction out and so the
question will be how does how do you
train a model like this so before we
start talking about pytorch it's helpful
to just go like you know back to like
the typ on which everyone everyone
stands on and that's something like
psych learn so imagine that you have
some data and then this data is
literally just like uh on on a 2d plane
so you have the 0.11 0.12 2223 and you
want to basically fit a line on this
data right um so the so so so so the
question is here well you know how do
you do this and so if you want to fit a
line like one way to do this within a
psychic learnist to just called like
linear regression and remember fit and
then so X is the is the training data
and then Y is the are the
labels so we we train a model and then
let's say you want to get the accuracy
for it you know turns out it's a perfect
model but then if you actually want to
figure out like the so the quote unquote
like model weights uh this would be like
one two and then The Intercept so this
is basically the the A's right so if you
have like Y is equal to ax plus b so a
is a is a is a vector in this case and
then B is a value
and so this gives you like the function
but trying to get back to this analogy
when we're talking about like a model is
just a function in this case if you said
like for example I want to predict the
value of the 35 then this would just
here in this case like return a single
uh single value of 16 so again so in
this case like we have a function that
takes in like two-dimensional data and
returns a onedimensional output label
um it has weights as in you know this is
one two so let's say imagine you save
those to disk and then you can load them
again and now someone else can use this
model you can deploy this model so very
much so like you'll notice like in in
Frameworks like
pytorch uh that you can think of like
your model as being like a data and a
function as in it's it's data when you
save it but it's a function when you're
doing an inference on top of it and this
is a very powerful framework to think
about so back to talking about like
training Loops you know so here for
example in this case with pych learn we
had this like linear regression. F XY uh
but let's say we use a framework like
fch lightning but again like the same
idea applies to whether using hugging
face triner ignite gas fast a like the
same idea all applies um so we're gonna
have like some model and we'll describe
what that is in a second and then what
you're basically going to do is call a
trainer fit on a model and some training
data loaders so this is some data and
then validation data loader just to make
sure that you're not overfitting on your
input
data um but it's sort of the same idea
right like model data so what's a
training data loader well a training
data loader could be something like it
so in this case it's a function that is
that applies to some data so for example
it could be like like here so let's just
go over this example so let's say you
have a CSV data set or an S3 bucket or
some binary data or links to images or
whatever ever um you want to be able to
for example like load in a few examples
at a time so instead of just like making
the trainer take in some raw data you
make it take a function that has access
to that data but can put it in a nice
enough format for your trainer to work
with uh so there's a whole bunch of data
loaders that exist uh like a recent one
is like P also has a data loader that
works well with like sort of internet
data as in ns3 bucket where you're
streaming stuff uh so definitely a super
convenient uh way way of working with
data
so this is how the like so here remember
so there's like to really understand
this function this trainer. fit function
there's two parts there's the model part
and then there's a training data loader
part and then there's the fifth part
right so so actually sorry three parts
so we covered training data loader next
let's cover the
sorry yeah next let's cover the fifth
part right so the way this works is so
remember like when we described like
machine learning in plain English like
you make a prediction with some model uh
and then after making that prediction
you want to see how different that
prediction is from the actual example uh
and then once you know what that
difference is you update your model so
this is this is going to be exactly that
so you have this first like training
step here so first off like we're going
to be iterating over like examples uh
from the data set um you're basically
going to figure out like well uh how
different was my prediction
from uh from from the prediction of the
model so this is what the straining step
is then you're going to basic so sorry
so so so this is just figuring out uh
figuring out an inference in the
backwards step here what you're going to
do is figure out like uh how by how much
should you change your model weights and
then apply this change in model weights
here so if this seems complicated this
will make a lot more sense when we're
we're discussing like real code like
this is just pseudo code at this point
but again in in plain English the way to
think about this is you want to figure
out how wrong your model is that's your
loss you want to figure out by how wrong
each layer is uh so that's loss
backwards and then you want to update
your model so that's optimize your do
step so understandably so you may think
at this point like well you know like
why should I bother to really train
models from scratch like I can just use
a pre-trained model and not have to
worry about any of this and you're right
like I mean for 90 99% of use cases like
that's definitely true but there's a few
reasons why you may want to train a
model from scratch so the first one
could be well you know if you're using a
pre-trained models and and you know
there have a whole bunch of failures
with it it'll help you debug things more
easily also you can do ml research more
easily sometimes you don't have a choice
like let's say let's say the kinds of
model or the kinds of data sets that
you're working with just don't exist and
need to do everything from scratch and
fourth it's fun you know it's it's it's
a fun skill set that can help you get
paid a lot so it's definitely like worth
worth investing more in
it um but really you should think of
this as being like over time you want to
make sure that like machine learning
model training doesn't become sort of
like this Arcane like knowledge that
like only a couple of Mages like know
about like it's actually not that tricky
like once you get to the basics of it
and hopefully the stock like makes a lot
of those points a lot more obvious to
you so great so we we've done all of
this you know this last 15 minutes was
all introduction and really you know now
we can finally start talking about
pytorch so what what is pytorch like
what's the difference between pytorch
and psychic learn so the main difference
is that like pytorch is What's called
the tensor computation
Library uh with strong GPU acceleration
so tensor computation means it's not
that different from numpy but with
strong GPU acceleration is like one
distinguishing feature and the other
main feature is it has it adds
facilities for tape based autograd
systems and again this is just like a
very fancy way of saying that you can
take a model and then figure out like
how wrong each of its layers are without
necessarily having having to do like a
whole bunch of calculus yourself so so
this is sort of like the main the
primary benefit of P so let's go over an
example right so let's say uh so all P
torch models need to inherit from a
class called like torch andn module and
then they'll have an init function and a
forward function uh so in the forward
function here you're going to see that
well we have this linear layer and then
what we're calling a nonlinear layer and
then a linear layer and a nonlinear
layer reason we do this is because if
you have two linear layers one have to
the others they'll just collapse into a
single linear layer and if it was just a
single linear layer you know great you
end up back with linear regression so
the example we just covered so the the
main difference with deep learning is
one it adds nonlinearities and we'll
this we I'll show you in a sec like how
those work um and it so adds linearities
and then it also adds like
nonlinearities and then it has like
facilities for you to do autograd on top
of it so so you don't need to think
about how to update the weights of this
model U and if you're just wondering
like okay well what do you mean by these
linear layers and activation layers uh
the linear layers here are for example
torch and then linear uh and this is
literally just linear aggression example
and then torch Ru and softmax and I'll
show you in a second like what those are
in a second but you know we have this
model great compiles and then great so
so now so now we have this model here so
let me just show you what this looks
like so if you print
it great you like so so this is like
what the model looks like stere so this
is sort of the data structure that
you're working with um so you may have
heard of other kinds of layers so you
may have heard of things like attention
RNN you know recurrent neural networks
convolution neural networks but you know
they're all sort of almost like special
cases of linear layers so that's why I'm
really covering it here so attention for
example and uh so attention is really
just like includes like three linear
layers and there's like other other
better talks that that that go over like
how this works exactly recurrent NE
networks also include linear layers and
bu networks aren't really upfront they
they don't really look like linear
layers but you can Implement them as
such and the reason why we have
different layers just to be clear is
because they have what's called like
different inductive biases as in that's
just a fancy term for they're good at
certain kinds of tasks like CNN's are
good at finding like information in a
space rnn's are good at remembering
stuff attention is like I guess I just
want to get at everything um but
foundationally in terms of Ops like like
thinking terms of linear layers will
help simplify this like quite a bit
quite a bit for you when you're think
about these for the first time okay so
yeah so so here we we printed a model
and you know just the thing to keep in
mind here is that really what we're
doing is very much like uh matrix
multiplication so let's just like look
at this thing like very quickly so we
have this linear layer and it's saying
okay it's taking in in features 100 so
that means it's taking in a vector uh
like it's it's taking in a vector of
size 100 but then it has like these
hidden features like the out features
and the in features that are higher
dimensional that way you can sort of
represent your problem using more than
just like 100 weights at the very end
you'll notice that it has like this out
features 10 as in let's say we're
predicting like one of 10 classes like
one of 10 possible animals so this is
where this shows up and the soft Max is
really just a way to pick the best like
like basically the best feature among
those 10 uh right so let's say each of
those 10 will be a number and I'll show
it to you a sec and then you like want
to pick it so again the the way to think
about this is what is your input size
output size like basically the hidden
feature size is basically how much
compute can you afford how much storage
can you afford odd features is
determined by your problem like how many
classes you have and then you're going
to have some nonlinear function like the
softmax that helps you pick the most
likely plus uh so just to show you like
there's no black magic here like let's
say we had the REO for example and like
what does this do right so let's say we
we generated this like random tensor
this is a very useful function in like
Rand n so it generates a random tensor
of size four and what the ru is going to
do is it's basically going to zero out
all the negative values so here you see
these two are negative so these are just
going to be zero uh and then this one
remains unchanged right so this is just
like a way of getting rid of negative
values and make sure that like your
models are more likely to converge as
far as softmax
goes uh notice here so so I said this
thing earlier that like softmac helps
you pick the most likely value and again
that sounds like it's a bit like black
magic but the way to think about it is
that let's say you have four possible
values for like a prediction and what
you want to do is you want them to
become a probability distribution so you
want the sum of all of those values to
be equals to one and then you would
basically generate a random number
between zero and one and pick one of
these uh and then you'll see here that
if you sum them up it's actually pretty
darn close to one it's just it's
floating point numberers it's going to
be hard for them to sum up to one but
this is how you would get the max right
so then let's say you want to figure out
like which of those values do your model
predict you would do something like np.
argmax on top of the sum and it would
tell you for example that the most
likely prediction is this guy right here
so like label zero right and then label
zero could mean cat dog or whatever else
so there's another key value story that
you would have to keep track of these
things so let's just quickly go over
like what what how these look like
exactly um so so there's this nice
Library I I like called like Rich that
shows you like how to inspect modules
and like torch functions are the same so
for example reu here you want to apply a
rectified linear unit function element
wise so this means element wise means
you're applying a function on each
element of a
tensor uh and there's like vectorized
versions there's like broadcasting
there's all this cool stuff that we can
talk about but in general like looking
at the docs is honestly really helps
sometimes to figure out how something
works whereas like with dor linear the
way that works is it's saying you apply
a linear transformation to the incoming
data so you have y is equal to x a
transpose plus b so a is a is a vector
it'll say how many inputs you have and
then B is some sort of bias uh just so
that you can like move around the
function and then y would be your label
in this case so again looking at softmax
here so it's saying it applies a softmax
function to an n-dimensional input
tensor rescaling it so that all the
elements are in the range 0 to one and
some to one
right again remember now like back to
this model right
here this tiny model that's a toy model
that we have um remember when I said
that like like models are functions but
they're also data so let's say I want to
print the values that are like in these
weights like you can just literally
print them like this you can just say
four param and model. parameters and you
can like print them and like visually
inspect them to see what's going on like
maybe you have lots of n which is like
that right but you could also like look
at for example the parameters for a
single weight so for for a single layer
so let's say I want to say for model.
forams oh sorry yeah getting lost here
yeah so for model. linear one so linear
one was the first in your layer you can
also look at the prams but any layer is
sort of the same and so again that means
you can save it you can load it and so
this is how like a lot of ml Ops gets
done and that way you you train a model
you save it you load it again you can
use it for something else
uh and the reason is because like again
it's it's both data and the
function so you know like I said there's
all sorts of other architectures like
Transformers CNN recurrent neur networks
and you could sort of dream your own and
a great way to actually learn pytorch is
to go ahead and try to implement one of
those things like from scratch right
like and just see what would it be like
let's say so right now we're with our
hugging face like hugging face course so
what would it look like to implement
like a really simple example of like of
an attention layer and you could sort of
do this like building it up with with uh
with linear layers but obviously like
production ready version would include
like so much more stuff but at least it
gets you like more familiar with the
basics so other things like so let's say
we we we talked about softmax which is a
way to to to make sure that the values
in a vector all sum up to one but
there's a whole bunch of other ones for
example there's one called like a g and
then just like looking at this thing you
know you could be like okay you could
sort of try to understand like how some
of these swings work there's like the
swish function uh also the same and and
the nice thing about experimenting with
ML so back to the tiny model
here uh machine learning is inherently
like a very empirical science right so
instead of like let's say trying out a
softmax here maybe you could try out a g
maybe you could try out a swish you
could come to the linear layer make it
bigger you could change it to a CNN you
could change it to RNN so it's like very
Lego based so try things out and then
you can just see what works best on your
problem domain and if you don't have
time or don't have the expertise to do
so that's where using pre-trained models
like really really
helps all
right excellent so now like I guess it's
time to talk it like you know in a bit
more detail as to like what is a tensor
exactly so let's say we have this thing
called a so it's going to be a torch
Rand n so that means it's going to have
five elements and then they're going to
be alloc on the CPU so let's inspect
this for a second and see what's
interesting here so one you'll notice
like data has a d type here so dtype
means it's a floating Point 32 this
could have been a floating Point 664 as
well uh the reason I want to talk about
like d types here is because um let's
say for if you're using like Nvidia gpus
then you may use something like floating
16 if you're using like an Intel CPU you
may want to use something like int 8 so
turns out like precision as in like very
precise numbers doesn't really matter
drastically for machine learning so you
can use lower Precision bits and take
advantage of Hardware acceleration so if
you want to take advantage of that kind
of Hardware acceleration like all you
would really need to do is come in here
for example and let's say add in like d
type uh equals like torch.
in8 uh and that's pretty much it you
know like you you like P torch takes
care of of the rest for you but there's
other also interesting stuff for example
like mkl or or so for example is it
using like the Onyx runtime environment
is it using some acceleration by Intel
is it using vulon uh what's the layout
of the tensor like like is there like a
specific way it's laid out into memory
does the tensor have a name what's the
type like maybe it's a complex tensor
because you're using doing something
with music like what's the shape and
what are the actual values inside of it
so this is like the tensor sort of like
the main main data structure that you're
going to be working with and again just
looking at its arguments will give you a
lot of ideas to like what's available
and what you can what you can do with
it so back to how to make an inference
though so let's say you have this
network right and now we want to print
it like again this is what this model
looks like and I want to just generate
so here it says in features is equals to
100 so I'm literally just going to
Generate random data so input to random
and then I'm going to make an inference
here and I'm going to print the output
right so this is sort of like when
you're thinking about like what is like
inference or you know just like you have
some data and you want to pass to a
model like this is the way it works and
this is how you would know like what the
input dimensions are um so that's pretty
much it right but let's say now like
well we're doing classification right so
we want to figure out like what is the
maximum value between these things so we
know okay that the largest value is this
like
1325 here and that this corresponds to
index one because it's zero index uh and
this is how you can end up making a
classification from just like the super
simple prediction on Raw data uh but in
of itself you know this is not super
interesting because we just have random
model weights on some random data and we
got like you know so we have garbage
into garbage garbage out so like none of
this none of this is interesting so far
but it shows you again like how how the
framework works and now we'll do
something a bit more useful
here so remember when I talked about
that when you're trying to train a model
with pytor it's like the things that you
need to think about are your model so
your model includes like some layers so
these could be linear layers they could
be CNN they could be RNN Etc it includes
your activation function so like maybe
it's softmax your goal which really you
can think of as being the loss as in
like I want you to for example make
these accurate like like predictions
well this formula the categorical cross
entropy like really helps but you could
have like all other kinds of formulas uh
that that that you know make more sense
for your use case and then the optimize
your you can think about it as how can
you achieve your goal so the main the
most popular way is what's called like
stochastic gradient descent but there's
other algorithms like atoms and this is
a whole literature on on optimizers and
most of the reasonable ones already
exist like in Port so you can again just
try them out and see what works best for
your use case so in terms of here of an
optimization what we're going to do is
we're saying okay we want to do a
classification so that's why we're using
cross entropy loss and we want to
perform stochastic gradient descent on
this network basically the netop
parameters we want a learning rate which
basically tells the model like how
quickly it should update its weights so
for example if it's converging too
slowly you make this value bigger if
it's converging too quickly or if it's
too chaotic then you you make the
smaller and then a momentum is also like
slightly more advanced concept but that
also helps with like
convergence so remember when we talked
about like pytorch here so we said for
example that if you want to generate a a
random tensor that's like let's say of
size two it's going to generate two
values but let's say we want to generate
a tensor of Dimension
32 so this basically means you're going
to have like uh like three tensors that
are of Dimension two stacked one on top
of each other uh so another way to do
this could be like you know let's say
instead of it being a tensor it could
just literally be like a list right and
you just like append them one on top of
each other but the benefit of doing
things in batched way is that like now
let's say you're make you want to make
three
inferences instead of like let's say
adding like a model do out on a for Loop
you could basically make a batched
example and then you have a single
operation that can run like really fast
like on gpus so this is again where the
numpy with GPU acceleration pitch of PCH
comes
in most numerical linear algebra
libraries also come in with like a bunch
of nice stuff so let's say we have this
batch here and so this is a matrix of
size like 3x two uh and then we're going
to multiply it by a value two right but
two isn't a matrix right so so if you
said 3x two times something of size one
like that operation should fail but py
here understands like what you mean and
it what it does it'll broadcast this two
into a vector for you and then multiply
each of those elements like by two for
you because it sort of like understood
like what you're trying to do
here um right so again like like so so
so so this is like where where using a
linear algebra I really helps like
whereas like you know you could use
something like python lists but you're
probably going to have to reimplement
all this nice stuff from scratch you may
not have like the nice like GPU
acceleration you're not going to have
like all the core Ops like either an A10
or blast or Etc like implemented for you
and you can just like work in python as
opposed to working like with C++
functions or fortrend functions which is
like you know not super fun um so back
to this batch example so let's say let's
say I create an examp example that's
like now 3x one and then I have like
let's say this is my B batch and then my
a batch was 2x3 so because of Dimensions
match and the inner Dimensions match you
can multiply these two things so one of
the reasons why gpus are so important
for machine learning is that they're
very good at taking in like these large
batches and multiplying them very
quickly with specialized kernels uh so
thankfully I mean like gpus aren't like
that expensive anymore like especially
like with something like Google colab
like all I really have to do here was
come in here click on edit notebook
settings choose a hardware accelerator
as a GPU and we were pretty much good to
go um so let's say okay so so now let's
try to do like a like more of a real use
case where let's say we have a batch
size of five and then we have 10 classes
so now like the random tensor that I'm
generating let's say again I haven't
changed my network like my network still
takes in an input size of 100 I'm GNA
saying okay well you know you also need
to be taking in a bat size of five so
I'm G to have five 100 siiz tensors like
stacked one on top of each other I'm
going to put them on a device called
cuda so that basically pytorch knows
like okay you're going to do something
on a GPU I'm also going to say requires
gradient equals to True here uh and then
I have like my target here which is
basically going to be let's say like the
D type is going to be torch long in this
case and then I'm going to generate like
10 number of classes I'm going generate
like 10 random labels uh and then the
way byor would know have wrong I is
essentially here with this function so
like if you go back to
Criterion right so Criterion here is the
cross entropy loss right so what I'm
going to do is I'm going to take the
cross entropy loss between my what my
inputs suspect and what my targets are
so basically my inputs and how wrong I
was so just a quickly sanity check and
look at some of the shapes here so so
yeah so this this may take a while let's
see here am I on GPU
maybe I should have made this smaller
let's see I ran this before anyway like
so while this runs like let's just give
it a
second okay it ran great so you'll see
here that the targets. shape for example
uh is going to be here so let's see
number of classes see targets.
shape so here the targets of shape is
going to be there's going to be bat size
many examples so basically we're going
to have like five examples that we're
predicting from right the input shape is
going to be the batch size times the
input size so five is 100 and in this
case you'll see the loss is actually a
number so this is a number that tells us
like how wrong we are and you may have
guessed that you ideally we want this
number to be as close to zero as
possible and this is exactly where you
know now like the the training part
really
starts so let's do this so we have this
tiny model and I'm going to send this
model to the GPU then I'm going to have
a Criterion so I'm going to say okay I'm
going to do a classification I'm going
to have my Optimizer I'm GNA do 5,000
Epoch so basically I'm going to go over
my data 5,000
times and pretty much all pyro training
Loops look some something like this
first make sure that you basically zero
out like how wrong you were from earlier
that's what the optimiz Z gr is you
calculate your current output so this is
what's called the forward propagation
you figure out how wrong you
were you get the gradients on how wrong
you
were and then you basically apply and
update to your weights and then you
start printing the loss and ideally you
want the Sloss to go down so let's see
if this actually
works okay for example here we're
starting with a loss of
232 229 where we go yeah so we started
with 232 229 223 23 1.97 so you'll
notice by the way that even though we
did 5,000 epochs and all we're doing is
training on a single example like
basically a single like fake example and
we have a model that has like like an
hidden layer of a thousand even then we
weren't able to get the training loss to
zero so generally doing random data
helps because it helps you debug errors
before you start working with real data
and it just goes to show you how hard
some of the stuff is and like why you
should maybe sometimes consider using a
pre-trained model or using like an open
model with like no with things that are
known to work so how would you make this
L zero well you know maybe just try more
Epoch maybe try making the hidden layer
larger maybe try adding more hidden
layers maybe try adding like different
kinds of layers and just try stuff try
stuff try stuff and eventually you're
going to find something that works and
maybe you'll learn something new out of
it so again because ml is a very
empirical uh
discipline the faster you you get that
experimenting quickly the better you're
going to get that learning what actually
works so this was as far as the training
Loop goes so I'm I'm going to I'm going
to repeat it like you zero out how wrong
you were from earlier you figure out
like what is your current prediction how
wrong that current prediction is you
apply how wrong you were on dead and
layer on on each of the layers you
optimize your model and then you
basically print out the loss and then I
you want that to go down so this is this
Loop will serve you well and everything
will sort of like look like this unless
you're using a training Loop then you
know you obviously won't see this
because because someone else would have
done this for
you um so let's talk a bit about
gradients right like how does this magic
work right so we had this thing on
optimize zero grad we had loss backwards
and we had optimiz your that step and
really the point I want to make to you
is that there's no real magic to this
like as in uh instead of using the
categorical cross entropy loss like I
could have just computed this loss
myself and then I could have seen okay
well I have this loss it's on this
device Etc um and and even then like
when you're thinking about this
Optimizer doep function uh really the
way it looks is like this like you
basically uh you go over a bunch of
epochs you figure out like basically
some gradient on a vector you apply a
learning rate and then you sort of
successively apply it over and over
again so it's an itative algorithm um so
again like none of this is any black
magic but let's talk about a bit about
the black magic which is really
automatic differentiation so let's
assume let's forget like the tiny model
for a second uh and let's assume that
you have something that looks like this
which is uh you have you have a let's
say here so let's say we have a tensor
that's just like 2 three and then this
tensor that has like the values like 5
six and then we're going to create a
tensor so that's basically 3 a plus b *
a right so this is a function so before
we this let's look at a bit about like
the calculus for it so let's say we have
3 a plus b let's say you want to derive
this with respect to a then you know you
have 3 a then that's three and then ba a
so then the derivative is B so then you
know that the end value is going to be
89 but now let's say you're deriving
this with respect to B well this is 3A
is not a function of B so it's going to
be zero and then here it would just be a
so then you know great then the value is
just 23 so let's just confirm that
that's actually what P chch does
so here we're going to basically have a
place to deposit the tensor so think of
it just like as a data structure where
you're putting the gradient updates and
then we're just going to say c.
backwards and when you do that you're
going to notice okay so it's 8
923 and that matches exactly like 8 923
here okay so this is just it's
complaining because I'm doing it I'm
doing this twice but this is fine uh so
again like when when you're thinking
about how to use Pyers for automatic
differentiation part of it is that like
if you have a large complicated model
Computing all the derivatives for it is
really annoying to do it by hand so you
can sort of convince yourself that it
works on some simple functions but if
you have like much more complicated
what's called like computational graphs
like neural networks this is where using
an autoi engine will really really be
helpful but you can always stand at each
check like what are what are the deres
exactly if you look at a do grad and
just s check them by hand to make sure
that everything works
great so once you've done this like you
you finally trained a model like success
what you can do is you can just save it
with like you can just say like net.
State dick and save it to some path uh
and then at that point it'll just be
like data that's stored on disk so you
can upload it to an S3 bucket you can
put it in an Azure blob you can send it
to your friend via email on Dropbox or
whatever and then what they could do is
they could call torch. load and load it
and then if they were to print it yeah
great like they just saw like the exact
same model that you got and then they
could again do for you know Lo for Pam
and loaded model. prams uh and get
exactly what the parameters are and then
call loaded model on an input and do it
for whatever they could call it and then
fine-tune it so again this this data as
a function Paradigm is really the one
thing you could take away from the stock
is that uh will help you really really
like grock how this stuff
works so that's really the the main
thing I want to cover today and I really
want to end like with a slightly more
advanced feature uh it's been very
exciting which is torch effects so
pytorch even though like as a user
you're mostly using it you're mostly
using python but python is actually
mostly written in C++ because it it it
helps us like memory manag and write
things in a more efficient way in
general one exception to this is FX so
what FX is is essentially a way to write
compiler passes over pytorch modules in
Python and the reason this is very
powerful so let's let's go over an
example here so we're going to basically
load in a model and what we're going to
do is we can you'll see we literally
have a for it's like oh for node and
graph. nodes and what we're going to do
is we're going to say if node. Target is
equal to linear. one linear one make it
linear two right so what is this
effectively let's look at the model
again so we have linear one we have
activation then we have linear two and
then we have soft Max so what we
literally did here is we skipped those
first two layers and created a new model
and now this new model we can save it we
can fine-tune it right so again this is
sort of just the tip of the iceberg but
I really wanted to show you this example
because you could do all sorts of stuff
like maybe you want to export your model
to some runtime like Tor r or RT maybe
you want to quantize your models as then
change the D type to an N8 or float 16
maybe you want to do something like
extract intermediate representations of
a model maybe you want to fuse two
operations so if you have two operations
that show up a lot next to each other
you can create one single operation
that's much faster and there's a whole
bunch of stuff you can dream up so this
is probably a very very fruitful area
where you can think about how to
innovate within the Pyro
ecosystem so like really in conclusion
um you know maybe in the Noto distant
future like model.fit or just like
model. finetune will be all that you
need uh and at that point like knowledge
of how to train networks from scratch
like may be lost uh but you know but
what I'm really hoping is like let's say
you know like the force The Four
Horsemen of the Apocalypse like descend
upon us and that we lose all pre-trained
models and that we lose like all open
data sets is that like the knowledge of
how to train like models from scratch
gets maintained that you'll still use it
that you'll still build Innovative stuff
with it um and at this point you know
you may think great like this was uh
this was maybe a great or maybe not so
great like introductory talk as to how
to use py torch so if you're really
interested in learning more about py
torch what I would highly recommend is
you read the docs the docs are great
probably the some of the best tutorials
and the other step is reading good code
so there's lots of good code around like
whether it's by people like Phil Wang
like Ross Whitman with Matias Fay but
also highly recommend you just check out
like some of the source code for hugging
face or fast AI you'll learn a ton about
how to write like good pie torch and and
even then if you're thinking great like
I'm I want to do some more advanced
stuff or maybe I even want to work at
pytorch UH probably the best way to
contribute to pytorch is to solve issues
on GitHub so we have lots of issues
tagged with beginner or boot camp tasks
so you can check those out and you can
create like a cool project with this so
try to basically create something cool
to get like the attention of a pie chch
developer but really most importantly is
like follow the gradient of your
interest like as in like this is a big
field there's a lot of interesting stuff
you could do so if you do something
interesting you're much more likely to
do something else that people find
interesting and you know if if if you
have something cool and you want
feedback on it like feel free to DM me
I'm more more than happy to
uh share feedback on it um and yeah and
I guess that was it for me thank you so
much folks if you have any questions I'd
love to take
them awesome thanks a lot Mark that was
a super comprehensive tour through I'd
say almost all of ml um so great in fact
it reminded me a bit of Jeremy Howard's
what is torch NN really he has a super
nice tutorial on the pie torch docs
where he kind of deconstructs all this
stuff from scratch and I I sort of saw
you also channeled Richard findan at the
end um where he had a similar analogy in
physics that you know imagine we lost
all knowledge what would be the one idea
that you would like people to remember
and he was like you know mat is made of
atoms and for you it's like know how to
train neuron networks from scratch so
yeah I definitely I'll say probably
matrix multiplication and autoi I think
those ideas would help us rebuild
significant chunks of civilization but
thank you like I don't think I'm worthy
of those confence but thank you so I
think we got time for like a couple of
question questions um so one thing here
was um you mentioned at some point this
uh sort of kind of technical detail
about I'm not needing High Precision
bits in machine learning or maybe deep
learning particular could you sort of
expand a little bit why is this and you
know why should I
care sure yeah so uh I mean so this is
maybe more historically but if you can
imagine like maybe the first like
scientific use cases of computers
historically uh were things that invol
were very calculus heavy so imagine like
ballistics like Nuclear simulations So
Hard science stuff and typically in hard
Sciences like the the lower Precision
bits really really matter like because
if you're off like by a bit you know
like your rocket like won't land you'll
attack someone you don't want to attack
so it's super important uh in machine
learning like just uh it's it's like I
don't know if I have a good theoretical
argument as to why this is the case like
all I can say is
anecdotally if you do try to deploy some
model or train some model with lower
Precision bits they tend to do uh you
know not that much worse than the higher
Precision ones the benefit of it though
is that you're definitely losing some
performance but you're gaining more in
terms of Hardware acceleration so like
Hardware providers will give you like
better kernels to do things really
really fast so it's definitely a
conscious sacrifice that you're taking
but for a lot of people it's worth it
and it could be worth it for a few
reasons like let's say for inference you
just want your inference to go faster
because you're doing real time so that's
a great reason to use lower Precision
but even for training like let's say
you're debating between okay I want to
train for one week with floating Point
32 or train for you know one week with
floting Point 16 so I'm effectively
doing twice the amount of training you
know that may be worth it because just
like time is limited on this earth right
it's the only rare resource that we have
and so it's just a thing to consider so
the the argument is purely as far as I'm
concerned like anecdotal but I'm sure
some people that are smarter than me
know some theoretical reason why that's
the case awesome thanks so I think we
hit the time um so thanks again once uh
for for for giving this awesome talk and
um thank you for contributing to the to
the co Community
event thank thank you I really
appreciate it thank you for listening
everyone appreciate
it great so I think we have our next
speaker here I'll just
add yob
hey cool so it's a pleasure to um
introduce our last speaker it's
yakob um yakob is the co-founder of
inceptive which designs RNA molecules
for vaccines and Therapeutics using deep
learning to make RNA based medicines
more
accessible um and for those who maybe
don't know um yakob was at Google for
more than a decade and he kind of knows
a little bit about Transformers uh since
he was behind the groundbreaking
attention is all you need paper um and
so we're really excited to have you here
to tell us what is maybe not so great
about Transformers we've heard all these
talks today about how Transformers are
like amazing and you know you should
just go use them and have fun and I I
think here we're going to get some kind
of cold water
maybe on
um so looking forward to it and um if
you want to stream your slides you don't
have slides right you're going to do it
in the background right exactly exactly
awesome so we'll um we'll be here and uh
I'll probably jump in at various points
when there questions please do please do
thank you for for the introductions and
and um uh thanks for having me um I
should also add you know um that that
paper was actually a really really fun
collaboration with a whole bunch of
people each of whom was absolutely
necessary for for this work to come out
this way um I learned a ton on the way
and I'm incredibly grateful to have had
that that opportunity to to work with
that with that bunch of people um but it
is true actually that um yeah I'll try
to find some cold water um I think I may
have found a little
bit um I should add as a caveat that
this is not built like an academic talk
uh so it's a bit different from from
talks that you know I gave at
conferences or so in uh that it
certainly is full of bold claims and has
absolutely guaranteed insufficient
evidence for pretty much all of them I
think um and so you know believe leave
at your own risk uh so so far so good um
you know people often say if it ain't
broke uh don't fix it I think it's
actually an incredibly useful often very
useful engineering um Mantra um but you
know there's a there's I think an
alternative uh sometimes or an
alternative mode which is you know if it
ain't broke well let's try to break it
you know U let's let's see what's wrong
with it can we find kind of from first
principles uh reasons why you know
something that isn't broken actually
doesn't seem to be broken actually is uh
is failing in certain ways um and so
maybe the subtitle to this talk could be
something like three great reasons we
should replace the Transformer um and
you know you would have never guessed
number three um and so since this is
enough uh or these are were enough
attempts uh at making jokes even in the
first three four slides the rest of the
talk can be very serious um but maybe
taking a step back uh you know let's
talk briefly about this kind of whole
premature optimization versus first
principles thinking which I think is
actually uh you know premature
optimization is really what this if it
ain't broke don't fix it tries to
prevent um and I think that sometimes
this idea of first principles thinking
Elon mus talks a lot about it on and off
um you know is potentially kind of a
workaround in a certain sense uh uh
preventing you from um you know doing
optimizations or going into a certain
direction that you that you wouldn't
necessarily have to go um and so as an
early example I actually remember one
one time uh you know as a preschooler um
you know quite a bit of stuff as a
preschooler right from experience a lot
of it some some not from experience
right but you've probably learned that
moving heavy stuff is hard um and you've
also learned that explosions are really
dangerous um and you've learned that
they look like giant Fireballs you know
not all of this is true uh and generally
you know at least I experienced that
remote controlled cars are F fast and
awesome and then the first time I kind
of you know somebody tried to explain to
me how combustion engines work uh there
was this heads scratching moment because
what I heard was well you
know cars are generally kind of built
around this giant hunk of metal um you
know and that giant hunk of metal is
really only there because in the middle
there are like small spaces uh in which
there um explosions uh and they're
containing by this giant hunk of very
very heavy giant hunk of heavy metal and
and that tries to then Channel all the
force of these explosions into moving
you know an even bigger hunk of metal
that is basically uh carrying this giant
hunk of metal this that is the engine um
and at least at the time that seemed
like a a fairly backwards way of of
doing this obviously I had no idea about
kind of you know the the the the energy
in fossil fuels and and all of that
stuff but it did SE on um and I think
it's actually not that different from
the thought process that uh um you know
many many years or decades later uh
actually uh was what started this
Transformer project um because you know
we did start with a few kind of first
principles assumptions um namely maybe
most importantly that it is possible or
likely possible to compute good
representations of some chunks of
sequences and parall
and what inspired us there most I guess
um is really that it seems like you know
those tree structures that linguists
often talk about when they you know try
to formalize kind of the statistical
dependencies between words and text are
actually kind of often actually kind of
largely right um so I personally believe
they're almost never exactly right um
but but often you know largely and what
that directly implies is you know if if
indeed those statistical dependencies um
are often approximately pre-structured
then it should be possible to compute
reasonable representations of you know
chunks of text for sure of sequences of
words in particular um uh in parallel
before you then you know uh uh kind of
join these parallel threads of execution
and then uh aggregate uh you know maybe
new more Global representations from
that and another another assumption we
made is that training speed is actually
really really what is most crucial for
model uh quality at the end of the day
and that the key bottleneck at the time
at least was basically the the longest
length of the longest sequential
computation and so we kind of placed
this bet that paralyzation actually um
of of these of this kind of uh you know
sub representation computation uh might
be able to buy us much more than the
computational cost uh that we had to
expend in order to effectively
marginalize out the dependencies meaning
basically assume all dependencies right
so one interesting thing that this self-
attention mechanism employed in every
layer in the Transformer does is it
basically actually considers all pairs
of stuff um right and so in a certain
sense you could you could say it
considers all dependencies all possible
dependencies marginalizes those out much
more
expensive uh uh then then say
considering only the true one if there
was such a thing and if you knew it um
but seemingly worth it if as a result of
that you can actually massively
paralize um so in my mind at least that
that process was pretty similar uh kind
of to to maybe what made me scratch my
head uh about cars and combustion
engines uh cars with combustion engines
uh many years prior um and and
effectively you know the the the the
latter two-thirds of the stock I'm going
to try to spend on a few maybe general
areas in which I'm scratching my head
about Transformers in that same way can
I buge in please so so this is a super
nice like historical kind of context
about the origin of of the Transformer
architecture and I was wondering if you
could expand a bit on the um the thing
about this first principles approach
because I would say like at least for
myself it was kind of surprising that um
you could kind of remove the sort of
somehow like temporal nature which like
as humans when we read text we tend to
read say left to right or back right to
left depending on where which language
you speak and I was just kind of curious
like was this something that you at
Google kind of figured out from like
linguists or it's like it was really
like taking a different perspective on
the problem okay so so it is a it is a
somewhat different perspective there was
a linguist involved um well actually
there probably were a few linguists
involved for me personally the linguist
that was involved most was uh was my dad
uh computational linguist so I kind of I
had no choice kind of a problem that
runs in the family in a certain sense um
and and so but but it it it wasn't just
you know what I took from him and and
and and others wasn't just the fact that
there's maybe this kind of you know
often largely approximately uh largely
approximately right tree structure
but um that in fact our ways or or this
model of thinking of language as
something that is sequential is overly
indexing on our representations of
language and so you know when we
communicate when we talk we largely say
words in sequence except you know then
we um you know often insert fillers uh
and we also you know often do this and
so forth so we interrupt ourselves the
entire time it's not exactly sequential
right we do jump back yes it's not like
we can just fill in things um but but we
try uh I think and and when when reading
texts in particular we actually can jump
back and I'm not an expert on on this
stuff but as far as I know it is not
true that we always read in order so
probably I mean necessarily you can show
we read more in order than not because
we eventually end up at the end of
something right uh some long string of
of characters or words um but as far as
I know humans tend to actually back and
forth a lot um now that
said yes we could go by the limitations
of our own cognitive
apparatus but if we have a different one
and this makes a big difference um and
probably I should have mentioned this uh
that actually you know what we were
optimizing for wasn't software that runs
on some accelerators or or some some
some some silicon of our choosing it had
to run on the accelerat we had and they
looked you know largely like the ones
that we still have to today um except
the ones today are are even faster
because they're evolving so fast um but
they are just insanely good at doing
these independent simpler uh
computations and Par and so if you
really wanted to to kind of improve
utilization uh and what you could
squeeze out from from these
accelerators it just seemed to be the
right direction to improve paraly
ability so that might be different from
how you want to or from how humans have
evolved the process language um probably
is quite different uh right um but yeah
it's just different uh Hardware or
wetware that has different
strengths awesome thanks for the the
context and please carry on to tell us
all the problems okay okay um so right
before I get to all the problems um I I
did want to SP like spend like maybe two
more minutes or three um on on you know
why overall I personally would think it
actually helped um and and really I
think the key thing is this
computational efficiency specifically on
today's typical accelerators um but
there's another reason and that has
nothing really to do with you know the
specifics of of the model or the
architecture but really that once in a
while when something different comes
along um it helps with momentum right it
helps with competition you know friendly
and all but but still it really just
helps to to get people to contribute new
fresh ideas or or or you know yeah get
people excited about stuff even if
initially that excitement sometimes is
disbelief or so right or U you know like
I said friendly competition so it was
like that the other way around um you
know the we had um Facebook's
convolutional seekto seek paper that uh
uh you know that we in kind of in our
last what we thought last stretches of
readying attention is all you need uh
that that we first read that came out
then and that had pretty good results
and so certainly you know that that
nobody knew at the time but that
certainly uh pushed us and motivated us
and so forth and and you know I think
similar things happened afterwards um
and in terms of momentum and and and
fresh ideas and contributions that got
people excited you know Sasha rash this
has this amazing the annotated
Transformer po which when I saw it was
just amazing and just really really fun
to see people um do that kind of stuff
um and and I think that that actually is
often underestimated as contribution uh
to to you know kind of the community's
reaction has underestimated as a
contribution to um what kind of you know
makes this work or made this work in a
certain sense um okay so nothing else
well there's this one other thing and
that's actually that decoupling the
architecture and the parameters of the
model from what you could call maybe the
registration of the
elements um uh of of the structures that
the Transformer ingests or or or
produces actually allowed uh
very easy adaptation to uh to new and
different input and output modalities
different kinds of structures right and
and effectively what it kind of did was
it generalized this this absolutely
revolutionarily powerful seekto seek
approach to something that's maybe a bit
more like stuff to stuff right so
technically you could say oh it's it's
set to something and then or set to set
maybe even if you drop a bunch of if if
you drop position representations and so
forth But ultimately if you can um en
code the topology if you wish or the
registration of these elements right
these these individual positions or as
as they were called initially uh that
the Transformer works on then you can
you can you know apply it fairly
generically not always efficiently but
fairly generically and that in turn you
know made this guy show up um and and
with him you know like this this this
horde of other Muppets and and you know
uh friends and family of of B initially
I guess focused on language really but
then uh also spilling into into vision
and into this really really impressive
flurry of activity around
self-supervised uh pre-training so there
was generative pre-training which you
know also uh uses the Transformer um in
a maybe in quotes more traditional way
um uh but exploiting this kind of um
flexibility uh you know around the input
and output structures that need not
necessarily just be sequences in that uh
sense anymore but anything you could
could encode in a set somehow um you
know just broadened uh the the or
increased the efficiency of of some of
the self-supervised approaches
specifically uh masked uh or mask style
you know de noising Auto encoders and
and and related approaches um and you
know this this has touched a whole range
of different applications uh including
now uh biochemistry so there is now a
prop bird and DNA bird and amino bird
that you know play a role in all sorts
of different applications around or
proteomics and also provideed great
sideway for me to kind of put in a
Shameless plug uh maybe speaking a
little bit to to what to some of the
things that we do at inceptive at least
which is you know as as as you heard in
the introduction earlier um we're
designing uh RNA and specifically
initially at least mRNA molecules uh
that we try to design such that they
have improved uh uh
properties in in desired ways such as
improved ability that you might not have
to Deep Freeze um mRNA based
Therapeutics or vaccines uh anymore or
that they might last much longer if you
do um higher protein expression which
could render it more effective and maybe
applicable to a broader range of of
diseases and it's just a fascinating
huge search uh uh problem with with just
ginormous spaces you know even for the
covid MRNA vaccines for instance you
have 10 to the 630 synonyms uh mRNA
sequences that would actually produce
the same protein uh in our cells uh that
we want to produce for these vaccines um
but it's also an incredible predictive
modeling challenge um right we we don't
understand most of these properties
meaning we don't understand how they
work uh and we don't even understand we
don't even know if we know all the
mechanisms in some Cas we know we don't
know all the mechanisms uh that that
kind of give rise to these properties or
that impact them and so we have very
little hope of generating enough data in
our lab for each of these uh properties
of interest and so really you know
self-supervised pre-training uh is is at
least one of the um one of the pieces of
the puzzle uh that we hope to be
applying uh in order to U you know
tackle tackle problems around RNA
Therapeutics and vaccines can I budge in
again sorry um so this question you
mention about like uh we don't really
understand um the properties of these
molecules I mean wasn't chemistry sort
of solved by you know Chemists in some
sense like 100 years ago um could you
maybe sort of expand a bit on on what
the challenges are yeah sure so um uh
I'll try to be quick uh because there
there's still a lot goinging but
basically
um kind of true yes uh chemistry kind of
solved but
um I would say there are aspects to some
of these macro molecules some of say the
biopolymers RNA DNA um that are not
exactly kind of small molecule chemistry
in the sense that you know just like
proteins uh their their structure their
3D structure their arrangement in space
really makes a difference and it can
affect uh chemical properties right so
depending on the arrangement on you know
the geometry effectively of of of the
bonds relative to each other uh you seem
to get different uh Behavior Uh when it
comes to things like hydrolysis that
eventually
degrades these micr molecules for
example um but that's just the chemical
side and and we're definitely interested
in that but there's also the the
biological side and there it gets even
crazier because you know in in the
environment of cells for example right
there there is a is an unknown set of
players some of which we know many of
which we don't of of enzymes of of
proteins around that effectively have
highly non-trivial effects on um uh on
the properties of of these molecules
like their their biological stability
how long they last in cells
how how much protein they express uh and
so on and so forth so none of these
properties really are kind of a function
of just the molecule itself uh in
question but really a function of any
number of of different molecules around
which I guess also explains why you know
uh end to end very large scale uh deep
learning um you know has maybe seems
like a you know a worthwhile thing to
try whether it's going to work or not
for the specific applic a of course we
don't know yet um there's some good
indications that it can be useful um you
know as we saw through Alpha fold and
and I guess some of the proteomics
mentioned models that I mentioned
earlier um but yeah I guess the proof is
still in the pudding awesome
clarification
sure okay so you know with that back to
these first principles I was uh alluding
to earlier and you know how how I
personally think the status quo might
actually be broken right now and and the
first kind of cluster of issues that I
see uh revolve around data uh this is um
maybe also the most important um given
that that data seems to at the end of
the day always be the one thing you know
it's the one thing that always matters
um it's not always the only thing that
matters uh certainly uh but it
definitely machine learning is the one
thing that always matters um and I guess
I'm in good company with opinions like
this right so Thomas tweeted a little
while ago few years ago
I was mostly interested in models um
over time he seen us interest shift to
data and you know seems like we're
slowly maturing um I I I agree um I do
think it's a bit cyclical um you know
these things go back and forth when a
revolutionary approach comes along um
and and you know promises and delivers
like learning uh massive improvements uh
then you know that that becomes for
quite a while tends to become the if it
works uh like it didn't this Kus or
doesn't this casus becomes the area you
know of of of most or a lot of attention
uh but then what we've seen in the field
a few times already is that as uh the
rewards you reap by applying those
methods and and improving them uh start
to slowly Plateau um you know you can
come back you can usually come back to
data in order to further improve stuff
um
and one interesting thing that I find
odd is um that you know we still haven't
found ways of endtoend learning from
samples from only the input or only the
output distributions so there are these
highly highly effective uh but also
pretty expensive pipeline heuristics I I
call I would call them heuristics um
like back translation or no noisy
student and lots of related methods that
effectively use learn models that are
learned on smaller amounts of lab
uh uh data input output pairs um in
order to then turn either samples from
only the input or samples from only the
output transl uh distribution into input
output examples and sure you can do that
and it works great
um with some drawbacks but still it
doesn't seem right and there are
architectures in fact several one in
particular I worked on together with a
bunch of people called Kermit um that
can be trained on um on on on input
examples output examples and input out
paired uh labeled examples but so far
it's not like they've actually shown
even remotely the same kind of
improvement um as as some of these in
quotes eristics and I find that pretty
odd and you know that's definitely kind
of a fairly obvious head scratcher in
the sense that we see this data can help
but end to end in kind of um yeah and
and simplistic or simpler architectures
uh not pipelined um schedules or so
forth we haven't been able to to
Really uh you know reach the same
improvements from from from that data um
a slightly maybe more involved situation
that is also very common actually is
that we're still not able to learn end
to end from unaligned data and so there
are many problems out there where the
vast majority of training data machine
translation probably being the most
uh uh the most prominent um where the
vast majority of training data today
actually comes from automated was
produced by automated systems that were
given ginormous amounts of input and
output uh examples but separately input
and output not aligned but knowing that
there should be alignable uh uh Pairs
and and so in machine translation for
instance uh work that I was involved in
uh back in 2008 already um more recently
far improved uh uh stuff um actually a
variety of of papers uh have basically
proposed these pipeline systems where
you have you have one step in which you
say for instance mine the web as a
collection of unaligned monolingual
documents and you try to identify
documents that are translated or that
should be paired as training data for
translation and you do so oftentimes
using a train translation system or
something quite similar um there's also
been huge progress in unsupervised
machine translation which makes this
assumption
uh right that that those links are not
known but that's all of all of those
approaches are fairly specific all the
ones I know of are fairly specific um to
machine translation and and so what I
haven't seen yet and I'm confused by
that is um a generic approach that
applies across Mt but also question
answering there should be tons of
questions out there that uh are that
also have answers somewhere out there on
the web that should be Parable um
and comes with different problems uh but
also shares I think a lot of the a lot
of the problems that machine translation
data um has image descriptions is is a
similar one there I maybe see more of a
reason for why we're in that situation
right we we've only recently started to
really use um you know training on on
otherwise
unstructured organically created
descriptions or you know of images all
texts and and similar uh uh in a way
that really um improves over over the
state-of-the-art trained only on on
label data certainly for image
classification um and and object
detection and friends um but still it
should be possible to to to get more
data this way and and the list of
problems for which this at least I think
seems to be true uh is is is actually
quite
long um maybe again kind of a um a
specialization or or an extension well
an extension of this could be that it
need not be the case that they come from
the same from the same modality right so
so usually today labeled examples for
one task say translation for instance
right they're specific to one set or one
pair of modalities so there is now end
to end there's end to end speech to
speech translation systems and they're
textto text translation systems uh and
so on and so forth but a translation
system that gets better if you show it
or when you show it pairs of text pairs
of spoken translated sentences um pairs
of images that depict you know have half
translations in them say for instance
you take a a photo of
um a restaurant menu in one language and
then the translated version uh in
another language where where you know
instances or pairs of of in all of these
modalities actually improve one at the
same system and this is despite the fact
that Transformers have actually uh
helped along the convergence of
architectures across modalities um and I
think one of the reasons why we've not
seen this certainly not much um is that
often times these uh joining these
modalities uh in single models is done
by effectively you know adding heads um
that are then specific to those
modalities and unfortunately we haven't
yet figured out um how to how to
engineer these heads such that they only
take care of things that are specific to
the modality they're responsible for and
not of anything else and that seems to
be uh I think a remaining challenge um
that that could address kind of this
observation that we currently can't seem
to be learning uh well from from uh
examples in different modalities um or
the union of many examples from many
different modalities each um but maybe
kind of as an side to this how about
multiple instances in the single
modality right so instead of
saying um oh you know we have text text
translation pairs we have uh uh you know
spoken uh language spoken language
translation pairs how about we have for
some of those texts multiple different
versions right so we have maybe multiple
ways of uh phrasing the same input
sentence or the same output sentence we
have multiple ways of pronouncing
similar input or synonymous input or
output sentence how about using that
using all of those in uh together at the
same time um there there is some some
interesting evidence from machine
translation and speech recognition
actually um that that this should help
quite a lot um but if you look at things
like uh computer vision problems where
it is also not uncommon that we have
many images of the same uh scene or the
same object uh there it seems much less
obvious how to actually then combine
those um in in in an elegant way and I
think actually ultimately my intuition
would be that it it hints at at an
underlying more fundamental underlying
problem that I'll try to kind of get
closer to later in the talk
um okay so we're technically already at
the time I wanted to use initially for
for just the presentation so I'll hurry
up a little bit um and and you know
maybe largely skip over this part but it
is also odd that in many problems or for
many different tasks um in computer
vision and elsewhere U but in computer
vision it's particularly pronounced
they're they're probably multiple
different uh sources of data that could
be particularly informative uh for for
learning models for for this task um and
one crass example uh is um you know
effectively things like say object
detection where clearly you know you
today most of those models are trained
on um two-dimensional annotations well
maybe annotations also labels also in
three dimensions but from uh annotations
of two dimension inputs images uh or
sometimes video um but you know and plus
some sematics plus some labels uh um
along with with bounding boxes but they
don't actually ever consider um
non-semantic uh data describing geometry
even though it's actually relatively
cheap to obtain this right so you know
unlabeled samples of Fairly complex 3D
geometri a obtained through Lar
can actually be created at fairly low
costs and I would say intuitively um if
if a model that tries to do object
detection understands a ton about not
about semantics of objects in the real
world but about their geometry of common
objects right you know what what do cars
what's the shape cars generally have
what are the shapes bicycles generally
have that that should help but actually
so far that doesn't seem to be something
uh uh that that we certainly not
commonly do um and it also seems like
pre-training for this kind of very
orthogonal data that you want to jointly
uh uh be able to learn from might not be
quite the right
Paradigm um and so you know maybe to sum
up the data part there's been amazing
progress around pre-training and
fine-tuning and this General Paradigm um
but transferring things across
modalities still typically involves what
I would call Franken models so these are
models where basically you know
depending on the modality you turn some
piece on and some other piece off and
you know you you basically swap out
pieces of of of the network um more or
less as deterministic functions of the
modality of your input or the modality
of your output and as a result but not
only because of this reason we're far
from able to far from being able to
utilize all the data that intuitively
would seem relevant to given problems
say like an object detection we're not
utilizing uh um 3D geometry data um and
given that data is by far the best
overall I would say and most reliable
way uh to improve models that strikes me
as a bit as a bit weird and from first
principles I would say there's a lot of
potential for changing things out um
we'll get that we'll get back to this uh
at least a little bit later um but
another area is indeed still efficiency
and you know I said earlier well
Transformers improve paralyzation or
paraliza of some of this um some of the
computation of of um representations of
pieces of your input or or your output
um and that's that's true and I think
that's that's really the the single
biggest reason uh uh why they help in
many cases um but I actually think
potentially the biggest head the biggest
amount of the single biggest kind of
source of inefficiency if you wish uh
still largely remains to be tackled um
and so you know non Auto agressive
machine translation made a lot of
progress diffusion models have very
recently made really impressive Lees
it's actually one of the one of the
really really uh fun things for me to to
you know on occasion uh look at back in
and largely computer visionland or
Graphics also uh is is what people are
doing now with the fusion models I think
it's really really cool addresses some
of the you know aesthetic uh uh issues
and also practical issues of autor
regressive models uh very very well and
in fact if it wasn't for diffusion
models it would have been at least
fourth and maybe a fifth reason uh why
we should request ters um in the talk uh
but yeah then despite Global receptive
fields that these Transformers have at
least you know for many modalities we
actually know that the information stays
largely local um so there's a there's a
paper by Lena VOA and a few others that
look at representations as they evolve
throughout layers and Transformers um
and and it seems like information
doesn't travel that far even though we
have this Global receptive field where
every piece of your input or
representation or your your presentation
at this can talk to any other piece in
every layer and there's some practical
some obvious reasons for this or more
obvious reasons for this one is the the
omnipresence of residual connections
that provide a very strong bias in for
uh keeping representations you know
roughly aligned with the locality or the
registration um say in sentences the
position of a word uh that that
representation is computed for um but
this also means that we allocate are
compute resources approximately uniform
across this registration across say for
example if you consider images or video
SpaceTime and okay that doesn't sound
like a like a terrible idea until you
look this image and if you currently
feed this image into a CNN or a vision
Transformer it doesn't matter most of
the flops expended look at Blue Sky and
only a small uh minority of them
actually look at the and what kind of
bird that might be that seems wrong and
seems bad enough for this image but if
you consider a video with a non-moving
camera then this is actually true for
most of the pixels most of the time in
most of the videos approximately at Le
Some Noise you know minute movements and
so forth but certainly for video this
seems to be uh you know a real a real
source of of inefficiency and that's
simply that the relev information for
any given task of these modalities the
relevant information content is often
clumpy because that's how the world is
and this is through you know at fixed
resolution fixed sampling rate so you
know fixed frames per second with a
fixed resolution it's certainly true and
sometimes much worse uh otherwise say
when you have Point clouds uh that are
distributed very non-uniformly in in
space and space time well space um and
there have been some end to end
approaches the perceiver is is an
interesting is an interesting one that
try to tackle this problem uh but so far
they don't seem to be particularly
efficient they solve some of the issues
but they don't seem to be uh to be a
real kind of complete answer to the
efficiency aspect of this
well and so to me that I can't help it
but to me this smells like compression
right the problem smells like it could
be solved by compression and so we know
that utilizing loss of compression
schemes that are actually aimed at human
perception not not machine perception uh
can help quite dramatically we've known
it for quite a while um and uh so so
could we just learn compression schemes
suitable across a variety of tasks and
you know how would they be if at all
different from today's learn
representations that that our neural
networks neural networks evolve kind of
layer by layer well one key difference
would be that they should either get
faster over time as training commences
uh or they should spread information
more uniformly as as basically as
training goes by and as as the model
gets there and I think ultimately it in
some way necessitates a change to the
way we we currently train these things
uh in part because we expect the sizes
of things to change right so if we want
to get the speed up then the input size
has to change the input shape can be
constant um uh if we want information to
spread more
uniformly uh then then again uh you I
would I would I would say one can argue
uh that Nar can of fix dispatching uh um
scheme and so forth Paradigm this can't
happen
now as I prepared this talk uh and
prepared the slides and in fact this was
this just happened earlier today um um I
was looking for one particular reference
uh that that highlighted one aspect of
this problem uh and had to ask Alexa
dovitz uh with whom I had had uh the
opportunity to work uh uh on on the
vision Transformer and was now inceptive
as well and asked him for this reference
and he didn't come back with a reference
he came back with the paper that hit
archive uh on November 3rd that actually
that I hadn't seen um uh so much for
archive sanity or Insanity that actually
for at least uh um some image tasks uh
seems to provide a a pretty good
solution that basically um you know kind
of uh is along these is along these
thoughts and ideas um so so yeah uh look
for that it's it's it's pretty it's
pretty neat I think um I haven't read it
in detail yet uh but yeah so so maybe
you know this is not one of those cases
where from first principles it's still
broken as of November 30 um okay and so
the last one that I'll have time the
last kind of block of things that I
think are are are broken um uh revolve
around symbols symbols objects you know
this this this kind of stuff and and
this is a super complex topic I'm
definitely not going to do any kind of
Justice um thankfully there are some
great reviews like the one from Claus G
um and and and friends but
um basically the specific kind of uh
um potential area and and currently
active area of of symbolic is uh object
uh uh Laden representations uh that I
would like to talk about shares uh one
thing with all the other kind of
instances of objects or symbolic is
techniques and and deep learning uh
namely that my overly simplistic view of
them is that they really are merely
efficiency hacks that we should learn
and that they you know don't do us any
good really um other than speeding stuff
up this is definitely a you know
somewhat controversial opinion um but if
we look at only symbols in uh in say
input and output representations it's
not too difficult to make that claim at
least Le a little bit okay so so a
thought experiment right usually we
represent text for instance as a
sequence of tokens maybe down to
characters or so right but this actually
comes at a significant cost in
expressivity we we take this for granted
um but you know we certainly can
currently train text models that
understand this thing not in full at
least right
um maybe if you were to now kind of add
formatting tab mags and stuff like that
maybe for this case it were it would
work and maybe you know you could claim
okay fine but that seems kind of you
know unimportant super rare and
Whimsical right but then let me ask you
why is it that I can only have new
emojis once a year and why do they have
to be selected by some committee right I
we recently had this case actually in
practice where no there's a DNA Emoji
but there's no RNA Emoji so we thought
oh can't we make the claim now that RNA
is something Mna in particular that's on
everybody's mind mind can't we just ask
them to get us an RNA emoji and we were
about 3 days too late for the deadline
and uh you know reaching out to uh to
the Unicode Consortium didn't help I
understand why and so forth but you know
there there definitely some drawbacks uh
to to uh requiring U um these kinds of
organization um that that stem
ultimately from the fact that we want to
use symbols to encode this stuff and it
gets pretty bad once you leave kind of
you know the the the Latin script
dominated uh uh parts and uh you
encounter that yes in fact there are
scripts to this day where proper typ
setting on computers doesn't really work
Mongolian I think is an example of that
to be honest I don't know exactly how
common the phenomena are that are not
well expressed uh in in in today's type
setting systems um so I can't really
kind of estimate the the gravity of
these issues but it's definitely the
case that we're losing out uh in terms
of our richness of representation of
things like text
um uh by using these kinds of symbols so
what if instead of doing that we
presented it you know across many tasks
in the more generic form say you know as
the pixels of an image and which we've
rendered the text and that's something
that we've actually tried from machine
translation um and and uh wrote a little
paper about uh while go um Elman um but
obviously that's terribly inefficient
right so now instead of using these
symbols that that we hand engineer you
know like emojis like Unicode at the end
of the day um you know now we have
pixels uh they usually requ you need
many more pixels uh certainly if you
want to express kind of these rich
formatting and and graphical phenomena
um and so with today's Machinery that's
terribly inefficient but let's say let's
assume for a second we actually had some
really efficient learned uh and
compressed implicit
presentation we could potentially get it
down all the way or not all the way but
fairly close um to to kind of those
symbolic representations um and they
might actually remain richer and it's
not entirely clear that we couldn't in
some cases go below that because text
also tends to be compressible even even
losslessly and so actually you know it
seems like this idea that effectively
seems obvious for IM in video might even
ultimately help with efficiency if it's
applied to text at least if you have it
in sufficiently large amounts so to make
a very long story and very very kind of
hypothetical story uh somewhat shorter I
wonder what would models look like if we
reduce the Reliance on on external
symbolic representations on on hand
design crafted symbolic representations
at least for input and up and if we
compensated for the efficiency loss by
generically or with generically learned
dense implicit
representations um and then on the way
increasingly unified use this kind of uh
transition to increasingly unify the
shapes of different modalities right so
we've now said that maybe images and
text should or could look very much
alike I don't know exactly what this
would look like have some some some
guesses but they're definitely too half
big to put them even on these slides um
but you know maybe we could also apply
this to other to other modalities as
well and then maybe even this multiple
heads input and output heads Hydra
problem would go away right could we
then actually just stuff this all into
one single giant neural network not one
where you know individual pieces have to
be turned on or off depending on what
the modality is at any given moment but
where it's really just the same very
dense highly optimized uh architecture
same set of parameters you might not
even need elements anymore or maybe
they're always canonical right they're
kind fixed length up to a certain degree
or fixed size so anyway I wonder what
that would look like and I actually feel
like you know maybe using at least some
of these uh uh as hap as they all are uh
observations and and you know quotes
first principles head scratching moments
uh to to inspire thought about um ideas
around what those models could look like
uh I personally think could be could be
quite
interesting awesome thanks a lot Jacob
was super nice talk it's um it's not
it's not every day that we get to see
the kind of like deep questions you know
about Transformers being um you know
Expos this way so very nice I think we
have time for a couple of questions um
there's there's one which is perhaps uh
a little bit technical but let's see how
we go so um it's uh there's a typo but
can Transformers be used to find which
code Snippets in one language are
similar to another so you know if you
you've got Java and pyth
and do you think you know graphical
neuron networks would be a solution and
I think what this question is getting at
is a little bit this thing about like
you emphasize a lot machine translation
and alignment and this kind of stuff and
you know do these problems also occur
when we deal with source
code super interesting question
um so let me think basically so I would
say I mean the the the simple
straightforward answer is uh yes um
Transformers could definitely be used uh
for for something like
this one interesting question is how
would you define what it means for
Snippets of code to be the same um and
and a very pragmatic definition that one
could potentially operationalize for
creating uh data around this is to run
the code and see what
happens and if you then you know end up
with either the same result or the same
behavior in quotes then maybe at some
point if that happens often enough you
want to say yeah these are reasonably
similar um and in in in the process this
basically generate instances um uh
increasing numbers of instances and
instances potentially increasing
complexity right if you start doing this
for single instructions and then you add
stuff you might even with you know
fairly unguided search procedures get to
a stage where you can just create
training data for a model uh that you
know then might look like something like
a seekto seek model but it could also
look like these um uh
um I guess what do you call them siames
or two Tower uh uh models that are
effectively uh trained to compare
different uh compare different instance
or input representations um for for
multiple instances
[Music]
um gnns in particular are basically ex
making use of the fact that that code is
has this has this much more stringent
structure than uh than natural language
or you know observations of the natural
world around us I think I mean certainly
in in in in in many cases I could
imagine could help with efficiency would
it dramatically transform our ability to
identify whether these two things
whether two code Snippets are the same
that I'm not so sure about because
ultimately whether they're the same or
not I guess doesn't have as much to do
with say their abstract syntax trees and
how those relate to each other um you
know where the crispness of GNN uh uh
would help as it does with the semantics
of the code um right the subtle issues
are you call two different Library
functions they're defined to be mostly
the same except for some subset of the
inputs and um and hence you ultimately
have to if you want to do it you know
just from from with zero other knowledge
you basically have to figure this
out ultimately I guess also made further
problematic because there are you know
many different ways of computing any one
thing and as a result even going all the
way down to executed instructions on on
on a given CPU or so or a given given
whatever it is uh might not actually
conclusively answer your question unless
they happen to be exactly the same so if
they compil to the same bite code or or
the same you know natively executed uh
uh um uh instruction sequence sure then
then you can but I don't think that's a
likely
event cool and maybe one more question
yeah thanks a lot one more question
would be um you know in this sort of
landscape of of challenges you mentioned
so things with symbolic representations
loss of compression and so on which of
these do you think is like the let's say
the most important so to speak if you
had to rank somehow these problems where
which ones keep you awake at night maybe
is the other way of of putting it so I
actually think they all are I really
think that they all are not exactly but
they all are roughly different sides of
the same coin so our difficulties in
learning you know interchangeably from
different modalities you know they stem
from the fact I think maybe overly
simplified but I think they largely stem
from the fact that we have this this
heads problem that basically we we don't
have a neural network into which we can
actually input these efficiently uh you
know without swapping out some sets of
parameters or maybe some some kind of
structural pieces um and so that you
could address by having a in a certain
sense uh a representation um you know at
a meta level that is able to express
things in all of these modalities um and
and that you you know you could get by
learning kind of these compression
schemes if you learn them across many
different mods um
and and hence you
know I
guess I also think that ultimately our
our our only way of potentially getting
rid of these symbols which which you
know yes I think they they currently you
know they are very um pragmatic uh
humanistics for for improving efficiency
but we don't know how optimal they are
and they're certainly not sufficient in
order to in order to express everything
we'd like to express but I would I would
guess they're not optimally efficient
either and there again I think having
these kinds of uh yeah learned implicit
representations um is is a is a
reasonably good kind of general
direction for for trying to address uh
address the potential inefficiencies but
also
um uh yeah kind of broaden their or
increase their expressive increase the
expressive expressivity of of models for
some of these modalities um and so in a
certain sense I think they all kind of
belong to each other um doesn't mean
that the and it's certainly unlikely
that the easiest or kind of yeah the
most um direct uh potential solution for
addressing any single one or any single
very specific subset of these that they
would at all look you know similar to
each other that that I don't think is
the case um but you know maybe kind of
in the limit if you want to generally
solve all of these it might be that
actually they share one of the same
solution so so he sort of telling us
that you know the future may involve
some sort of you know not maybe
radically different architecture but you
know some some there is life beyond
Transformers in some sense I actually
think the the architectures
will increasingly uh get simpler and
simpler and simpler and they will
increasingly look um very much like I me
you'll be you'll be able to tell the
architecture from the hardware runs on
and so um ultimately I think to to me
personally um the big lesson that I
learned uh in in in the vision
Transformer uh uh work is that these
patches that we use there have nothing
to do with the
images they they're they're not chosen
based on the image they're they're fixed
so why is it that some patches work
better than other patch sizes and so
forth It's because they have slightly
different computational tradeoffs it has
nothing to do with the nature of the
images uh as far as I can tell um maybe
in extreme cases that that could happen
you could imagine certain distributions
of images and certain kind of patch
sizes that that just make it harder or
make it easier um but but by and large
across kind of the the universe of
images the universe of possible patch
sizes or the space of possible patch
sizes uh what matters I think is how
well they are suited to the underlying
Arbor and so my guess is what we're
going to see is that the architectures
will look more and more boring which I
think is great U because architecture
engineering tends to be this kind of you
know black magic alchemy that more often
doesn't work than it does um and and
ultimately I think um yeah basically
we'll use more and more uniform
necessarily will necessitate more and
more similar and uniform
representations um that that have where
for instance also the the relation
between um representation size if you
wish and compute budget becomes much
much less direct right so right now if
you want to expend um more flops on the
same image
right take more patches of the same size
and you're going to do exactly that um
and and then that doesn't get better at
least from from our experiments that
doesn't seem to get better you know
because somehow your your additional
patches now overlapping or whatever um
add kind of more in a certain sense more
interesting detail it just gets better
because you add more flops as you would
kind of expect but that it shouldn't it
shouldn't be that kind of your your your
way of computing your initial input
representation if you want to think
about it that way is kind of the screw
that you use in order to adjust the
expanded compute up or down if anything
it should be kind of how information
Rich given the task or for the task
given
input might be and ideally you know
ultimately thinking of as humans um a
little bit at least maybe what really
should necessitate how much compute you
spend is how much time you have if I
have a ton of time to compute the answer
for a given input take the time I
mean assuming the energy comes for free
and in a sustainable Manner and so on
and so forth
but but why not take longer but if you
have to make a decision really quickly
because you know the product calls for
it in that in that situation ideally the
model is able to at least give you a
reasonable guess uh in in much less time
with much less compute and so I feel
like that's what we should actually
strive for like any time algorithm style
um um anyway so yeah long story short I
think uh architectures uh will hopefully
become less boring and just ideally
suited to to the hardware they run on
and uh you know in a certain sense the
remaining work to get us there um is is
I think in figuring out um how to unify
and uh and how to make effective and
efficient uh the representations that we
use awesome thank you I think we've um
run out of time so thank you once again
for sharing your your deep knowledge
with us and the community and um yeah
it's been it's been a pleasure so
for listen hope next time we ask you all
these problems are solved well we'll see
I mean given that paper that came that
that that you know I only just saw
earlier today that came out on November
3D who knows that face great thank you
very much jaob thank you
thanks cool so that's the end of day one
thanks a lot yeah thanks a lot for
following this event today everyone uh
we have a second live event tomorrow at
the same time it begins at 8: a.m. PST
and it's going to end around noon PST
with theing Team more focused and giving
you technical talks on how to funun the
Transformers model I tou on the flow and
how to to upload the model to the to sh
the community and how to build a space
to build a nice demo out of it and so
it's mostly members of the gek team as I
said as well as a guest speaker from the
AWS sagemaker team and another speaker
from the gradio team so thanks a lot for
following today and see you
soon bye everyone see you tomorrow bye